<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DECISION  J</title>
    <description>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 12 Aug 2022 16:01:51 +0900</pubDate>
    <lastBuildDate>Fri, 12 Aug 2022 16:01:51 +0900</lastBuildDate>
    <generator>Jekyll v4.1.1</generator>
    
      <item>
        <title>[XAI] SHAP</title>
        <description>&lt;h1 id=&quot;review-shap&quot;&gt;[Review] SHAP&lt;/h1&gt;

&lt;p&gt;ML/DL 모형들이 널리 쓰여지면서 모형의 결과에 대해 해석이 가능한 지에 대한 연구도 활발히 진행되고 있습니다.(a.k.a. 설명 가능한 모형(eXplainable AI)) Feature Importance나 LIME과 같이 주로 변수들이 모형에 얼마나 영향을 미쳤는지를 파악하는 형태로 해석하게 되는데요! 이런 방법론 중 가장 이론적 뒷받침이 탄탄하다고 알려진 &lt;strong&gt;SHAP&lt;/strong&gt;에 대해 공부한 바를 간단히 정리해보고자 합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;shap이란&quot;&gt;SHAP이란?&lt;/h3&gt;
&lt;p&gt;SHAP(SHapley Additive exPlanation)은 Shapley value를 활용하여 모형의 결과에 대해서 각 특성(변수)의 기여도가 어느 정도인지 계산하는 방법론입니다. 이 기여도를 통해서 어떤 변수가 모형의 결정에 얼마나 영향을 미쳤는지 확인할 수 있고, 이를 통해 모형을 해석하는 것이지요. 방법론의 목적이나 아이디어는 Permutation기반의 feature importance와 차이가 없습니다만, SHAP의 경우가 좀 더 이론적 배경이 탄탄하며, 변수의 기여도를 계산할 때 조건부 확률의 개념을 적용함으로써 변수 간의 상관관계에 영향을 덜 받는다는 장점이 있습니다. SHAP의 저자인 Lunberg와 Lee는 kernel SHAP과 Tree SHAP의 두 방법을 제안했습니다. 이 방법론들을 알아보기 전에 먼저 Shapley value에 대해서 간단히 확인해보겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;shapley-value&quot;&gt;Shapley Value&lt;/h3&gt;
&lt;p&gt;Shapley value는 게임 이론에서 파생된 개념이라고 합니다. 어떤 결과를 위해 참여자들(players)들이 얼마나 기여했는지를 계산하는 것이라고 하는데요. XAI에서는 이 개념이 각 변수들을 참여자들로 보고 모형의 결과를 나타내기 위해 변수들이 얼마나 기여했는지를 계산하는 방식으로 쓰여집니다. 구체적으로 어떤 개념인지 유명한 사례를 통해 살펴보겠습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/Post Images/Shapley1.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;우리의 모형이 어떤 아파트의 가격을 30만 유로라고 예측했다고 가정합시다. 이 때 모형 안에 있는 변수는 3가지입니다. 1. 근처에 공원이 있으며, 2. 50제곱미터 크기의 2층이고, 3. 고양이 출입이 금지되는 곳이라는 것입니다. 이 세 가지 정보 중 고양이 출입이 금지되었다는 변수가 가지는 기여도에 대해 확인해보고 싶습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/Post Images/Shapley2.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;위 그림처럼 고양이 다른 변수는 동일하게 고정한 채, 고양이 출입 가능 여부만 바꿔서 계산해보면 고양이 출입에 대한 기여도를 확인할 수 있습니다. 모형은 고양이 출입이 허용되면 집 값이 1만 유로 더 비싸질 것이라고 예측하는 것으로 보아 고양이 출입이 집 값에 + 영향을 미친다고 보고 있군요.
Shapley value는 여기서 그치지 않고 &lt;strong&gt;다른 두 변수의 가능한 모든 경우의 수 조합&lt;/strong&gt;으로 확장하여 고양이 출입 변수에 대한 기여도를 계산합니다. 이를 통해 가장 객관적인 고양이 출입 변수에 대한 기여도를 확인할 수 있게 되는 것입니다. 모든 경우의 수는 $2^3=8$ 가지 입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/Post Images/Shapley3.png&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;kernel-shap&quot;&gt;Kernel SHAP&lt;/h3&gt;
&lt;p&gt;앞서 SHAP은 Shapley value를 활용하여 변수 기여도를 계산한다고 말씀드렸습니다. 그 중에서 가장 먼저 나온 Kernel SHAP의 매커니즘을 살펴보겠습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sample coalitions $ z_k \in ${0,1}$^M, k \in ${1,…,K}$ $ (1=feature present in coalition, 0=feature absent)&lt;/li&gt;
  &lt;li&gt;Get prediction for each $z_k$ by first converting $z_k$ to the original feature space and then applying model $g$ : $g(h_x(z_k))$&lt;/li&gt;
  &lt;li&gt;Compute weight for each $z_k$ with the SHAP kernel&lt;/li&gt;
  &lt;li&gt;Fit weigted linear model&lt;/li&gt;
  &lt;li&gt;Return Shapley value $\phi_k$, the coefficients from the linear model&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;복잡해보이지만 하나씩 살펴보면 아이디어는 어렵지 않습니다. 우선 $z_k$부터 이해해보면 변수가 있으면 1, 없으면 0으로 표시하는 조합들입니다. NLP에서의 one-hot embedding과 똑같은 개념이라고 이해하면 될 것 같습니다. 이렇게 나타내는 이유는 위의 shapley value에서 살펴본 것처럼 변수의 존재 유무의 조합에 따라 변수 기여도를 계산하기 위해서 입니다.&lt;/p&gt;

&lt;p&gt;2번을 보시면 $h_x$ 함수가 나옵니다. 이는 0,1로 구성되어 있는 $z_k$를 x와 유사하도록 mapping 시켜주는 일종의 장치입니다. 아래 그림을 살펴보시죠.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/Post Images/SHAP.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;x를 살펴보시면 age, weight, color 세 가지 변수가 있고, 아래에는 그 중 age만 존재하는 조합 $z$에 대해 표현되어 있습니다. 이 때 이 $z$를 기존 $x$처럼 변환시켜주는 것이 $h_x$입니다. 그림에서는 각각 17과 pink로 대체하는 것을 볼 수 있는데요. 이는 데이터 셋 내의 다른 변수들을 랜덤으로 고릅니다. &lt;em&gt;(랜덤으로 대체하는 이 부분 때문에 Kernel SHAP 또한 Permutation 기반 해석 방법과 동일한 문제를 갖는다고 합니다.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;이제 SHAP에 필요한 구성 요소들에 대해 알았으니 SHAP의 큰 아이디어를 정리해보겠습니다. SHAP에 대해 정리하던 중 이를 잘 표현한 블로그 내 그림이 있어 가져왔습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/Post Images/SHAP2.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;현재 우리가 해석하고 싶은 모형은 black box인 $f$입니다. 우리는 이 f를 가장 잘 설명하는 $g$라는 모형을 “새로” 추정하고자 합니다. $g$는 Shapley value를 계수로 하는 linear 모형이며, 마침내 이 계수 값을 통해 우리는 각 변수들의 기여도를 확인할 수 있습니다. 선형 회귀의 $\beta$를 해석하는 것과 동일하게 말이죠.&lt;/p&gt;

&lt;p&gt;$$
g(z) = \phi_0 + \sum^M_{j=1}\phi_j z_j 
$$&lt;/p&gt;

&lt;p&gt;$g$를 학습하기 위한 Loss function은 아래와 같습니다. &lt;em&gt;(아이디어만 확인하기 위함이므로 수식을 단순화 하였습니다. 실제로는 shapley kernel term이 가중치로 들어있습니다.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;$$
L(f, g) = \sum[f(h_x(z))-g(z)]^2
$$&lt;/p&gt;

&lt;h3 id=&quot;shap의-활용&quot;&gt;SHAP의 활용&lt;/h3&gt;
&lt;p&gt;이렇게 SHAP의 이론은 복잡한 것 같지만 사실 실제 활용은 어렵지 않습니다. 파이썬이 알아서 다 해주니까요! ㅎㅎ&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/Post Images/SHAP3.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;파이썬에서 SHAP 라이브러리를 통해 나온 결과입니다. 세로 축에는 모형이 사용하고 있는 변수들이 쭉 나열되고 가로 축에는 각 변수들의 SHAP value들이 표현됩니다. SHAP value의 절대값이 클수록 모형에 기여하는 영향이 큰 것입니다. 오른쪽 이중축에는 변수 자체의 값에 대해 표현되고 있는데요, 빨간 색일수록 큰 것이고 파랑색일수록 작은 값이라는 뜻입니다. 플랏을 해석해보자면, 먼저 맨 위의 변수인 &lt;em&gt;Hormonal Contraceptives years&lt;/em&gt;를 살펴보면 Shap value가 음수인 부분에는 파란색 점이, 양수인 부분에는 빨간점이 비교적 많이 분포하는 것을 확인할 수 있습니다. 이를 해석해보면 이 변수의 값이 작을 수록 모형의 결과 값에는 -, 음의 영향을 미치는 것을 알 수 있고, 값이 클수록 모형에 + 영향을 미칩니다. 특히, 변수의 값이 클 때 비교적 모형에 많이 기여하는 것을 확인할 수 있네요! 
또 아래 쪽에 존재하는 변수들은 대부분 값이 클수록 모형에 양의 영향을 미치는 것을 알 수 있습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/1705.07874&quot;&gt;Lundberg, Scott M., and Su-In Lee. “A unified approach to interpreting model predictions.” Advances in Neural Information Processing Systems. 2017.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/1802.03888&quot;&gt;Lundberg, Scott M., Gabriel G. Erion, and Su-In Lee. “Consistent individualized feature attribution for tree ensembles.” arXiv preprint arXiv:1802.03888. 2018.)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Blog: &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/shap.html&quot;&gt;SHAP&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Blog: &lt;a href=&quot;https://datanetworkanalysis.github.io/2019/12/24/shap2#fnref:1&quot;&gt;DNA, SHAP에 대한 모든 것&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Blog: &lt;a href=&quot;https://moondol-ai.tistory.com/378&quot;&gt;SHAP에 대해 알아보자!&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sat, 16 Jul 2022 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/papers/2022/07/16/SHAP.html</link>
        <guid isPermaLink="true">http://localhost:4000/papers/2022/07/16/SHAP.html</guid>
        
        <category>XAI</category>
        
        <category>SHAP</category>
        
        <category>ShapleyValue</category>
        
        <category>model</category>
        
        
        <category>Papers</category>
        
      </item>
    
      <item>
        <title>[Time Series] Anomaly Transformer</title>
        <description>&lt;h1 id=&quot;paper-review-anomaly-transformer&quot;&gt;[Paper review] Anomaly Transformer&lt;/h1&gt;

&lt;p&gt;시계열 데이터의 이상치 탐색 방법론으로서 Transformer 매커니즘을 활용한 &lt;strong&gt;Anomaly Transformer&lt;/strong&gt;에 대해 리뷰해보겠습니다. ICLR 2022 의 spotlight 논문이라고 하네요!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Xu, Jiehui, et al. “Anomaly transformer: Time series anomaly detection with association discrepancy.” arXiv preprint arXiv:2110.02642 (2021)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;(본 리뷰의 모든 수식과 그림은 원 논문을 참고했습니다.)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;idea&quot;&gt;Idea&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Anomaly Transformer&lt;/strong&gt;는 이름에서 알 수 있듯이 &lt;em&gt;Transformer&lt;/em&gt;를 활용하여 시계열 데이터에서의 &lt;em&gt;Anomaly detection&lt;/em&gt;을 잘 해보자는 목적을 가지고 있습니다. 이상점을 탐색하는 방법으로는 reconstruction based 방법을 사용하고 있습니다. Unsupervised learning으로 주어진 train 시계열 데이터의 패턴을 통해 정상 시계열 패턴을 잘 재구축 하도록 모형을 학습시킵니다. 추후에 test set이 들어왔을 때, 모형이 재구축(reconstruction)한 시계열 패턴과의 비교를 통해 그 차이가 큰 (anomaly score가 큰) 지점을 이상점으로 탐색하는 방법론입니다. 이 때, 정상 시계열 패턴을 잘 학습하도록 해주기 위해 Transformer를 활용합니다. Self-attention을 이상점 탐지에 특화되게 바꾼 Anomaly-attention을 제안하였습니다. 그럼 Anomaly Transformer에 대해 본격적으로 알아보도록 하겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;anomaly-transformer&quot;&gt;Anomaly Transformer&lt;/h3&gt;
&lt;p&gt;Anomaly Transformer의 architecture는 다음과 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/Anomaly_Detection/Anomaly_Transformer/Architecture.png&quot; alt=&quot;Anomaly Transformer architecture&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Input embedding $X_{0}$가 들어오면 크게 &lt;strong&gt;Anomaly Attenion&lt;/strong&gt; layer와 Feed Forward layer를 L번 반복 수행하며 input time series의 pattern을 학습하게 됩니다. 이후 이 학습된 정보를 가지고 가장 보편적인 pattern의 reconstruction data를 산출하는데요! 이 구조에서 가장 주목해야 하고, Anomaly transformer가 높은 성능을 가지는 이유는 단연 Anomaly attention일 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;anomaly-attention&quot;&gt;Anomaly Attention&lt;/h3&gt;
&lt;p&gt;Anomaly attention은 기본적인 attention을 가지고 저자들이 time series anomaly detection에 맞게 매커니즘을 조금 수정한 형태입니다. 가장 큰 부분은 attention 내부를 &lt;em&gt;Prior-Association&lt;/em&gt;과 &lt;em&gt;Series-Association&lt;/em&gt;의 두 가지로 나누었다는 것입니다. 
먼저 prior association은 이름에서 알 수 있듯이, 모형에서 사용하는 Gaussian kernel의 시그마를 학습합니다. Gaussian Kernel은 attention의 Query, Key를 학습할 때 영향을 줍니다. 커널을 통해 다양한 패턴의 시계열 자료에 적용할 수 있다고 저자는 설명하고 있습니다. Series asssociation은 Query와 Key를 학습하는 부분입니다. 저자들은 이 두 association을 통해서(정확히는 prior association) 시계열 자료를 point-wise가 아닌, temporal dependency를 학습할 수 있다고 설명합니다. Gaussian Kernel의 $\sigma$를 통해서 인접한 time point에 더 큰 가중치를 줄 수 있다는 것이지요.&lt;/p&gt;

&lt;h3 id=&quot;association-discrepancy&quot;&gt;Association Discrepancy&lt;/h3&gt;
&lt;p&gt;Association Discrepancy는 앞서 살펴본 prior association과 series association간의 차이를 계산하는 것입니다. 차이를 계산하는 방법은 KL divergence를 활용하였으며, KL divergence의 assymetric한 점을 보완하기 위해 순서를 바꿔가며 계산한 평균을 사용하였습니다. Anomaly attention은 이 지표를 활용하여 가중치를 업데이트 하게 됩니다.&lt;/p&gt;

&lt;p&gt;$$ AssDis(P,S; X) = \frac{1}{L} \sum^L_{l=1}(KL(P^l_i||S^l_i) + KL(S^l_i||P^l_i)) $$
$$ where\,\, i=1,…,N $$&lt;/p&gt;

&lt;h3 id=&quot;mini-max-association-learning&quot;&gt;Mini Max Association Learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Mini Max Strategy&lt;/li&gt;
  &lt;li&gt;Association-based Anomaly Criterion&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt;

&lt;h3 id=&quot;느낀점&quot;&gt;느낀점&lt;/h3&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Paper: &lt;a href=&quot;https://arxiv.org/pdf/2110.02642.pdf&quot;&gt;Anomaly Transformer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Youtube: &lt;a href=&quot;https://www.youtube.com/watch?v=C3dphckvyn0&amp;amp;ab_channel=%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90%EC%82%B0%EC%97%85%EA%B2%BD%EC%98%81%EA%B3%B5%ED%95%99%EB%B6%80DSBA%EC%97%B0%EA%B5%AC%EC%8B%A4&quot;&gt;고려대학교 산업경영공학과 DSBA연구실 세미나&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Github: &lt;a href=&quot;https://github.com/thuml/Anomaly-Transformer&quot;&gt;Code (Pytorch)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;architeture&quot;&gt;Architeture&lt;/h3&gt;
&lt;p&gt;Efficient Det은 구글 브레인의 이전 논문인 Efficient Net을 Backbone으로 활용한 Object detection 버전 모델입니다. 논문 곳곳에서 효율적이고 가볍다는 언급을 자주 할만큼 효율성에 집중한 듯 하지만 정확도 또한 20년 기준 SOTA를 달성한 굉장한 모형입니다. 기본적으로 YOLO 등과 비슷하게 One stage detection 구조를 가지고 있습니다. 자세한 아키텍처는 아래와 같이 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;$$ Efficient\, Det = BiFPN + Compound\, Scaling + Efficient\, Net$$&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/Efficient_Det/architecture.png&quot; alt=&quot;Efficient Det architecture&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Backbone인 Effcient Net외에 &lt;em&gt;Bi-FPN&lt;/em&gt;과 &lt;em&gt;Compound Scaling&lt;/em&gt;이라는 두 기법이 추가로 더 적용되어 있는 것을 확인하실 수 있습니다. 지금부터 두 방법이 어떤 것인지 알아보도록 하겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;bi-fpn&quot;&gt;Bi-FPN&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/Efficient_Det/bifpn.png&quot; alt=&quot;Bi-FPN과 기존 Feature pyramid 방식들&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;기본적으로 Object detection 알고리즘들은 이미지나 비디오의 &lt;em&gt;feature&lt;/em&gt;를 기준으로 detecting을 합니다. 이 때, 다양한 resolution에서 feature를 받아 분석하는 것을 &lt;strong&gt;multi-scale feature fusion&lt;/strong&gt; 이라고 합니다. 대표적인 방식으로는 FPN(Feature Pyramid Network)이 있죠! FPN은 각 scale에서 feature를 받을 수 있지만 Top-Down 방식만 가능하다는 단점이 있습니다. 이에 Bottom-Up flow도 추가한 것이 PANet입니다. 가장 정확하다는 평가를 받는 feature fusion 방식입니다.&lt;/p&gt;

&lt;p&gt;하지만 이러한 PANet은 굉장히 많은 parameter를 가질 수 밖에 없기 때문에 cost가 늘어난다는 단점이 있습니다. (이 단점을 해결하기 위해 NAS-FPN이 등장했지만 정확도가 떨어집니다.) Bi-FPN은 본 논문에서 제시하는 방법론으로, 정확성을 담보하면서도 PANet보다 효율적인 구조입니다. 개인적으로는 PANet 기본 구조에 효율성을 갖도록 몇 가지를 수정한 방법이라고 보여집니다.&lt;/p&gt;

&lt;p&gt;우선 Bi-FPN의 Scale connection 부분부터 살펴보겠습니다. Bi-FPN은 PANet과는 달리 input edge만을 갖는 노드는 없게 설계하여 parameter를 줄였습니다. 또한 Same level의 input node는 바로 bottom-up 단계의 output node로 연결하여 많은 cost없이 feature fusion이 일어나도록 설계했습니다. 마지막으로 Top-down / Bottom-up path들을 NAS-FPN처럼 반복함으로써 정확도를 확보했습니다.&lt;/p&gt;

&lt;p&gt;이렇게 연결된 feature들을 fusion하는 방법에 대해 알아보겠습니다. 기본적으로 각 Input들을 그냥 결합할 경우 scale에 따라 가중치가 달라지기 때문에, 이를 반영하여 결합하는 것이 각 scale의 주요 특징들을 반영하는 데 중요합니다. 본 논문에서는 &lt;em&gt;Fast Normalized fusion&lt;/em&gt;을 사용해서 빠르게 결합하는 방식을 사용했습니다.&lt;/p&gt;

&lt;p&gt;$$ O = \sum_i \frac{w_i}{\epsilon+\sum_j w_j} * I_i$$&lt;/p&gt;

&lt;h3 id=&quot;compound-scaling&quot;&gt;Compound Scaling&lt;/h3&gt;
&lt;p&gt;앞 서 Bi-FPN에서 반복을 통해 feature들을 파악한다고 했는데요! 그러면 layer의 반복 수는 어떻게 정하면 될까요? 또 Efficient Det의 scale을 키울 때, 각각의 channel이나 layer들은 어떻게 정해야 효율적으로 scale up 할 수 있을까요? 이런 것들을 정하는 것이 &lt;strong&gt;Compound Scaling&lt;/strong&gt; 파트입니다.&lt;/p&gt;

&lt;p&gt;결론적으로 말하면 케이스 별로 optimize해주는 것은 아니고 저자들이 실험을 통해 최적의 궁합을 찾았다는 것입니다. 저자들은 &lt;em&gt;compound coefficient&lt;/em&gt; $\phi$를 사용하여 각 scale에서의 layer 수, channel 수, dimmension 등을 컨트롤합니다. Compound coefficient가 컨트롤하는 정보들은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;$$
W_{bifpn} = 64 * 1.35^\phi \\ 
D_{bifpn} = 3 + \phi &lt;br /&gt;
D_{box} = D_{class} = 3 + [\phi/3]
R_{input} = 512 + \phi * 128
$$&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/Efficient_Det/compound.png&quot; alt=&quot;Compound Scaling&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;보시는 것처럼 각 scale별로 자동적으로 coefficient들이 결정되는 것을 볼 수 있습니다. 이를 통해 각 feature들이 조화롭게 학습되기를 바라는 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;experiment-results&quot;&gt;Experiment Results&lt;/h3&gt;
&lt;p&gt;Efficient Det이 기존 Object Detection 방법들에 비해 얼마나 좋은 성능을 갖고 있는지 확인해보겠습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/Efficient_Det/result.png&quot; alt=&quot;Experiment result&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;논문에서 다양한 실험 결과들을 확인할 수 있지만 가장 대표적인 것으로 가져왔습니다. 위의 plot을 보시면 모델의 정확도가 다른 기존 방법론들과 비슷한데도 model의 parameter가 가장 작으며, hardware latency도 작음을 알 수 있습니다. 작지만 빠르고 강력한 모델이 바로 Efficient Det이라고 할 수 있겠네요!&lt;/p&gt;

&lt;h3 id=&quot;마무리&quot;&gt;마무리&lt;/h3&gt;
&lt;p&gt;지금까지 Efficient Det에 대해 살펴보았습니다. 개인적으로는 parameter 수를 최대한 줄이며 효율성을 추구했는데도 성능이 증가한 부분이 인상 깊습니다. 다만 Compound scaling 부분에서 처럼 hueristic하게 parameter들을 결정해도 모든 케이스에 대해서 같은 성능을 담보할 수 있을지 궁금합니다. &lt;del&gt;(물론 굉장히 유명한 모델인만큼 그럴 것이라 믿어 의심치 않습니다 ㅎㅎ)&lt;/del&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;reference-1&quot;&gt;Reference&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Paper: &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2020/papers/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.pdf&quot;&gt;Efficient Det&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Github: &lt;a href=&quot;https://github.com/google/automl/tree/master/efficientdet&quot;&gt;google/automl/Efficient Det&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 28 Jun 2022 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/papers/2022/06/28/Anomaly-Transformer.html</link>
        <guid isPermaLink="true">http://localhost:4000/papers/2022/06/28/Anomaly-Transformer.html</guid>
        
        <category>TimeSeries</category>
        
        <category>Anomaly-detection</category>
        
        <category>Transformer</category>
        
        <category>Attention</category>
        
        
        <category>Papers</category>
        
      </item>
    
      <item>
        <title>[CV] Efficient Det</title>
        <description>&lt;h1 id=&quot;paper-review-efficient-det&quot;&gt;[Paper review] Efficient Det&lt;/h1&gt;

&lt;p&gt;가볍고 효율적인 방식임을 강조하지만 정확성도 놓치지 않은 구글의 &lt;strong&gt;Efficient Det&lt;/strong&gt; 논문을 리뷰해보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tan, Mingxing, Ruoming Pang, and Quoc V. Le. “Efficientdet: Scalable and efficient object detection.” Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;(본 리뷰의 모든 수식과 그림은 원 논문을 참고했습니다.)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;architeture&quot;&gt;Architeture&lt;/h3&gt;
&lt;p&gt;Efficient Det은 구글 브레인의 이전 논문인 Efficient Net을 Backbone으로 활용한 Object detection 버전 모델입니다. 논문 곳곳에서 효율적이고 가볍다는 언급을 자주 할만큼 효율성에 집중한 듯 하지만 정확도 또한 20년 기준 SOTA를 달성한 굉장한 모형입니다. 기본적으로 YOLO 등과 비슷하게 One stage detection 구조를 가지고 있습니다. 자세한 아키텍처는 아래와 같이 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;$$ Efficient\, Det = BiFPN + Compound\, Scaling + Efficient\, Net$$&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/Efficient_Det/architecture.png&quot; alt=&quot;Efficient Det architecture&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Backbone인 Effcient Net외에 &lt;em&gt;Bi-FPN&lt;/em&gt;과 &lt;em&gt;Compound Scaling&lt;/em&gt;이라는 두 기법이 추가로 더 적용되어 있는 것을 확인하실 수 있습니다. 지금부터 두 방법이 어떤 것인지 알아보도록 하겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;bi-fpn&quot;&gt;Bi-FPN&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/Efficient_Det/bifpn.png&quot; alt=&quot;Bi-FPN과 기존 Feature pyramid 방식들&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;기본적으로 Object detection 알고리즘들은 이미지나 비디오의 &lt;em&gt;feature&lt;/em&gt;를 기준으로 detecting을 합니다. 이 때, 다양한 resolution에서 feature를 받아 분석하는 것을 &lt;strong&gt;multi-scale feature fusion&lt;/strong&gt; 이라고 합니다. 대표적인 방식으로는 FPN(Feature Pyramid Network)이 있죠! FPN은 각 scale에서 feature를 받을 수 있지만 Top-Down 방식만 가능하다는 단점이 있습니다. 이에 Bottom-Up flow도 추가한 것이 PANet입니다. 가장 정확하다는 평가를 받는 feature fusion 방식입니다.&lt;/p&gt;

&lt;p&gt;하지만 이러한 PANet은 굉장히 많은 parameter를 가질 수 밖에 없기 때문에 cost가 늘어난다는 단점이 있습니다. (이 단점을 해결하기 위해 NAS-FPN이 등장했지만 정확도가 떨어집니다.) Bi-FPN은 본 논문에서 제시하는 방법론으로, 정확성을 담보하면서도 PANet보다 효율적인 구조입니다. 개인적으로는 PANet 기본 구조에 효율성을 갖도록 몇 가지를 수정한 방법이라고 보여집니다.&lt;/p&gt;

&lt;p&gt;우선 Bi-FPN의 Scale connection 부분부터 살펴보겠습니다. Bi-FPN은 PANet과는 달리 input edge만을 갖는 노드는 없게 설계하여 parameter를 줄였습니다. 또한 Same level의 input node는 바로 bottom-up 단계의 output node로 연결하여 많은 cost없이 feature fusion이 일어나도록 설계했습니다. 마지막으로 Top-down / Bottom-up path들을 NAS-FPN처럼 반복함으로써 정확도를 확보했습니다.&lt;/p&gt;

&lt;p&gt;이렇게 연결된 feature들을 fusion하는 방법에 대해 알아보겠습니다. 기본적으로 각 Input들을 그냥 결합할 경우 scale에 따라 가중치가 달라지기 때문에, 이를 반영하여 결합하는 것이 각 scale의 주요 특징들을 반영하는 데 중요합니다. 본 논문에서는 &lt;em&gt;Fast Normalized fusion&lt;/em&gt;을 사용해서 빠르게 결합하는 방식을 사용했습니다.&lt;/p&gt;

&lt;p&gt;$$ O = \sum_i \frac{w_i}{\epsilon+\sum_j w_j} * I_i$$&lt;/p&gt;

&lt;h3 id=&quot;compound-scaling&quot;&gt;Compound Scaling&lt;/h3&gt;
&lt;p&gt;앞 서 Bi-FPN에서 반복을 통해 feature들을 파악한다고 했는데요! 그러면 layer의 반복 수는 어떻게 정하면 될까요? 또 Efficient Det의 scale을 키울 때, 각각의 channel이나 layer들은 어떻게 정해야 효율적으로 scale up 할 수 있을까요? 이런 것들을 정하는 것이 &lt;strong&gt;Compound Scaling&lt;/strong&gt; 파트입니다.&lt;/p&gt;

&lt;p&gt;결론적으로 말하면 케이스 별로 optimize해주는 것은 아니고 저자들이 실험을 통해 최적의 궁합을 찾았다는 것입니다. 저자들은 &lt;em&gt;compound coefficient&lt;/em&gt; $\phi$를 사용하여 각 scale에서의 layer 수, channel 수, dimmension 등을 컨트롤합니다. Compound coefficient가 컨트롤하는 정보들은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;$$
W_{bifpn} = 64 * 1.35^\phi \\ 
D_{bifpn} = 3 + \phi &lt;br /&gt;
D_{box} = D_{class} = 3 + [\phi/3]
R_{input} = 512 + \phi * 128
$$&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/Efficient_Det/compound.png&quot; alt=&quot;Compound Scaling&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;보시는 것처럼 각 scale별로 자동적으로 coefficient들이 결정되는 것을 볼 수 있습니다. 이를 통해 각 feature들이 조화롭게 학습되기를 바라는 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;experiment-results&quot;&gt;Experiment Results&lt;/h3&gt;
&lt;p&gt;Efficient Det이 기존 Object Detection 방법들에 비해 얼마나 좋은 성능을 갖고 있는지 확인해보겠습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/Efficient_Det/result.png&quot; alt=&quot;Experiment result&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;논문에서 다양한 실험 결과들을 확인할 수 있지만 가장 대표적인 것으로 가져왔습니다. 위의 plot을 보시면 모델의 정확도가 다른 기존 방법론들과 비슷한데도 model의 parameter가 가장 작으며, hardware latency도 작음을 알 수 있습니다. 작지만 빠르고 강력한 모델이 바로 Efficient Det이라고 할 수 있겠네요!&lt;/p&gt;

&lt;h3 id=&quot;마무리&quot;&gt;마무리&lt;/h3&gt;
&lt;p&gt;지금까지 Efficient Det에 대해 살펴보았습니다. 개인적으로는 parameter 수를 최대한 줄이며 효율성을 추구했는데도 성능이 증가한 부분이 인상 깊습니다. 다만 Compound scaling 부분에서 처럼 hueristic하게 parameter들을 결정해도 모든 케이스에 대해서 같은 성능을 담보할 수 있을지 궁금합니다. &lt;del&gt;(물론 굉장히 유명한 모델인만큼 그럴 것이라 믿어 의심치 않습니다 ㅎㅎ)&lt;/del&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Paper: &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2020/papers/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.pdf&quot;&gt;Efficient Det&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Github: &lt;a href=&quot;https://github.com/google/automl/tree/master/efficientdet&quot;&gt;google/automl/Efficient Det&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 13 Apr 2022 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/papers/2022/04/13/EfficientDet.html</link>
        <guid isPermaLink="true">http://localhost:4000/papers/2022/04/13/EfficientDet.html</guid>
        
        <category>CV</category>
        
        <category>object-detection</category>
        
        <category>Efficient-det</category>
        
        
        <category>Papers</category>
        
      </item>
    
      <item>
        <title>Favicon 추가하기</title>
        <description>&lt;h2 id=&quot;필요할-때-다시-보려고-만든-자료&quot;&gt;&lt;em&gt;필요할 때 다시 보려고 만든 자료&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;이번에 Github blog를 리뉴얼하면서 여러 가지들을 공부하고 있습니다.&lt;br /&gt;
시행착오를 겪었던 것들 중 다음에도 참고할만한 내용들을 간단하게 정리해보려고 합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;오늘 정리해볼 내용은 &lt;strong&gt;Favicon 추가하기&lt;/strong&gt; 입니다. &lt;br /&gt;
Github blog를 처음 만들고 나면 favicon이 없어서 페이지 탭에 지구본 모양이 나타나는데요! &lt;br /&gt;
이 지구본 모양을 예쁜 저만의 favicon으로 바꿔보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;favicon-이미지-물색&quot;&gt;Favicon 이미지 물색&lt;/h2&gt;

&lt;p&gt;우선 맘에 드는 favicon 이미지를 찾아봅시다!
저는 이 사이트(&lt;a href=&quot;https://www.flaticon.com/&quot;&gt;Flaticon&lt;/a&gt;)를 참고했습니다. &lt;br /&gt;
저의 블로그 이름이 Decision J의 블로그이니만큼 예쁜 J 이미지를 찾아보았습니다 :)&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/logo.ico/favicon-32x32.png&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;favicon-만들기&quot;&gt;Favicon 만들기&lt;/h2&gt;
&lt;p&gt;선택한 이미지를 Favicon으로 만들어보겠습니다.
우선 &lt;a href=&quot;https://realfavicongenerator.net/&quot;&gt;realfavicongenerator&lt;/a&gt; 사이트에 접속해줍니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/github blog/favicon_generator.png&quot; alt=&quot;realfavicongenerator 사이트&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;이 곳에서 &lt;strong&gt;Select your Favicon image&lt;/strong&gt; 메뉴를 클릭하면 아까 물색했던 favicon image를 업로드할 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/github blog/favicon_generator2.png&quot; alt=&quot;Favicon 미리보기&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;업로드하면 위와 같이 favicon이 적용된 탭 모습 등의 프리뷰 화면을 제공하구요! &lt;br /&gt;
아래로 쭉 내려보시면 &lt;strong&gt;Generate Your Favicons and HTML code&lt;/strong&gt; 메뉴가 있습니다. 이 곳을 클릭해줍니다!&lt;/p&gt;

&lt;p&gt;얼마 간의 세팅 시간이 지나고 나면 다음과 같은 화면을 보실 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/github blog/favicon_generator3.png&quot; alt=&quot;Favicon 생성&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;여기서 해주셔야할 일은 두 가지입니다.&lt;/p&gt;

&lt;h3 id=&quot;1-assets-폴더에-logoico-만들기&quot;&gt;1. Assets 폴더에 logo.ico 만들기&lt;/h3&gt;
&lt;p&gt;Download your package에 쓰여져 있는 메뉴를 클릭해서 Real favicon generator가 만들어준 이미지 파일들을 다운로드합니다.
압축을 풀고 해당 폴더를 깃헙 블로그 폴더의 assets 내에 &lt;strong&gt;logo.ico&lt;/strong&gt;라는 이름으로 넣어줍니다!&lt;/p&gt;

&lt;h3 id=&quot;2-headhtml-코드-수정&quot;&gt;2. head.html 코드 수정&lt;/h3&gt;
&lt;p&gt;다음은 위 사이트에서 보여주는 HTML코드를 우리 블로그의 head.html에 추가해주어야 합니다. &lt;br /&gt;
블로그 테마에 따라 head를 변경해주는 파일은 여러 가지가 있을 텐데요!
제가 사용하고 있는 YAT 테마는 &lt;em&gt;_includes/custom-head.html&lt;/em&gt; 파일에서 favicon html코드를 추가할 수 있었습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/github blog/favicon_generator4.png&quot; alt=&quot;custom-head 코드 추가&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Real favicon generator 사이트에서 생성해준 HTML을 복붙하시면 되는데, 이 때 위와 같이 png 파일들의 경로를 1번에서 생성해준 logo.ico으로 href 경로를 수정해주시면 됩니다!&lt;/p&gt;

&lt;h2 id=&quot;favicon-생성&quot;&gt;Favicon 생성!&lt;/h2&gt;
&lt;p&gt;이 과정을 다 거치고 나면 나만의 예쁜 Favicon이 탭에 생성됩니다!!&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/github blog/favicon.png&quot; /&gt;
&lt;/p&gt;
</description>
        <pubDate>Sat, 26 Mar 2022 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/github-blog/2022/03/26/Favicon-%E1%84%8E%E1%85%AE%E1%84%80%E1%85%A1%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5.html</link>
        <guid isPermaLink="true">http://localhost:4000/github-blog/2022/03/26/Favicon-%E1%84%8E%E1%85%AE%E1%84%80%E1%85%A1%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5.html</guid>
        
        <category>github</category>
        
        <category>blog</category>
        
        <category>favicon</category>
        
        
        <category>Github-Blog</category>
        
      </item>
    
      <item>
        <title>[CV] YOLOX</title>
        <description>&lt;h1 id=&quot;paper-review-yolox&quot;&gt;[Paper review] YOLOX&lt;/h1&gt;

&lt;p&gt;YOLO series의 2021년 가장 최신 버전, &lt;strong&gt;YOLO X&lt;/strong&gt; 를 리뷰해보려고 합니다.&lt;/p&gt;

&lt;p&gt;(이전 버전 리뷰: &lt;a href=&quot;https://decision-j.github.io/computer-vision/2021/05/31/Yolo_review.html&quot;&gt;YOLO v1 review&lt;/a&gt;,  &lt;a href=&quot;https://decision-j.github.io/computer-vision/2021/07/26/Yolo_v2_review.html&quot;&gt;YOLO v2 review&lt;/a&gt;, &lt;a href=&quot;https://decision-j.github.io/computer-vision/2021/08/03/Yolo_v3_review.html&quot;&gt;YOLO v3 review&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;가장 최신의 YOLO series 논문을 살펴보면서 YOLO series는 마무리 하겠습니다!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, Jian Sun. “YOLOX: Exceeding YOLO Series in 2021”&lt;/em&gt; &lt;em&gt;arXiv preprint arXiv:2107.08430 (2021)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;(본 리뷰의 모든 수식과 그림은 원 논문을 참고했습니다.)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;YOLOX는 간단히 말하면 YOLO v3에 최신 object detection 기법들을 접목시켜 성능을 개선한 것이라고 할 수 있습니다. 그래서 논문 자체도 YOLO v3를 Baseline으로 하고 기법들을 순차적으로 &lt;strong&gt;add-on&lt;/strong&gt; 해나가는 방향으로 저술되어 있습니다. (add-on 한개가 추가될 때마다 성능이 개선됩니다.)&lt;/p&gt;

&lt;p&gt;이제 그 add-on을 하나씩 알아보겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;decoupled-head&quot;&gt;Decoupled head&lt;/h3&gt;

&lt;p&gt;YOLOX에서 적용한 add-on들 중 굉장히 큰 틀이 바뀌었다고 생각하는 지점이 두 가지가 있는데요. 그 중 하나가 바로 &lt;strong&gt;Decoupled head&lt;/strong&gt;입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLOX/decoupled.PNG&quot; alt=&quot;Decoupled head&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;먼저 YOLO의 이전 버전들의 모델들이 object를 어떻게 detect 했는지 잠시 떠올려보죠. 먼저 anchor box를 통해 box boundary안에 object가 있는지 판단합니다. 있다면 detect을 위해 두 가지를 결정해야 하는데요. 먼저 object가 어떤 것인지를(강아지인지, 자동차인지 등) 정의하는 &lt;strong&gt;Classification&lt;/strong&gt; 문제를 풉니다. 나머지 하나는 이미지(혹은 비디오) 내부에 어디까지가 object인지, 즉, box 크기를 어느정도로 가져가야 하는지를 결정하는 &lt;strong&gt;Regression&lt;/strong&gt; 문제를 풉니다. (박스의 width, height 등의 값은 수치이기 때문에 이를 맞추는 것은 regression으로 보는 것 같습니다.) 이전 버전의 모델들은 이 문제들을 하나의 구조, 즉, 하나의 head에서 해결합니다. 그래서 YOLO v3의 경우 output의 dimension이  &lt;strong&gt;N x N x [3 * (4 + 1 + 80)]&lt;/strong&gt; 이었죠! 여기서 4, 1, 80에 각각 regression 문제, object정의, classification 문제에 대한 모델의 답이 담겨있습니다.&lt;/p&gt;

&lt;p&gt;문제는 이렇게 하나의 head에서 여러 문제를 해결하려다 보니 최적의 결과를 내지 못한다는 것인데요. 대표적으로 classification, regression 문제에 하나의 loss를 적용해야하는 문제 등을 들 수 있을 겁니다. 이에 이 두 문제를 따로 판단하고자 등장한 기법이 Decoupled head입니다. 저자들은 YOLO v3에 위 그림과 같이 Decoupled head를 적용하여 따로 따로 답을 적어내게 만들었습니다. 이에 따라서 output도 3가지가 추출되겠네요! ㅎㅎ&lt;/p&gt;

&lt;p&gt;이렇게 Decoupled head 적용을 통해 모델이 적은 epoch을 가지고도 높은 수준의 AP에 더 잘 수렴하게 되는 결과를 가져온다고 합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLOX/decoupled_result.PNG&quot; alt=&quot;Decoupled head로 좋아진 성능&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;strong-data-augmentation&quot;&gt;Strong data augmentation&lt;/h3&gt;

&lt;p&gt;두 번째 add-on, strong data augmentation입니다. 챕터 제목에서도 알 수 있듯이, 데이터를 여러 기법으로 증가시켜 모델의 성능을 좋게 만드는 것입니다. YOLOX에서 적용된 data augmentation기법은 &lt;strong&gt;Mosaic&lt;/strong&gt;와 &lt;strong&gt;MixUp&lt;/strong&gt;입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLOX/mixup_mosaic.png&quot; alt=&quot;MixUp &amp;amp; Mosaic&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;기법 이름과 위 예시 그림에서 어떻게 데이터를 증가시키는지 유추가 가능합니다. Mosaic는 여러 이미지를 격자로 섞어서 data를 만들어내고 MixUp은 투명도를 올려 이미지를 겹침으로써 새로운 data를 만들어내네요!&lt;/p&gt;

&lt;h3 id=&quot;anchor-free&quot;&gt;Anchor-free&lt;/h3&gt;

&lt;p&gt;Decoupled head에 이어 YOLOX의 두 번째 큰 변경점, &lt;strong&gt;Anchor-free&lt;/strong&gt;입니다. YOLO는 redmon의 V1 ~ V3 까지, 그리고 그 후에 나온 V4, V5 또한 Anchor-based로 만들어졌습니다. 그러나 앞선 리뷰에서도 살펴보았듯이 Anchor-based는 여러 문제가 있습니다. 먼저 anchor box의 수부터 위치까지 heuristic하게 지정해주어야 할 부분들이 많습니다. 또한 box의 수들로 인해 모델의 복잡성이 증가하게 되기도 합니다. 이에 최근 object detection에서는 anchor-free 매커니즘이 등장했는데요! YOLOX에서는 YOLO에서도 Anchor-free를 적용했습니다.&lt;/p&gt;

&lt;p&gt;YOLOX에서 사용한 Anchor-free 알고리즘은 &lt;em&gt;FCOS(Fully Convolutional One-Stage object detection)&lt;/em&gt;입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLOX/FCOS.png&quot; alt=&quot;FCOS&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;왼쪽 그림에서 볼 수 있는 것처럼 최초로 object의 center라고 생각되는 지점을 predict 합니다. 이후에 center로부터 t, b, r, l, 즉, object의 경계선까지 box의 크기를 regression합니다. 이미지 내에 object가 두개라면 어떨까요? 만약 모델이 예측한 center point가 두 개의 object를 포함하고 있다면, 미리 정해진 object의 크기에 맞추어 &lt;strong&gt;각각&lt;/strong&gt; box의 크기를 regression할겁니다. 오른쪽 그림이 이를 잘 나타내줍니다. 만약 YOLOX 모델이 오른쪽 그림 속 point를 예측하였을 때, 미리 정해진 “사람”이라는 object의 크기만큼 regression하고 (주황색 box) “테니스채”라는 object크기 정도만큼 regression하여 (파란색 box) 최종적으로 detect할겁니다.&lt;/p&gt;

&lt;h3 id=&quot;simota&quot;&gt;SimOTA&lt;/h3&gt;

&lt;p&gt;그렇다면 YOLOX가 예측한 center point가 &lt;strong&gt;object안에 포함되어 있다&lt;/strong&gt;는 정보를 알아야만 그 point에서 regression을 진행할 수 있을겁니다. 아무 점에서나 다 box를 define한다면 그만큼 복잡해지고 detect 속도는 떨어질테니까요! 여기서 &lt;strong&gt;Object안에 포함되어 있다&lt;/strong&gt;는 것을 &lt;strong&gt;Label Assignment&lt;/strong&gt;라고 표현합니다. Sample data내에서 어떤 point들이 object를 포함하고 있는지를 할당해주는 작업입니다. object안에 존재하는 point를 &lt;em&gt;Positive&lt;/em&gt;, 포함되어 있지 않은 point를 &lt;em&gt;negative&lt;/em&gt;라고 합니다.&lt;/p&gt;

&lt;p&gt;Label assignment에도 여러 가지 방법들이 있습니다. 저자들은 그 중에서 &lt;strong&gt;SimOTA&lt;/strong&gt;를 사용했습니다. SimOTA는 Simple OTA의 줄임말로 OTA method를 simple하게 바꾼 버전이라고 생각하시면 되겠습니다.&lt;/p&gt;

&lt;p&gt;(아무래도 YOLO의 속도에 중점을 두어 simple하게 개량한 것 같습니다.)&lt;/p&gt;

&lt;p&gt;(OTA가 YOLOX 저자가 저술한 또 다른 논문이기 때문에 손쉬웠을 것으로 생각됩니다. &lt;del&gt;그래서 SimOTA를 선택한것 같..&lt;/del&gt; )&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLOX/ota.PNG&quot; alt=&quot;OTA&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;OTA는 Optimal Transport Assignment의 줄인 말로, 이름에서 나타나는 것처럼 Label assignment작업을 Optimal Transport 문제로 정의하고 이를 해결하는 방식으로 진행합니다. Optimal Transport는 간단하게 쿠팡을 생각하시면 편합니다! 물건을 팔고자하는 판매자들이 있고 이를 구매하고자 하는 고객이 있습니다. 여러 판매자와 여러 고객을 가장 효율적으로 matching시켜주는게 Optimal Transport의 개념입니다. (쿠팡의 새벽배송!)&lt;/p&gt;

&lt;p&gt;이를 OTA에서는 Label이 정해지길 바라는 후보 point들이 구매자, pre-defined 되어 있는 Label의 종류들이 구매자라고 보는 것입니다. 위의 그림에서 보면 FPN에서 Label이 할당되기를 기다리고 있는 domain이 세팅됩니다. 세팅을 기다리고 있는 label은 말과 사람, 배경 등 총 5가지네요. 이를 가장 효율적으로 최적화하여 매칭시켜주는 것이 OTA가 할 일입니다. OTA는 이를 &lt;em&gt;Sinkorn-knopp Iteration&lt;/em&gt;으로 해결합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLOX/ota2.png&quot; alt=&quot;OTA와 다른 method 비교&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;OTA는 다른 Label assignment 방법인 ATSS나 PAA에 비하여 경계선에 해당하는 부분에서의 할당이 더 정확하다고 합니다. 위 그림을 보면 여성과 아이가 겹쳐지거나 아이의 팔과 배경이 존재하는 등 애매한 빨간 동그라미 안의 구역에서 OTA는 정확하게 할당하는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;experiment-results&quot;&gt;Experiment Results&lt;/h3&gt;

&lt;p&gt;이렇게 YOLOX에서 추가된 굵직한 add-on들을 살펴봤습니다. 결과를 확인해볼까요?&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLOX/result.png&quot; alt=&quot;result&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;초당 detect하는 frame수를 뜻하는 FPS에서는 YOLO v5에 살짝 밀리긴 하지만 크게 나쁜 정도는 아니며 AP에서는 SOTA를 달성했네요!&lt;/p&gt;

&lt;p&gt;원 논문에서는 YOLO v3뿐 아니라 YOLO v4, v5를 baseline으로 하고 위 add-on 들을 추가한 결과도 제시되어 있으니 관심있으신 분들은 실제 논문을 참고해주세요!&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/2107.08430&quot;&gt;Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, Jian Sun. “YOLOX: Exceeding YOLO Series in 2021”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/ftp/arxiv/papers/1903/1903.08589.pdf&quot;&gt;DC-SPP-YOLO: Dense Connection and Spatial Pyramid Pooling Based YOLO for Object Detection&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1904.01355.pdf&quot;&gt;FCOS: Fully Convolutional One-Stage Object Detection&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2103.14259&quot;&gt;OTA: Optimal Transport Assignment for Object Detection&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Mon, 27 Sep 2021 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/papers/2021/09/27/YOLOX_review.html</link>
        <guid isPermaLink="true">http://localhost:4000/papers/2021/09/27/YOLOX_review.html</guid>
        
        <category>CV</category>
        
        <category>object-detection</category>
        
        <category>yolo</category>
        
        
        <category>Papers</category>
        
      </item>
    
      <item>
        <title>[CV] YOLO v3</title>
        <description>&lt;h1 id=&quot;paper-review-yolo_v3&quot;&gt;[Paper review] YOLO_v3&lt;/h1&gt;

&lt;p&gt;YOLO series의 3번째 버전, &lt;strong&gt;YOLO v3&lt;/strong&gt; 입니다. (이전 버전 리뷰: &lt;a href=&quot;https://decision-j.github.io/computer-vision/2021/05/31/Yolo_review.html&quot;&gt;YOLO v1 review&lt;/a&gt;,  &lt;a href=&quot;https://decision-j.github.io/computer-vision/2021/07/26/Yolo_v2_review.html&quot;&gt;YOLO v2 review&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Object detection에 관련된 여러 competition이나 project를 살펴볼 때 빠짐없이 등장하는 모델이었는데요!&lt;/p&gt;

&lt;p&gt;논문도 아주 짧고 굵어서 호다닥 리뷰해보겠습니다.&lt;/p&gt;

&lt;p&gt;구성은 이전 v2와 마찬가지로 기존 버전에서 어떤 것을 업데이트 했는지 말하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Redmon, Joseph, and Ali Farhadi. “Yolov3: An incremental improvement.”&lt;/em&gt; &lt;em&gt;arXiv preprint arXiv:1804.02767 (2018)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;(본 리뷰의 모든 수식과 그림은 원 논문을 참고했습니다.)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;bounding-box-prediction&quot;&gt;Bounding Box Prediction&lt;/h3&gt;

&lt;p&gt;이전 YOLO v2에서 &lt;strong&gt;Direct location prediction&lt;/strong&gt;이라는 이름의 소챕터로 다루어졌던 내용의 연장입니다. YOLO v2에서의 내용은 Anchor box의 도입에 따라 restriction이 추가된  &lt;strong&gt;$$b_x, b_y, b_w, b_h$$&lt;/strong&gt;의 좌표를 prediction하는 것이었습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLO_v2/direct_location_pred.PNG&quot; alt=&quot;Direct location prediction에서 발췌&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;YOLO v3에서도 비슷합니다. 이 때 Coordinate의 update를 위한 gradient로 $$\hat{t_&lt;em&gt;} - t_&lt;/em&gt;$$, 일종의 SSE를 사용한다고 합니다.&lt;/p&gt;

&lt;p&gt;($$b_*$$를 사용하지 않고 t의 값들을 사용하는 이유가 무엇인지 궁금하네요!)&lt;/p&gt;

&lt;p&gt;또한 YOLO v3에서는 &lt;em&gt;Objectness score&lt;/em&gt;를 계산하여 정확도를 높입니다. Logistic regression을 통해 각각의 Bounding box가 object를 포함할 확률을 예측하는데요.  1이면 ground truth object를 overlap한다고 볼 수 있습니다. 각 Bounding box들 중에서도 IOU가 &lt;strong&gt;가장 큰 box 1개&lt;/strong&gt;만이 실제 object에 할당될 수 있습니다. (대표 박스라고 볼 수 있겠습니다.) Objectness score(아마도 logistic regression의 예측확률?)가 threshold 0.5를 넘었더라도 대표 박스가 되지 못한 개체들의 prediciton은 &lt;strong&gt;무시&lt;/strong&gt;되기 때문에 추후 loss function계산에서 제외됩니다.&lt;/p&gt;

&lt;h3 id=&quot;class-prediction&quot;&gt;Class prediction&lt;/h3&gt;

&lt;p&gt;YOLO v3에서는 class를 prediction하기 위해 &lt;strong&gt;Binary cross-entropy&lt;/strong&gt;를 class 각각에 대해 적용합니다. 이는 일반적으로 softmax를 적용하는 것과는 다른 것인데요. Softmax function을 사용하는 것보다 개별 확률을 예측해주는 것이 &lt;strong&gt;multi-label detection&lt;/strong&gt; (ex. class가 여자(woman)이면서 사람(Person)인 경우)에서 더 좋은 성능을 가져온다고 합니다.&lt;/p&gt;

&lt;h3 id=&quot;darknet-53&quot;&gt;Darknet-53&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLO_v3/darknet53.jpeg&quot; alt=&quot;Darknet-53&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;YOLO v2에서 가장 크게 업그레이드 된 파트 중 하나, &lt;strong&gt;Darknet-53&lt;/strong&gt;입니다. 기존 YOLO v2에서는 19개의 convolutional layer를 가진 Darknet 19를 backbone으로 활용했습니다. 하지만 점차적으로 복잡해지고 좋은 성능을 요구하는 추세에 맞추어 convolutional  layer를 53개로 늘렸습니다. 이 때, layer가 깊어짐에 따라 diminishing effect 등의 부작용을 방지하기 위해 residual network를 중간 중간 섞어 주었습니다. Residual shortcut connection으로 인해 기존 대비 훨씬 깊어진 Convolutional layer를 가질 수 있게 된 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;predictions-across-scales&quot;&gt;Predictions Across Scales&lt;/h3&gt;

&lt;p&gt;YOLO v3에서 darknet-53과 더불어 가장 큰 업데이트이지 않을까 생각됩니다. 바로 prediction box의 scale을 3가지로 나누어 예측한다는 것입니다. 아무래도 region based predict에서 출발한 YOLO였기 때문에 항상 scale에 따른 예측 robust를 불안해하고 신경쓰고 있었는데 이 부분에 대한 보완인 것으로 보입니다. 구체적으로 어떻게 진행하는 지 다음의 도식과 함께 살펴보겠습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLO_v3/darknet53_scale.jpg&quot; alt=&quot;Darknet-53&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;방금 전 살펴본 darknet-53 형태입니다. 빨강, 파랑, 초록의 세 박스를 표시해두었는데요. 이 부분들에서 하나씩 feature map을 뽑아내어 predict을 진행하는 구조입니다. 먼저 빨강 네모에서 32 x 32 feature map을 얻을 수 있습니다. 이를 통해 13 x 13 scale을 가진 box로 predict 하는 것이 가능합니다. (Input size가 416 x 416이므로) 마찬가지로 파란 네모에서는 16 x 16 feature map을 얻을 수 있으며 26 x 26 scale predict이 가능합니다. 마지막으로 초록 네모에서는 8 x 8 feature map을 활용하여 52 x 52 scale predict이 가능하겠네요! 총 &lt;strong&gt;13, 26, 52&lt;/strong&gt; 세 가지 scale의 박스를 통해 object를 detect할 수 있습니다.&lt;/p&gt;

&lt;p&gt;(의문 1: YOLO v2 논문에서 Multi-Scale Training이라는 part로 YOLO는 batch마다 input size를 바꿔가며 train을 진행한다고 배웠습니다. 위의 scale들은 input size, 여기서는 416 x 416이 바뀌면 자연스럽게 바뀌게 되는 값들인데 그렇다면 predict box의 scale이 가변하는 것이 궁금하네요! ㅎㅎ)&lt;/p&gt;

&lt;p&gt;이 때, 각 scale의 prediction값들은 독립적으로 진행되는 것이 아니라 앞 선 결과를 뒤 scale에 반영해주는 식으로 연결되어 있습니다. &lt;strong&gt;Ethan Yanjia Li&lt;/strong&gt;님의 블로그에 이를 잘 나타내주는 diagram이 있어 가져왔습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLO_v3/multi_scale.jpeg&quot; alt=&quot;Different scale predict&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;위 diagram을 살펴보시면 먼저 13 x 13 feature vector가 output산출을 위해 Fully Convolutional Network를 탈 때, 중간에 그 결과를 뽑아내어 &lt;strong&gt;upsample&lt;/strong&gt;해줍니다. (x2배) 이 vector를 상위 scale, 즉, 26 x 26 feature vector와 &lt;strong&gt;concatenate&lt;/strong&gt;해서 다음 FCN을 진행합니다. 마지막 52 x 52도 마찬가지입니다. 이를 통해  &lt;strong&gt;meaningful semantic information&lt;/strong&gt;과 &lt;strong&gt;finer-grained information&lt;/strong&gt;을 반영할 수 있다고 저자는 밝히고 있습니다.&lt;/p&gt;

&lt;p&gt;Output 형태는 &lt;strong&gt;N x N x [3 * (4 + 1 + 80)]&lt;/strong&gt;입니다. N은 image의 pixel 정보이고 3은 사용할 box의 수입니다. (본 논문에서는 COCO dataset에 대해 3개의 box를 사용하고 있습니다.) 4는 box의 offset, 1은 objectness prediction, 80은 class predict 정보입니다. 각각의 scale box에 대해서 총 3개의 output을 얻을 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;experiment-results&quot;&gt;Experiment Results&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLO_v3/result.png&quot; alt=&quot;result&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;YOLO v3의 result결과입니다. (plot을 상당히 개성적으로 그리셨습니다 ㅎㅎ) YOLO v3는 mAP 50일 때 가장 좋은 결과를 보여줍니다. (cf. overall mAP 등으로 측정했을 때는 정확도가 조금 떨어지는 것을 확인할 수 있는데요. 저자는 어차피 인간이 object를 판별할 때도 mAP 30이나 50이나 큰 차이를 느끼지 못한다고 언급하며 mAP 50에서의 성능이 좋다면 상관없다고 밝히고 있습니다.) 여전히 타 알고리즘에 비해 현저히 빠른 속도와 준수한 정확도를 보여주는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;(의문 2: result plot을 보면 YOLOv3-320, 416, 608로 되어있는 것을 확인할 수 있는데요. Input size의 크기를 의미하는 것 같습니다. 앞 서 의문 1과 연계하여 320과 608은 predict box scale이 달라진 걸까요..?)&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/1804.02767&quot;&gt;Redmon, Joseph, and Ali Farhadi. “Yolov3: An incremental improvement.” &lt;em&gt;arXiv preprint arXiv:1804.02767&lt;/em&gt; (2018)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/dive-really-deep-into-yolo-v3-a-beginners-guide-9e3d2666280e&quot;&gt;YOLO v3 참고 블로그&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://bestinau.com.au/yolov3-architecture-best-model-in-object-detection/&quot;&gt;YOLO v3 참고 블로그 2&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://machinelearningmastery.com/how-to-perform-object-detection-with-yolov3-in-keras/&quot;&gt;YOLO v3 구현 in keras&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Tue, 03 Aug 2021 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/papers/2021/08/03/Yolo_v3_review.html</link>
        <guid isPermaLink="true">http://localhost:4000/papers/2021/08/03/Yolo_v3_review.html</guid>
        
        <category>CV</category>
        
        <category>object-detection</category>
        
        <category>yolo</category>
        
        
        <category>Papers</category>
        
      </item>
    
      <item>
        <title>Narrow Confidence Interval for low N with small p</title>
        <description>&lt;p&gt;최근 Proportion 예측치에 대한 신뢰구간에 대해 생각해 볼 기회가 있었습니다. 어떤 완성된 제품의 판매 이후 기간 내 &lt;strong&gt;누적 불량률&lt;/strong&gt;에 대해 예측하는 업무에 참여했는데요! 이 때 점 추청치도 물론 중요하지만, &lt;strong&gt;구간 추청치&lt;/strong&gt;에 대한 관심도 이에 못지 않은 것 같습니다. 범위로 표현되다보니 어느 정도의 규모의 불량률이 발생할 수 있는가에 대한 관심이라고 생각합니다.&lt;/p&gt;

&lt;p&gt;이때 문제가 되었던 건 제품의 출시 초기, 즉, 누적 불량률 proportion에서 &lt;strong&gt;n이 적을 때&lt;/strong&gt;의 신뢰 구간이 &lt;em&gt;매우 크게 벌어진다&lt;/em&gt;는 것입니다. 따라서 예측치로서의 가치가  퇴색되는 문제가 있습니다. 이에 초기 불량률 예측에서도 나름대로 적정한 범위를 갖는 신뢰구간을 제시하기 위해 고민해본 결과를 정리해보겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;problem-setting&quot;&gt;Problem setting&lt;/h3&gt;

&lt;p&gt;앞 서 간단하게 설명한 문제를 구체적인 토이 데이터를 가지고 정의해보겠습니다. (본 포스팅에서 제시되는 데이터는 실제 데이터가 아니며 실제 데이터의 특성을 반영하여 임의로 생성된 데이터입니다.)&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;timestamp&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;x_cumulative&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;n_cumulative&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;proportion&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;21-04-01 0시&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;37&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;21-04-01 6시&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;76&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.09&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;21-04-01 12시&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;123&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.09&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;21-04-01 18시&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;168&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;21-04-02 0시&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;216&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.09&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;21-04-02 6시&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;25&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;264&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;…&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;…&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;…&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;…&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/statistics/CI_lowN/data.png&quot; alt=&quot;누적 불량률&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;위 데이터는 2021년 4월 1일부터 4월 8일까지 1주일간 6시간에 한번씩 제품의 &lt;strong&gt;누적 불량률 예측치&lt;/strong&gt;를 기록한 것입니다. x는 불량이 일어난 제품의 수를 뜻하고 n은 출시된 전체 제품 수입니다. proportion은 이들의 비율로서 불량률에 해당하며 테이블에 기록된 내용들을 전부 어떤 모델로 예측했다고 가정해보겠습니다.&lt;/p&gt;

&lt;p&gt;예측된 누적 불량률의 신뢰구간을 구해보겠습니다. 우선 가장 기본인 정규 근사를 이용한 비율 신뢰구간을 적용해보겠습니다.&lt;/p&gt;

&lt;p&gt;$$ CI_{normal} = \hat{p} \pm z\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} $$&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/statistics/CI_lowN/CI.png&quot; alt=&quot;정규근사 신뢰구간&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;서두에서 말씀드린 것처럼 4월 1일의 &lt;em&gt;초기 예측 불량률에 대한 구간&lt;/em&gt;이 위아래로 크게 벌어진 것을 확인할 수 있습니다. 이후 4월 3일 경부터 n이 누적되어 커지면서 구간이 안정을 찾는 것을 확인할 수 있네요! 저는 저 초기 불량률에 대한 신뢰구간을 적정하게 줄여서 활용 가능성을 높이고 싶었습니다.&lt;/p&gt;

&lt;p&gt;그래서 생각한 것이 예측된 값의 특성을 반영해주면 좋겠다는 것이었습니다. 불량률의 모습을 살펴보시면 p가 작음을 확인할 수 있습니다. 불량률이나 연체율 등의 &lt;em&gt;나쁜 비율&lt;/em&gt;들에서 흔히 나타나는 imbalance 특성인데요! 이를 반영하여 구간을 도출하면 좀 더 개선된 구간이 도출될 수 있지 않을까 생각했습니다.&lt;/p&gt;

&lt;p&gt;여기까지 진행되었을 때 떠올랐던 것이 바로 &lt;strong&gt;Bayesian Credible Interval&lt;/strong&gt;이었습니다. Bayesian Interval은 Bayesian inference를 통해서 posterior의 분포를 이용하여 p의 interval을 계산해주는 방법입니다. 이 때 prior를 통해서 불량률의 사전 정보를 반영해줄 수 있으니 low p에 대한 정보를 반영할 수 있다고 생각했습니다!&lt;/p&gt;

&lt;h3 id=&quot;jeffreys-prior&quot;&gt;Jeffrey’s Prior&lt;/h3&gt;

&lt;p&gt;우선 Bayesian Interval 중에서 가장 유명한 &lt;strong&gt;Jeffrey’s prior&lt;/strong&gt;부터 적용해보겠습니다. Jeffrey prior는 $$Beta(1/2, 1/2)$$을 prior로 주는 것인데요. Beta분포 자체가 0~1 사이의 proportion에 대한 정보를 담고 있는 분포니까 prior로서 적합합니다.&lt;/p&gt;

&lt;p&gt;(* 사실 Bayesian Credible Interval은 Confidence Interval과 살짝 다른 개념이긴 하지만 대체로 비슷하므로 그냥 사용하겠습니닷)&lt;/p&gt;

&lt;p&gt;(* low p를 반영해준다고 하고 jeffrey를 먼저 적용하는 건 말이 안되지만 (뒤에서 설명) 비교를 위해서 먼저 적용해보았습니닷)&lt;/p&gt;

&lt;p&gt;Interval을 구하는 수식과 실제 계산 결과는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;$$
\begin{gathered}
CI_{jeffrey} = [Beta(\alpha/2; x+1/2, n-x+1/2),Beta(1-\alpha/2; x+1/2, n-x+1/2)]
\end{gathered}
$$&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/statistics/CI_lowN/jeffrey.png&quot; alt=&quot;jeffrey 신뢰구간&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;분홍색으로 구해진 것이 jeffrey prior를 통해 구한 Interval인데요. 기존 신뢰구간에 비해 살짝 작아지긴 했지만 여전히 초기 불량률에 대한 구간이 벌어져있네요 ㅠㅠ 제가 생각한 Jeffrey prior의 문제는 다음과 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/statistics/CI_lowN/jeffrey_prior.png&quot; alt=&quot;jeffrey prior&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Jeffrey prior의 모습입니다. Low p에 대한 density도 높지만 Large p에 대한 density도 높은 것을 확인할 수 있습니다. 저희의 불량률 데이터는 large p가 나올 수 없는(정확히는 나오면 안되는 ^^;;) 형태이기 때문에 prior의 형태를 수정할 필요가 있어보입니다!&lt;/p&gt;

&lt;h3 id=&quot;modified-prior&quot;&gt;Modified Prior&lt;/h3&gt;

&lt;p&gt;그래서 prior로 사용되고 있는 Beta 분포에서 parameter를 바꿔줬습니다. Jeffrey의 0.5, 0.5가 아니라 &lt;strong&gt;0.05, 35&lt;/strong&gt;를 parameter로 갖는 beta 분포를 prior로 사용해보겠습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/statistics/CI_lowN/mod_prior.png&quot; alt=&quot;Modified prior&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;수정된 prior의 모습은 위와 같습니다. small p에 대한 density만이 높게 잡히는 형태로 되어있음을 확인할 수 있습니다! 이러한 사전 정보를 가지고 Interval을 계산하면 수식이 다음과 같이 변경됩니다.&lt;/p&gt;

&lt;p&gt;$$
\begin{gathered}
CI_{mod} = [Beta(\alpha/2; x+0.05, n-x+35),Beta(1-\alpha/2; x+0.05, n-x+35)]
\end{gathered}
$$&lt;/p&gt;

&lt;p&gt;이제 준비는 끝났습니다! 계산된 Interval의 결과를 보시죠!&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/statistics/CI_lowN/CI_mod.png&quot; alt=&quot;Modified 신뢰구간&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;초록색 선으로 구해진 것이 modified Interval입니다. 기대 했던대로 다른 구간들에 비해 초기 불량률에 대해 작은 구간으로 define되었음을 확인할 수 있습니다!&lt;/p&gt;

&lt;h3 id=&quot;limitation&quot;&gt;Limitation&lt;/h3&gt;

&lt;p&gt;간단하게 small p에 대한 사전 정보를 반영하여 low n의 신뢰 구간을 좁히는 방법을 생각해보았습니다. 이 방법의 개선해야할 부분, 더 생각해볼 지점은 다음과 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;prior의 parameter 세팅&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;modified Interval의 prior는 $$Beta(0.05, 35)$$로 되어있습니다. 이 때 0.05, 35는 어떻게 정해졌을까요? 네, 제가 임의로 정했습니다 ㅎㅎ 하지만 모든 데이터에 대해 임의로 사람이 지정해줄 수는 없는 노릇이니, 이 부분을 결정하는 logic이 필요해보입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;굳이 신뢰구간을 좁혀야 하는가?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;“신뢰 구간”이라는 것 자체가 n이 커지면 자연히 좁아지고 n이 작으면 넓어지는 것이 &lt;strong&gt;당연&lt;/strong&gt;합니다. 이는 통계적 추론에서 수반되는 불확실성의 반영이기도 합니다. 따라서 n이 작은 기간에 신뢰 구간을 좁히는 것이 &lt;em&gt;필요한&lt;/em&gt; 작업인지에 대해서는 업무 내에서의 활용 방안, 분석의 목적에 맞추어 판단해야 될 부분으로 보입니다.&lt;/p&gt;

&lt;p&gt;결론적으로 이 방법론은 무조건적인 적용이 아니라 (모든 것이 그렇지만) 때와 장소에 따라서 분석자가 적절히 적용하여 활용하는 것이 가장 좋겠다는 의견입니다! ㅎㅎ&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval&quot;&gt;Wikipedia “Binomial_proportion_confidence_interval”&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://projecteuclid.org/journals/annals-of-statistics/volume-30/issue-1/Confidence-Intervals-for-a-binomial-proportion-and-asymptotic-expansions/10.1214/aos/1015362189.full&quot;&gt;Brown, Lawrence D., T. Tony Cai, and Anirban DasGupta. “Confidence intervals for a binomial proportion and asymptotic expansions.” &lt;em&gt;The Annals of Statistics&lt;/em&gt; 30.1 (2002): 160-201.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://stats.stackexchange.com/questions/227107/bayesian-confidence-interval-jeffreys-prior-other-than-the-0-5-centroid&quot;&gt;StackExchange about Jeffrey’s prior&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Wed, 28 Jul 2021 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/statistics/2021/07/28/Narrow-Confidence-Interval-for-low-N-with-small-p.html</link>
        <guid isPermaLink="true">http://localhost:4000/statistics/2021/07/28/Narrow-Confidence-Interval-for-low-N-with-small-p.html</guid>
        
        <category>Statistics</category>
        
        <category>Confidence</category>
        
        <category>Interval</category>
        
        <category>proportion</category>
        
        <category>CI</category>
        
        
        <category>Statistics</category>
        
      </item>
    
      <item>
        <title>[CV] YOLO v2</title>
        <description>&lt;h1 id=&quot;paper-review-yolo_v2&quot;&gt;[Paper review] YOLO_v2&lt;/h1&gt;

&lt;p&gt;이전 포스팅의 &lt;a href=&quot;https://decision-j.github.io/computer-vision/2021/05/31/Yolo_review.html&quot;&gt;YOLO v1&lt;/a&gt;에 후속 버전인 YOLO v2에 대해 리뷰해보고자 합니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Redmon, Joseph, and Ali Farhadi. “YOLO9000: better, faster, stronger.” Proceedings of the IEEE conference on computer vision and pattern recognition (2017)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;본 논문의 구성은 독특하게도 &lt;strong&gt;Better, Faster, Stronger&lt;/strong&gt;의 세 파트로 구성되어 있습니다.&lt;/p&gt;

&lt;p&gt;각각 어떻게 이전 버전을 업데이트하고 발전시켰는 지를 소개하고 있는데요. 하나씩 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;(본 리뷰의 모든 수식과 그림은 원 논문을 참고했습니다.)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;better&quot;&gt;Better&lt;/h3&gt;

&lt;p&gt;제목에서부터 느낌이 오듯이 &lt;strong&gt;Better&lt;/strong&gt; 파트에서는 이전 YOLO v1의 &lt;u&gt;성능 개선&lt;/u&gt;이 주요 포인트입니다.  저자는 YOLO v1의 주요 문제점으로 두 가지를 꼽는데요, &lt;em&gt;Localization 오류&lt;/em&gt;와 &lt;em&gt;Low recall&lt;/em&gt;입니다. YOLO v2에서는 이를 해결하기 위한 다양한 방법들이 소개되고 있습니다. 그 중에서도 주요 포인트들만 공부해보겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Convolutional With Anchor Boxes&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;YOLO에서는 output을 위한 final layer가 fully connected layer였습니다. YOLO v2에서는 이를 Faster R-CNN 등 다른 모델들과 같이 Convolutional layer로 바꿨습니다. 또한 각 region domain에서 object class를 정했었는데 이를 Anchor box를 도입하여 대체했습니다. Anchor box는 임의로 배치된 box들의 정보를 통해 도움을 받아 object를 detection하는 방법입니다. YOLO에서는 v2에서 최초로 적용되었네요! 이는 지난 포스팅에서도 언급했었던 YOLO v1의 regional based approach의 한계점을 개선하는 효과가 있습니다.&lt;/p&gt;

&lt;p&gt;YOLO v2는 Anchor box도입으로 정확성 측면에선 다소 떨어졌지만 (기존 region based predict에서 보다 box의 숫자들이 상당히 많이 늘어나기 때문에 accuracy 측면에선 불리합니다.) &lt;u&gt;recall 측면&lt;/u&gt;에서 81%에서 &lt;strong&gt;88%&lt;/strong&gt;까지 개선이 이루어졌습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Dimension Clusters&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anchor box를 도입하고나니 문제가 생겼습니다. 얼마나 많은 Anchor box를 설정할지, 어떤 위치에 설정할지를 결정해야 하는 것이죠! 기존의 모델들은 사람이 임의로 &lt;em&gt;hand-picked&lt;/em&gt;의 방식으로 이를 설정해주었습니다. 본 논문에서는 training set의 box들의 centroid 좌표를 이용하여 &lt;strong&gt;K-means clustering&lt;/strong&gt;을 통해 Anchor box의 사전 정보(논문에서 prior)를 결정해줍니다.&lt;/p&gt;

&lt;p&gt;이 때, 기존의 K-means처럼 Euclidean distance를 이용하면 아무래도 면적이 큰, centroid와 박스 coordinate의 거리가 큰 box들은 문제가 생길 여지가 많기 때문에 custom distance를 사용합니다. 이를 &lt;em&gt;Average IOU&lt;/em&gt;라 부릅니다.&lt;/p&gt;

&lt;p&gt;$$
\begin{gathered}
d(box, centroid) = 1-IOU(box, centroid)
\end{gathered}
$$&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLO_v2/kmeans.PNG&quot; alt=&quot;K 결정하기&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;위 플랏을 보시면 k가 5일 때 비교적 높은 Avg. IOU를 기록함을 확인할 수 있습니다. 저자들은 model의 complexity를 낮추면서 가장 높은 IOU를 담보할 수 있는 k값이 5라고 생각하고 Anchor box의 사전 개수를 &lt;strong&gt;5개&lt;/strong&gt;로 결정했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Direct location prediction&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anchor box 도입은 두 번째 문제를 가져옵니다. 바로 모델이 &lt;strong&gt;instable&lt;/strong&gt; 할 수 있다는 것입니다. Model은 box의 $$(x,y)$$ 좌표를 예측하게 되는데요. 이 때 아무런 restriction이 없다면 초기값에 따라 매우 불안정하게 box의 위치가 예측되고 (image domain 내에서 큰 변동) 이에 따라 모델이 stable한 결과를 내는데 불리해집니다.&lt;/p&gt;

&lt;p&gt;이에 본 논문에서는 아래와 같은 계산을 통해 일종의 restriction을 만듭니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLO_v2/direct_location_pred.PNG&quot; alt=&quot;Direct location prediction&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;여기서 $$t_x, t_y, t_w, t_h, t_o$$가 model에서 예측치로 나오는 값들인데요! logistic activation ($$\sigma$$) 등을 통해서 &lt;strong&gt;$$b_x, b_y, b_w, b_h$$&lt;/strong&gt;로 묶어둠으로써 model의 prediction을 stable하게 유지될 수 있도록 세팅합니다.&lt;/p&gt;

&lt;p&gt;저자는 Dimension Clusters와 Direct location prediction 방법을 통해 &lt;strong&gt;5%의 mAP(정확성 지표) 개선&lt;/strong&gt;을 얻었다고 설명합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Multi-Scale Training&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;YOLO v2는 다양한 size의 이미지들을 robust하게 학습하기 위해서 Multi-Scale training을 적용했습니다. 우선 위에서 살펴본 Anchor Box의 도입의 영향으로 YOLO v2의 &lt;em&gt;기본 input size&lt;/em&gt;는 기존 448 x 448에서  416 x 416으로 바뀝니다. 하지만 Multi-Scale training은 매 iteration의 layer input을 &lt;strong&gt;고정하지 않고&lt;/strong&gt; 매 10번의 batch가 지날 때마다 320 x 320에서 608 x 608 까지의 32간격으로 input size를 바꾸어 train을 진행합니다. 이렇게 하면 다양한 size에도 robust한 학습이 가능하며, 다양한 resolution의 이미지에 대해서도 학습이 가능하다는 장점이 있습니다.&lt;/p&gt;

&lt;p&gt;이 포스팅에서 다 다루지는 않았지만 이 외에도 Batch-normalization, Passthrough layer 등을 통해 YOLO v1 대비 mAP를 개선시켰습니다. VOC2007 mAP기준 63.4에서 &lt;strong&gt;78.6&lt;/strong&gt;까지 높아졌음을 알 수 있네요!&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLO_v2/mAP_up.PNG&quot; alt=&quot;YOLO v2 짱짱 than YOLO v1&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;faster&quot;&gt;Faster&lt;/h3&gt;

&lt;p&gt;안그래도 기존 detection method들 대비 속도가 뛰어났던 YOLO v1이었는데 &lt;em&gt;Faster&lt;/em&gt;라니.. 어떻게 했을지 확인해보겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Darknet-19&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;YOLO v2가 발표될 당시 많은 detection method들은 base feature extractor scheme으로 VGG-16을 많이 사용하고 있었습니다. VGG-16은 계산량이 많기 때문에 빠른 스피드를 담보할 수 없는데요. 이에 저자들은 &lt;em&gt;Googlenet architecture&lt;/em&gt;를 기반으로 독자적인 custom model을 만들어 사용했습니다. 그 것이 바로 &lt;strong&gt;Darknet-19&lt;/strong&gt;입니다.&lt;/p&gt;

&lt;p&gt;Darkent 19는 &lt;strong&gt;19 개의 convolutional layer와 5개의 maxpooling layer&lt;/strong&gt;로 이루어진 구조를 가지고 있습니다. 저자는 이를 통해 정확도는 90.0% (VGG-16) 에서 88.0%로 매우 미세하게 줄어들었지만, 계산량 자체는 30.69 billion에서 8.52 billion으로 매우 줄어들어 훨씬 빠른 속도를 담보할 수 있다고 설명합니다.&lt;/p&gt;

&lt;p&gt;이 외에도 &lt;strong&gt;Classification과 Detection을 위한 train과정을 각각 진행&lt;/strong&gt;함으로써 model의 학습 및 예측 속도를 높였습니다.&lt;/p&gt;

&lt;h3 id=&quot;stronger&quot;&gt;Stronger&lt;/h3&gt;

&lt;p&gt;Stronger 파트는 제가 이 논문을 읽으면서 가장 흥미롭게 느꼈던 부분입니다. 앞서 Faster 파트에서 YOLO v2는 Classification과 Detection 과정을 따로 train한다고 언급했는데요! Classification을 training하는 ImageNet 1000 class classification dataset은 이름에서 알 수 있듯이 &lt;strong&gt;1,000개의 class&lt;/strong&gt;가 존재합니다. 이에 비해 detection dataset은 &lt;strong&gt;20개의 class&lt;/strong&gt;만 존재하기 때문에 matching이 되지 않죠! 이 부분을 해결해주는 파트가 Stronger입니다.&lt;/p&gt;

&lt;p&gt;Class의 차이가 난다는 것이 어떤 문제점을 가져오는지 논문에서 예시로 설명하고 있습니다. 예를 들어, detection dataset에는 &lt;em&gt;“DOG”&lt;/em&gt;이라는 class밖에 없는데, classification dataset에는 &lt;em&gt;“Norfolk terrier”, “Yorkshire terrier”,  “Bedlington terrier”&lt;/em&gt; 등의 보다 세분화된 class들이 존재한다는 것이죠! 이러한 문제를 해결해주기 위해서 저자는  &lt;strong&gt;Hierarchical tree&lt;/strong&gt;를 사용합니다. Hierarchical tree의 개념은 아래 그림을 보면 좀 더 쉽게 알 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLO_v2/tree.PNG&quot; alt=&quot;Hierarchical Word Tree&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;가장 상위 node에 Pyhsical object라고 하는 가장 큰 개념이 속해져있고 그 아래로 카테고리가 나눠지면서 단어들이 분류되어 위치하는 것을 확인할 수 있습니다. 이렇게 Tree를 만드는 목적 중 가장 중요한 것은 &lt;strong&gt;Probability&lt;/strong&gt;를 계산할 수 있게 된다는 것입니다. Tree를 사용하지 않는다면 Classification object에서 &lt;em&gt;Norfolk terrier&lt;/em&gt;가 나왔을 때 detection probability가 어떻게 계산될 지 알 수 없습니다. 하지만 tree를 이용한다면 root node로부터의 &lt;strong&gt;조건부 확률&lt;/strong&gt;을 이용하여 간접적으로 확률을 구할 수 있게 됩니다!&lt;/p&gt;

&lt;p&gt;$$
P(\textit{Norfolk terrier})=P(\textit{Norfolk terrier }|\textit{ terrier}) * P(\textit{terrier }|\textit{ dog}) *\cdot\cdot\cdot * P(\textit{animal } | \textit{Physical Object})
$$&lt;/p&gt;

&lt;p&gt;위의 수식을 보면 한결 이해하기가 쉽습니다. 이 때 제일 꼭대기 노드인 $$P(\textit{Physical Object})=1$$임을 가정합니다. 이런 식으로 구성된 확률 값을 가지고 dataset이 classification이라면 architecture의 classification 부분의 loss function으로 update하고 detection이라면 전체 모델의 loss function을 가지고 update해주게 됩니다.&lt;/p&gt;

&lt;p&gt;이러한 class의 matching 및 확장은 굉장히 흥미로웠습니다. class의 규모가 다른 dataset들을 가지고 얼마든지 병합해서 training을 진행할 수 있다는 점에서 좋은 아이디어라고 느꼈습니다.&lt;/p&gt;

&lt;h3 id=&quot;experiment-results&quot;&gt;Experiment Results&lt;/h3&gt;
&lt;p&gt;이러한 노력들로 YOLO v2가 얼마나 업그레이드 되었는지 수치로 비교한 결과를 살펴보겠습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLO_v2/result.PNG&quot; alt=&quot;Performance comparison with other detectors from PASCAL VOC 2007, 2012&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;전작인 YOLO v1의 63.6 mAP에 비하여 상당히 발전된 &lt;strong&gt;78.6&lt;/strong&gt;을 기록한 것을 확인할 수 있네요! FPS 속도도 45에서 &lt;strong&gt;40&lt;/strong&gt;으로 한결 더 빨라졌습니다!&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/pdf/1612.08242.pdf&quot;&gt;Redmon, Joseph, and Ali Farhadi. “YOLO9000: better, faster, stronger.” Proceedings of the IEEE conference on computer vision and pattern recognition (2017)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://taeu.github.io/paper/deeplearning-paper-yolov2/&quot;&gt;https://taeu.github.io/paper/deeplearning-paper-yolov2/&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Mon, 26 Jul 2021 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/papers/2021/07/26/Yolo_v2_review.html</link>
        <guid isPermaLink="true">http://localhost:4000/papers/2021/07/26/Yolo_v2_review.html</guid>
        
        <category>CV</category>
        
        <category>object-detection</category>
        
        <category>yolo</category>
        
        
        <category>Papers</category>
        
      </item>
    
      <item>
        <title>[Kaggle] Google AI Open Images - Object Detection Track</title>
        <description>&lt;h2 id=&quot;kaggle-study-2---google-ai-open-images---object-detection-track&quot;&gt;Kaggle Study #2. - Google AI Open Images - Object Detection Track&lt;/h2&gt;

&lt;p&gt;Object Detection에 대한 논문을 읽어가면서 이를 실제 데이터로 적용해보고 싶다는 생각에 과거 kaggle competition을 찾아보았습니다. 그래서 발견한 것이 2018년 진행된  &lt;em&gt;Google AI Open Images - Object Detection Track&lt;/em&gt; 이었습니다. 3년전 대회이긴 하지만 discussion들이나 코드들을 참고하면서 공부할 목적으로 선택해보았습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;대회-overview&quot;&gt;대회 Overview&lt;/h3&gt;
&lt;p&gt;먼저 &lt;em&gt;Google AI Open Images - Object Detection Track&lt;/em&gt;에 대해 간단히 정리해보겠습니다.
본 대회는 99,999장의 test 이미지 파일에 대해서 object detection을 수행하는 것이 목적입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/kaggle_google_object_detection/overview.png&quot; alt=&quot;Example annotations: the house by anita kluska&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Detecting을 제대로 해냈는지 평가하는 지표로 3가지를 살펴봅니다. &lt;strong&gt;Detecting한 object의 category, Box의 좌표, Confidence&lt;/strong&gt;입니다. 즉, 이미지 내의 사물을 인식하고, 그 위치를 정확히 포착하고, 이에 대한 예측 확률이 높아야 좋은 rating을 받는 것이죠!&lt;/p&gt;

&lt;p&gt;이미지 데이터에 대한 자세한 설명은 &lt;a href=&quot;https://storage.googleapis.com/openimages/web/challenge.html&quot;&gt;Open Images Challenge page&lt;/a&gt;에서 확인할 수 있습니다!&lt;/p&gt;

&lt;h3 id=&quot;how-to&quot;&gt;How to?&lt;/h3&gt;
&lt;p&gt;이 문제에 적용해보고 싶은 모델은 YOLO v3입니다.
최근 YOLO 논문들을 version별로 살펴보고 있는데 실제로 얼마나 빠르고 정확한지 확인해보고 싶었거든요!&lt;/p&gt;

&lt;p&gt;여러 구현 방법들이 있지만 제가 선택한 방법은 &lt;strong&gt;darknet&lt;/strong&gt;을 이용하는 것입니다. 실제 대회 참가가 아니고 우수 코드들을 보면서 공부하는 것이 목적이기 때문에, 우선 제 결과물은 빠르게 내는 것이 중요하다고 생각했습니다. &lt;strong&gt;darknet&lt;/strong&gt;은 open source neural network framework로 &lt;strong&gt;C와 CUDA&lt;/strong&gt;를 기반으로 짜여져 있어 매우 빠르면서도 코드를 파이썬에서도 쉽게 돌릴 수 있기 때문에 선택하게 되었습니다.&lt;/p&gt;

&lt;p&gt;코드는 &lt;a href=&quot;https://github.com/AlexeyAB/darknet&quot;&gt;AlexeyAB&lt;/a&gt;의 darknet코드 깃헙을 clone하여 사용했습니다. 그리고 darknet을 make하기 전에 제 환경에 맞게 &lt;strong&gt;두 가지 customizing&lt;/strong&gt;을 해주었습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Makefile 수정&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;깃헙 코드를 clone하면 그 안에 Makefile이라는 이름의 파일이 생성됩니다. 여러 옵션들을 지정할 수 있는 파일인데요. 저는 &lt;em&gt;GPU, CUDNN, CUDNN_HALF, OPENCV&lt;/em&gt; 항목을 1로 바꾸어 주었습니다. (기존은 0) 이렇게 해야 GPU를 사용하여 detecting 할 수 있거든요!&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;detector.c 수정&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;src 폴더 내의 detector.c라고 하는 c언어 코드를 일부 수정했습니다. AlexeyAB의 darknet 실행 코드는 몇 가지 옵션을 지정할 수 있습니다. 그 중에서 &lt;em&gt;save_labels&lt;/em&gt;를 추가하면 detecting한 box의 정보들을 저장해줍니다. 이 때, detecting한 class의 id와 box의 가로 세로 정보가 default로 저장됩니다. 대회의 제출물에 필요한 정보는 class_id, xmin, ymin, xmax, ymax, confidence 값이기 때문에 이 정보들이 저장되도록 코드를 수정했습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고&lt;/strong&gt;: detector.c를 수정하는 과정에서 &lt;em&gt;conflicting types for&lt;/em&gt; 오류가 나면서 &lt;em&gt;detecor.o를 build하는 데 실패&lt;/em&gt;하는 문제가 나타났습니다. 리눅스에서 종종 발생하는 문제라는데.. 이리저리 찾아보면서 헤맸는데 갑작스럽게 저절로 해결되었네요;; 지속적으로 이런 문제가 발생한다면 뭔가 조치가 필요할 것 같습니다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;// pseudo labeling concept - fast.ai
if (save_labels){
    char labelpath[4096];
    replace_image_to_label(input, labelpath);

    FILE* fw = fopen(labelpath, &quot;wb&quot;);
    int i;
    for (i = 0; i &amp;lt; nboxes; ++i) {
        char buff[1024];
        int class_id = -1;
        float prob = 0;
        for (j = 0; j &amp;lt; l.classes; ++j) {
            if (dets[i].prob[j] &amp;gt; thresh &amp;amp;&amp;amp; dets[i].prob[j] &amp;gt; prob) {
                prob = dets[i].prob[j];
                class_id = j;
            }
        }
        if (class_id &amp;gt;= 0) {
            sprintf(buff, &quot;%d %2.4f %2.4f %2.4f %2.4f %2.4f\n&quot;, class_id,
            prob, #add for confidence
            dets[i].bbox.x - dets[i].bbox.w / 2., #add for xmin
            dets[i].bbox.y - dets[i].bbox.h / 2., #add for ymin
            dets[i].bbox.x + dets[i].bbox.w / 2., #add for xmax
            dets[i].bbox.y + dets[i].bbox.h / 2.); #add for ymax
            fwrite(buff, sizeof(char), strlen(buff), fw);
        }
    }
    fclose(fw);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;이제 다 됐습니다. 아래 실행 코드를 통해 test 이미지들에 대한 detecting을 수행하기만 하면 됩니다!&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;darknet&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;detector&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cfg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;openimages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cfg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yolov3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;openimages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cfg&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yolov3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;openimages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dont_show&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;txt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;txt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_labels&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;잘 진행되는지 궁금하니까 한 장에 대해서만 수행 결과를 확인해보도록 하겠습니다. 아래 사진은 99,999장의 test 이미지 중 한 장을 가져온 것입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/kaggle_google_object_detection/test.jfif&quot; alt=&quot;test image; 3bc4e7965d04d91f.jpg&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;위 사진을 가지고 실행코드를 돌려보면 아래와 같은 실행 결과를 얻을 수 있습니다.&lt;/p&gt;

&lt;pre style=&quot;auto&quot;&gt;
&lt;code&gt;
  /content/darknet
   CUDA-version: 11000 (11020), cuDNN: 7.6.5, CUDNN_HALF=1, GPU count: 1  
   CUDNN_HALF=1
   OpenCV version: 3.2.0
   0 : compute_capability = 600, cudnn_half = 0, GPU: Tesla P100-PCIE-16GB
  net.optimized_memory = 0
  mini_batch = 1, batch = 1, time_steps = 1, train = 0
     layer   filters  size/strd(dil)      input                output
     0 Create CUDA-stream - 0
   Create cudnn-handle 0
  conv     32       3 x 3/ 1    608 x 608 x   3 -&amp;gt;  608 x 608 x  32 0.639 BF
     1 conv     64       3 x 3/ 2    608 x 608 x  32 -&amp;gt;  304 x 304 x  64 3.407 BF
     2 conv     32       1 x 1/ 1    304 x 304 x  64 -&amp;gt;  304 x 304 x  32 0.379 BF
     3 conv     64       3 x 3/ 1    304 x 304 x  32 -&amp;gt;  304 x 304 x  64 3.407 BF
     4 Shortcut Layer: 1,  wt = 0, wn = 0, outputs: 304 x 304 x  64 0.006 BF
     5 conv    128       3 x 3/ 2    304 x 304 x  64 -&amp;gt;  152 x 152 x 128 3.407 BF
     6 conv     64       1 x 1/ 1    152 x 152 x 128 -&amp;gt;  152 x 152 x  64 0.379 BF
     7 conv    128       3 x 3/ 1    152 x 152 x  64 -&amp;gt;  152 x 152 x 128 3.407 BF
     8 Shortcut Layer: 5,  wt = 0, wn = 0, outputs: 152 x 152 x 128 0.003 BF
     9 conv     64       1 x 1/ 1    152 x 152 x 128 -&amp;gt;  152 x 152 x  64 0.379 BF
    10 conv    128       3 x 3/ 1    152 x 152 x  64 -&amp;gt;  152 x 152 x 128 3.407 BF
    11 Shortcut Layer: 8,  wt = 0, wn = 0, outputs: 152 x 152 x 128 0.003 BF
    12 conv    256       3 x 3/ 2    152 x 152 x 128 -&amp;gt;   76 x  76 x 256 3.407 BF
    13 conv    128       1 x 1/ 1     76 x  76 x 256 -&amp;gt;   76 x  76 x 128 0.379 BF
    14 conv    256       3 x 3/ 1     76 x  76 x 128 -&amp;gt;   76 x  76 x 256 3.407 BF
    15 Shortcut Layer: 12,  wt = 0, wn = 0, outputs:  76 x  76 x 256 0.001 BF
    16 conv    128       1 x 1/ 1     76 x  76 x 256 -&amp;gt;   76 x  76 x 128 0.379 BF
    17 conv    256       3 x 3/ 1     76 x  76 x 128 -&amp;gt;   76 x  76 x 256 3.407 BF
    18 Shortcut Layer: 15,  wt = 0, wn = 0, outputs:  76 x  76 x 256 0.001 BF
    19 conv    128       1 x 1/ 1     76 x  76 x 256 -&amp;gt;   76 x  76 x 128 0.379 BF
    20 conv    256       3 x 3/ 1     76 x  76 x 128 -&amp;gt;   76 x  76 x 256 3.407 BF
    21 Shortcut Layer: 18,  wt = 0, wn = 0, outputs:  76 x  76 x 256 0.001 BF
    22 conv    128       1 x 1/ 1     76 x  76 x 256 -&amp;gt;   76 x  76 x 128 0.379 BF
    23 conv    256       3 x 3/ 1     76 x  76 x 128 -&amp;gt;   76 x  76 x 256 3.407 BF
    24 Shortcut Layer: 21,  wt = 0, wn = 0, outputs:  76 x  76 x 256 0.001 BF
    25 conv    128       1 x 1/ 1     76 x  76 x 256 -&amp;gt;   76 x  76 x 128 0.379 BF
    26 conv    256       3 x 3/ 1     76 x  76 x 128 -&amp;gt;   76 x  76 x 256 3.407 BF
    27 Shortcut Layer: 24,  wt = 0, wn = 0, outputs:  76 x  76 x 256 0.001 BF
    28 conv    128       1 x 1/ 1     76 x  76 x 256 -&amp;gt;   76 x  76 x 128 0.379 BF
    29 conv    256       3 x 3/ 1     76 x  76 x 128 -&amp;gt;   76 x  76 x 256 3.407 BF
    30 Shortcut Layer: 27,  wt = 0, wn = 0, outputs:  76 x  76 x 256 0.001 BF
    31 conv    128       1 x 1/ 1     76 x  76 x 256 -&amp;gt;   76 x  76 x 128 0.379 BF
    32 conv    256       3 x 3/ 1     76 x  76 x 128 -&amp;gt;   76 x  76 x 256 3.407 BF
    33 Shortcut Layer: 30,  wt = 0, wn = 0, outputs:  76 x  76 x 256 0.001 BF
    34 conv    128       1 x 1/ 1     76 x  76 x 256 -&amp;gt;   76 x  76 x 128 0.379 BF
    35 conv    256       3 x 3/ 1     76 x  76 x 128 -&amp;gt;   76 x  76 x 256 3.407 BF
    36 Shortcut Layer: 33,  wt = 0, wn = 0, outputs:  76 x  76 x 256 0.001 BF
    37 conv    512       3 x 3/ 2     76 x  76 x 256 -&amp;gt;   38 x  38 x 512 3.407 BF
    38 conv    256       1 x 1/ 1     38 x  38 x 512 -&amp;gt;   38 x  38 x 256 0.379 BF
    39 conv    512       3 x 3/ 1     38 x  38 x 256 -&amp;gt;   38 x  38 x 512 3.407 BF
    40 Shortcut Layer: 37,  wt = 0, wn = 0, outputs:  38 x  38 x 512 0.001 BF
    41 conv    256       1 x 1/ 1     38 x  38 x 512 -&amp;gt;   38 x  38 x 256 0.379 BF
    42 conv    512       3 x 3/ 1     38 x  38 x 256 -&amp;gt;   38 x  38 x 512 3.407 BF
    43 Shortcut Layer: 40,  wt = 0, wn = 0, outputs:  38 x  38 x 512 0.001 BF
    44 conv    256       1 x 1/ 1     38 x  38 x 512 -&amp;gt;   38 x  38 x 256 0.379 BF
    45 conv    512       3 x 3/ 1     38 x  38 x 256 -&amp;gt;   38 x  38 x 512 3.407 BF
    46 Shortcut Layer: 43,  wt = 0, wn = 0, outputs:  38 x  38 x 512 0.001 BF
    47 conv    256       1 x 1/ 1     38 x  38 x 512 -&amp;gt;   38 x  38 x 256 0.379 BF
    48 conv    512       3 x 3/ 1     38 x  38 x 256 -&amp;gt;   38 x  38 x 512 3.407 BF
    49 Shortcut Layer: 46,  wt = 0, wn = 0, outputs:  38 x  38 x 512 0.001 BF
    50 conv    256       1 x 1/ 1     38 x  38 x 512 -&amp;gt;   38 x  38 x 256 0.379 BF
    51 conv    512       3 x 3/ 1     38 x  38 x 256 -&amp;gt;   38 x  38 x 512 3.407 BF
    52 Shortcut Layer: 49,  wt = 0, wn = 0, outputs:  38 x  38 x 512 0.001 BF
    53 conv    256       1 x 1/ 1     38 x  38 x 512 -&amp;gt;   38 x  38 x 256 0.379 BF
    54 conv    512       3 x 3/ 1     38 x  38 x 256 -&amp;gt;   38 x  38 x 512 3.407 BF
    55 Shortcut Layer: 52,  wt = 0, wn = 0, outputs:  38 x  38 x 512 0.001 BF
    56 conv    256       1 x 1/ 1     38 x  38 x 512 -&amp;gt;   38 x  38 x 256 0.379 BF
    57 conv    512       3 x 3/ 1     38 x  38 x 256 -&amp;gt;   38 x  38 x 512 3.407 BF
    58 Shortcut Layer: 55,  wt = 0, wn = 0, outputs:  38 x  38 x 512 0.001 BF
    59 conv    256       1 x 1/ 1     38 x  38 x 512 -&amp;gt;   38 x  38 x 256 0.379 BF
    60 conv    512       3 x 3/ 1     38 x  38 x 256 -&amp;gt;   38 x  38 x 512 3.407 BF
    61 Shortcut Layer: 58,  wt = 0, wn = 0, outputs:  38 x  38 x 512 0.001 BF
    62 conv   1024       3 x 3/ 2     38 x  38 x 512 -&amp;gt;   19 x  19 x1024 3.407 BF
    63 conv    512       1 x 1/ 1     19 x  19 x1024 -&amp;gt;   19 x  19 x 512 0.379 BF
    64 conv   1024       3 x 3/ 1     19 x  19 x 512 -&amp;gt;   19 x  19 x1024 3.407 BF
    65 Shortcut Layer: 62,  wt = 0, wn = 0, outputs:  19 x  19 x1024 0.000 BF
    66 conv    512       1 x 1/ 1     19 x  19 x1024 -&amp;gt;   19 x  19 x 512 0.379 BF
    67 conv   1024       3 x 3/ 1     19 x  19 x 512 -&amp;gt;   19 x  19 x1024 3.407 BF
    68 Shortcut Layer: 65,  wt = 0, wn = 0, outputs:  19 x  19 x1024 0.000 BF
    69 conv    512       1 x 1/ 1     19 x  19 x1024 -&amp;gt;   19 x  19 x 512 0.379 BF
    70 conv   1024       3 x 3/ 1     19 x  19 x 512 -&amp;gt;   19 x  19 x1024 3.407 BF
    71 Shortcut Layer: 68,  wt = 0, wn = 0, outputs:  19 x  19 x1024 0.000 BF
    72 conv    512       1 x 1/ 1     19 x  19 x1024 -&amp;gt;   19 x  19 x 512 0.379 BF
    73 conv   1024       3 x 3/ 1     19 x  19 x 512 -&amp;gt;   19 x  19 x1024 3.407 BF
    74 Shortcut Layer: 71,  wt = 0, wn = 0, outputs:  19 x  19 x1024 0.000 BF
    75 conv    512       1 x 1/ 1     19 x  19 x1024 -&amp;gt;   19 x  19 x 512 0.379 BF
    76 conv   1024       3 x 3/ 1     19 x  19 x 512 -&amp;gt;   19 x  19 x1024 3.407 BF
    77 conv    512       1 x 1/ 1     19 x  19 x1024 -&amp;gt;   19 x  19 x 512 0.379 BF
    78 conv   1024       3 x 3/ 1     19 x  19 x 512 -&amp;gt;   19 x  19 x1024 3.407 BF
    79 conv    512       1 x 1/ 1     19 x  19 x1024 -&amp;gt;   19 x  19 x 512 0.379 BF
    80 conv   1024       3 x 3/ 1     19 x  19 x 512 -&amp;gt;   19 x  19 x1024 3.407 BF
    81 conv   1818       1 x 1/ 1     19 x  19 x1024 -&amp;gt;   19 x  19 x1818 1.344 BF
    82 yolo
  [yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00
    83 route  79 		                           -&amp;gt;   19 x  19 x 512
    84 conv    256       1 x 1/ 1     19 x  19 x 512 -&amp;gt;   19 x  19 x 256 0.095 BF
    85 upsample                 2x    19 x  19 x 256 -&amp;gt;   38 x  38 x 256
    86 route  85 61 	                           -&amp;gt;   38 x  38 x 768
    87 conv    256       1 x 1/ 1     38 x  38 x 768 -&amp;gt;   38 x  38 x 256 0.568 BF
    88 conv    512       3 x 3/ 1     38 x  38 x 256 -&amp;gt;   38 x  38 x 512 3.407 BF
    89 conv    256       1 x 1/ 1     38 x  38 x 512 -&amp;gt;   38 x  38 x 256 0.379 BF
    90 conv    512       3 x 3/ 1     38 x  38 x 256 -&amp;gt;   38 x  38 x 512 3.407 BF
    91 conv    256       1 x 1/ 1     38 x  38 x 512 -&amp;gt;   38 x  38 x 256 0.379 BF
    92 conv    512       3 x 3/ 1     38 x  38 x 256 -&amp;gt;   38 x  38 x 512 3.407 BF
    93 conv   1818       1 x 1/ 1     38 x  38 x 512 -&amp;gt;   38 x  38 x1818 2.688 BF
    94 yolo
  [yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00
    95 route  91 		                           -&amp;gt;   38 x  38 x 256
    96 conv    128       1 x 1/ 1     38 x  38 x 256 -&amp;gt;   38 x  38 x 128 0.095 BF
    97 upsample                 2x    38 x  38 x 128 -&amp;gt;   76 x  76 x 128
    98 route  97 36 	                           -&amp;gt;   76 x  76 x 384
    99 conv    128       1 x 1/ 1     76 x  76 x 384 -&amp;gt;   76 x  76 x 128 0.568 BF
   100 conv    256       3 x 3/ 1     76 x  76 x 128 -&amp;gt;   76 x  76 x 256 3.407 BF
   101 conv    128       1 x 1/ 1     76 x  76 x 256 -&amp;gt;   76 x  76 x 128 0.379 BF
   102 conv    256       3 x 3/ 1     76 x  76 x 128 -&amp;gt;   76 x  76 x 256 3.407 BF
   103 conv    128       1 x 1/ 1     76 x  76 x 256 -&amp;gt;   76 x  76 x 128 0.379 BF
   104 conv    256       3 x 3/ 1     76 x  76 x 128 -&amp;gt;   76 x  76 x 256 3.407 BF
   105 conv   1818       1 x 1/ 1     76 x  76 x 256 -&amp;gt;   76 x  76 x1818 5.376 BF
   106 yolo
  [yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00
  Total BFLOPS 148.812
  avg_outputs = 1358830
   Allocate additional workspace_size = 52.43 MB
  Loading weights from yolov3-openimages.weights...
   seen 64, trained: 32013 K-images (500 Kilo-batches_64)
  Done! Loaded 107 layers from weights-file
   Detection layer: 82 - type = 28
   Detection layer: 94 - type = 28
   Detection layer: 106 - type = 28
  ./input/test/3bc4e7965d04d91f.jpg: Predicted in 34.763000 milli-seconds.
  Person: 28%
  Person: 42%
  Man: 32%
  Clothing: 28%
  Person: 49%
  Man: 37%
  Clothing: 41%
  Person: 39%
  Person: 53%
  Man: 30%
  Clothing: 34%
  Person: 35%
  Person: 55%
  Clothing: 29%
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;상당히 기나 긴 YOLO의 layer들을 거쳐 최종적으로 &lt;strong&gt;Person, Man, Clothing&lt;/strong&gt; object들을 찾아낸 것을 확인할 수 있습니다. 글로만 보면 와닿지 않으니 직접 찾아낸 결과들을 눈으로 확인해볼까요?&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/kaggle_google_object_detection/test_result.jfif&quot; alt=&quot;Detecting Person, Man, Clothing&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;매우 빠른 시간 안에 (거의 3~5초?) 저 class들을 찾아주는 것을 알 수 있습니다. YOLO의 장점인 빠른 속도를 느낄 수 있었습니다.&lt;/p&gt;

&lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/kaggle_google_object_detection/no_submission.PNG&quot; alt=&quot;submission을 만들었는데 왜 제출을 못하니...&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;두둥.. submission파일을 열심히 만들고나서야 이걸 깨달았습니다.. 지금 이 대회는 더 이상의 submission을 받지 않고 있네요. 열과 성을 다해 만든 결과물은 아니지만 그래도 제출은 해보고 어느 정도나 rating이 되는지 알고 싶었는데 아쉽습니다 ㅠㅠ&lt;/p&gt;

&lt;h3 id=&quot;마무리&quot;&gt;마무리&lt;/h3&gt;
&lt;p&gt;계속되는 변명같지만 코드 공부가 목적이었기 때문에 빠른 시간 안에 baseline 결과물을 만들어 보았습니다.
아무래도 pre-trained model만 가지고 아무런 tuning없이 진행하다보니 성능이 그렇게 좋지는 않습니다. 실제로 위의 result 결과를 보면 사람처럼 &lt;u&gt;큰 class를 제외하고는&lt;/u&gt; 잘 찾지 못한 모습을 알 수 있습니다. (컴퓨터, 의자, 책상 등은 detecting 실패) 실제 모델을 적용할 때에는 더 세세한 tuning과 train이 필요할 것으로 보입니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Kaggle page&lt;/strong&gt;: &lt;a href=&quot;https://www.kaggle.com/c/google-ai-open-images-object-detection-track/overview&quot;&gt;Google AI Open Images - Object Detection Track, 2018&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data description&lt;/strong&gt;: &lt;a href=&quot;https://storage.googleapis.com/openimages/web/challenge.html&quot;&gt;Open Images Challenge page&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;AlexeyAB Darknet source code&lt;/strong&gt;: &lt;a href=&quot;https://github.com/AlexeyAB/darknet&quot;&gt;https://github.com/AlexeyAB/darknet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Darknet 활용 참고 블로그&lt;/strong&gt;: &lt;a href=&quot;https://m.blog.naver.com/bigdata-pro/221781790878&quot;&gt;https://m.blog.naver.com/bigdata-pro/221781790878&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Codes&lt;/strong&gt;: &lt;a href=&quot;https://github.com/decision-J/ComputerVision/tree/main/%5BKaggle%5D%20Google%20AI%20Open%20Images%20-%20Object%20Detection%20Track(2018)&quot;&gt;decision-j gitgub&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 15 Jul 2021 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/competition/2021/07/15/Google-AI-Open-Images-Object-Detection-Track.html</link>
        <guid isPermaLink="true">http://localhost:4000/competition/2021/07/15/Google-AI-Open-Images-Object-Detection-Track.html</guid>
        
        <category>CV</category>
        
        <category>object-detection</category>
        
        <category>yolo</category>
        
        <category>kaggle</category>
        
        
        <category>Competition</category>
        
      </item>
    
      <item>
        <title>[CV] YOLO v1</title>
        <description>&lt;h1 id=&quot;paper-review-yolo_v1&quot;&gt;[Paper review] YOLO_v1&lt;/h1&gt;

&lt;p&gt;Object detection에 관한 논문들을 읽어보고자 합니다.
논문 리스트는 시간 순으로 정리되어 있는 깃헙이 있어 &lt;a href=&quot;https://github.com/hoya012/deep_learning_object_detection&quot;&gt;이 곳&lt;/a&gt;을 참고하여 히스토리를 거슬러 올라가려고 합니다. 너무나 잘 정리가 되어있어 많은 도움을 받고 있습니다.&lt;/p&gt;

&lt;p&gt;먼저 가장 궁금했던 &lt;strong&gt;YOLO&lt;/strong&gt; 계열의 논문들을 죽 따라가보며 리뷰해볼까 합니다! 가장 첫 번째 버전인 &lt;em&gt;You Only Look Once: Unified, Real-Time Object Detection, Redmon, Joseph, et al. , Proceedings of the IEEE conference on computer vision and pattern recognition (2016)&lt;/em&gt; 부터 출발합니다~!&lt;/p&gt;

&lt;p&gt;(본 리뷰의 모든 수식과 그림은 원 논문을 참고했습니다.)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;yolo의-장점&quot;&gt;YOLO의 장점&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;You Only Look Once&lt;/em&gt;라는 제목에 걸맞게 YOLO detector가 추구하는 방향은 어느 정도의 정확성을 갖추면서(가장 높은 정확성 x) 다른 detector들보다 훨씬 빠르게 이미지를 판별하는 것입니다. 저자도 YOLO가 “extremly fast”하다며 이를 강조하고 있는데요! 이렇게 빠른 속도가 가능한 이유는 detection model을 일종의 single regression 문제로 단순화 시켰기 때문입니다. YOLO는 주어진 이미지 픽셀을 가지고 &lt;strong&gt;bounding box&lt;/strong&gt; (object를 판별하는 네모 범위)의 위치와 이를 나타내는 &lt;strong&gt;class probability&lt;/strong&gt; 를 &lt;strong&gt;single convolution&lt;/strong&gt; 으로 빠르게 계산합니다. 이 때문에 복잡한 classification pipeline이 필요없으며 YOLO의 계산 속도를 빠르게 만듭니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLO_v1/single.PNG&quot; alt=&quot;YOLO detection system&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;YOLO의 &lt;strong&gt;두 번째 장점&lt;/strong&gt; 은 이미지 픽셀 조합을 전부 다 들여다보기 때문에 robust하면서도 전체적인 모든 object를 다 찾아낼 수 있다는 것입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLO_v1/all_pixel.PNG&quot; alt=&quot;YOLO can detect from all image pixel&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;위 사진을 보면 YOLO가 전체 이미지 픽셀을 모두 search하여 가장 confidence가 높은 box들을 굵은 표시로 찾아낸 것을 확인할 수 있습니다. 이렇게 모든 pixel 조합들을 탐색할 수 있는 이유는 역시 YOLO가 빠른 속도를 가진 detector이기 때문입니다.&lt;/p&gt;

&lt;p&gt;YOLO의 &lt;strong&gt;세 번째 장점&lt;/strong&gt; 은 generalizable representation에 대해 학습한다는 것입니다. 논문 후반부에 제시되는 예술 작품에 대한 detector 비교에서 YOLO는 뛰어난 성능을 보여줍니다. 학습한 데이터셋과 특성이 다른 (사진이 아닌 그림인) 예술 작품들에서도 사물을 잘 잡아낸다는 것은 그만큼 object의 general한 특징을 잘 학습한다는 것을 의미합니다. 저자들은 이러한 YOLO의 장점이 &lt;strong&gt;new domain&lt;/strong&gt; 에서도 어느 정도의 성능을 담보해줄 수 있다고 기대합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLO_v1/general.PNG&quot; alt=&quot;YOLO learns generalizable representation&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;yolo-methodology&quot;&gt;YOLO methodology&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLO_v1/method.PNG&quot; alt=&quot;YOLO's unified model&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;YOLO의 model 매커니즘을 가장 잘 이해할 수 있는 figure입니다. 설명에서 알 수 있듯이 YOLO는 두 가지 도구를 가지고 object detection을 수행합니다. 첫 째, Confidence score가 높은 Bounding box, 둘 째 Class probability가 높은 셀 구역입니다. 먼저 이 두 가지 도구를 갖추기 위해 이미지 픽셀을 &lt;strong&gt;$S * S$ 구역으로 grid하게&lt;/strong&gt; 나누어줍니다. 이 구역들의 전체 조합에서 확률 계산을 통해 두 도구를 갖추는 것입니다.&lt;/p&gt;

&lt;p&gt;먼저 각 grid cell은 &lt;strong&gt;B개의 Bounding box&lt;/strong&gt; 를 예측합니다. B개를 선택하는 기준은 &lt;em&gt;confidence score&lt;/em&gt;입니다. confidence score는 bounding box가 object를 담고 있는지, 담고 있다면 얼마나 정확하게 담고 있는지를 나타내는 지표입니다. 이는 Confidence score를 계산하는 식을 살펴보면 더 잘 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;$$
\begin{gathered}
\textit{Confidence score} = P(Object) \cdot IOU^{truth}_{pred}
\end{gathered}
$$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;$P(object)$&lt;/strong&gt; 는 말 그대로 어떤 object를 담고 있을 확률을 의미합니다. 따라서 이 확률이 낮다면 아무리 IOU가 높아도 Confidence score가 높아질 수 없습니다. 일단 Object를 담고 있을 확률이 높다면 얼마나 정확하게 box가 object 주위를 감싸고 있는지를 알아봐야 합니다. &lt;strong&gt;$IOU^{truth}_{pred}$&lt;/strong&gt; 가 바로 그 부분입니다. 이 두 값의 곱이 높은 bounding box를 찾아줌으로써 우리는 object를 가장 잘 둘러싸고 있는 bounding box를 찾아낼 수 있습니다.
YOLO는 bounding box output으로 box의 좌표값인 $x, y, w, h$ 와 confidence score를 가집니다.&lt;/p&gt;

&lt;p&gt;두 번째로 각 cell이 어떤 class를 가질 확률이 높은지 class probability map을 그려줍니다. Conditional probability로 어떠한 Object가 주어졌을 때 어떤 Class를 가질 확률이 높은지 찾아주는 map입니다.&lt;/p&gt;

&lt;p&gt;$$
\begin{gathered}
\textit{Class probability} = P(Class_i | Object)
\end{gathered}
$$&lt;/p&gt;

&lt;p&gt;모든 grid cell에 전부 위 확률을 구해주며, 이를 통해 각 cell이 어떤 class를 가질 확률이 높은지 알 수 있습니다. 예시 figure를 살펴보면 강아지가 있는 부분은 하늘색, 자전거는 노랑색, 차량 및 배경은 빨간색 등으로 Class probability가 높은 구역끼리 표현된 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;종합하면 YOLO는 위 두 도구를 이용하여 &lt;strong&gt;confidence score가 높은 box들을 선택&lt;/strong&gt; 한 뒤, 그 box들에 포함된 grid cell들이 &lt;strong&gt;어떤 class에 들어갈 확률이 높은지&lt;/strong&gt; 파악하여 그 class로 detection하는 매커니즘을 가지고 있습니다. 이에 YOLO의 prediction은 $S * S * (B * 5 + C)$ 의 tensor 구조를 갖게 됩니다.($C = classes$)&lt;/p&gt;

&lt;h3 id=&quot;network-design&quot;&gt;Network design&lt;/h3&gt;
&lt;p&gt;그렇다면 이런 매커니즘을 수행하는 Network는 어떤 아키텍처를 가지고 있을까요?
빠른 수행을 요하는 YOLO인만큼 꽤 단순한 구조를 가지고 있습니다. 저자들은 GoogLeNet을 참고하여 &lt;strong&gt;24개의 convolutional layer를 가지고 2개의 fully connected layer&lt;/strong&gt; 가 붙는 구조를 만들었습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLO_v1/model.PNG&quot; alt=&quot;Network Design&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;위 구조의 구체적인 layer dimension값은 PASCAL VOC set을 학습하는 $7 * 7 * 30$에 맞추어져 있습니다. Convolutional layer는 ImageNet task에서 pretrain 된 것을 사용했으며 input resolution을 높이기 위해 input size를 imageNet의 두배인 $448 * 448$로 세팅했습니다.
저자들은 정규 YOLO 외에 속도를 매우 빠르게 design한 Fast YOLO도 소개했습니다. Fast YOLO는 convolutional layer를 단 9개만 사용하면서 더 simple하게 network를 design했습니다.&lt;/p&gt;

&lt;h3 id=&quot;loss-function&quot;&gt;Loss function&lt;/h3&gt;
&lt;p&gt;YOLO의 train을 담당하는 Loss function에 대해 간단히만 살펴보고자 합니다. 논문에 소개된 Object function은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;$$
\begin{gathered}
\lambda_{noobj}\sum^{S^2}&lt;em&gt;i\sum^B_j I^{noobj}&lt;/em&gt;{ij} (C_i-\hat{C_i})^2 + &lt;br /&gt;
\sum^{S^2}&lt;em&gt;i I^{obj}_i\sum&lt;/em&gt;{c\in Class}(p_i(c)-\hat{p}&lt;em&gt;i(c))^2 + &lt;br /&gt;
\sum^{S^2}_i\sum^B_j I^{obj}&lt;/em&gt;{ij} (C_i-\hat{C_i})^2 + &lt;br /&gt;
\lambda_{coord}\sum^{S^2}&lt;em&gt;i\sum^B_j I^{obj}&lt;/em&gt;{ij} [(x_i-\hat{x_i})^2 + (y_i-\hat{y_i})^2] + &lt;br /&gt;
\lambda_{coord}\sum^{S^2}&lt;em&gt;i\sum^B_j I^{obj}&lt;/em&gt;{ij} [(\sqrt{w_i}-\sqrt{\hat{w_i}})^2 + (\sqrt{h_i}-\sqrt{\hat{h_i}})^2] &lt;br /&gt;
\textit{where } \lambda_{noobj} = 0.5, \lambda_{coord} = 5
\end{gathered}
$$&lt;/p&gt;

&lt;p&gt;꽤 길어보이는데요! 하나씩 살펴보겠습니다. 먼저 가장 중요한 Key역할을 하는 Indicator function들에 대해 살펴봐야 합니다. 본문에 $i$는 cell, $j$는 bounding box의 인덱스입니다. 즉, $I^{obj}&lt;em&gt;i$는 해당 cell안에 object의 존재 여부 / $I^{obj}&lt;/em&gt;{ij}$는 i번째 cell안의 j번째 bounding box가 prediction하기에 충분히 &lt;em&gt;responsible&lt;/em&gt; 한 지에 대한 여부를 나타냅니다. 여기서 &lt;em&gt;responsible&lt;/em&gt; 이란 IOU가 가장 높은 bounding box를 나타낼 겁니다. 즉, YOLO의 Loss function은 각 셀과 box들이 object를 담고 있느냐 없느냐에 따라 conditional하게 function의 모양이 달라지는 것입니다.&lt;/p&gt;

&lt;p&gt;먼저 cell과 box가 모두 object를 포함하지 못한다고 판단되면 어떻게 될까요? ($I^{noobj}_{ij} = 1$) 첫 번째 term을 제외하고 아래의 모든 term들이 사라지게 됩니다. 즉 &lt;strong&gt;가장 간단한 loss function&lt;/strong&gt; 이 되는 것이죠. 아무래도 중요도가 떨어지기 때문입니다. (object도 포함하지 않은 영역을 굳이 coordinate까지 최적화할 필요는 없겠죠.)&lt;/p&gt;

&lt;p&gt;그 다음 단계는 cell은 object를 포함하고 있으나 box는 &lt;em&gt;responsible&lt;/em&gt;하지 않은 경우입니다. ($I^{obj}_{i} = 1$) 이 경우, 두 번째 term만이 남게 되죠. 역시 bounding box가 믿음직스럽지 못하기 때문에 굳이 coordinate까지 최적화할 필요가 없습니다.&lt;/p&gt;

&lt;p&gt;마지막으로 cell도 object를 포함하고 box도 &lt;em&gt;responsible&lt;/em&gt; 한 경우입니다. ($I^{obj}_{ij} = 1$) YOLO가 가장 최적화하고 싶어하고 정확하게 잡아내고 싶어하는 cell과 box일 것입니다. 이에 여러 가지 penalty항들이 추가됩니다. Class($C$)에 대한 정보들, x, y 좌표값과 weight($w$), height($h$)에 해당하는 box의 크기에 대한 부분까지 최적화의 대상이 됩니다. 이러한 Loss function으로 YOLO는 더 완벽한 box를 학습하게 됩니다.&lt;/p&gt;

&lt;p&gt;이렇게 conditional한 loss function을 사용함으로써 YOLO는 더 빠른 속도를 가지게 되는 것 같습니다. 굳이 중요하지 않은 부분은 깊이 있게 학습하지 않기 때문이죠. 이러한 특징은 속도 뿐만 아니라 YOLO가 generality한 특성을 갖는데에도 도움을 줄 것으로 생각됩니다.&lt;/p&gt;

&lt;h3 id=&quot;experiment-results&quot;&gt;Experiment Results&lt;/h3&gt;
&lt;p&gt;이제 실제 dataset에서 다른 detector들과 비교하여 YOLO가 어떤 성능을 나타내주는 지 비교해봅시다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/computer_vision/YOLO_v1/results.PNG&quot; alt=&quot;Performance comparison with other detectors from PASCAL VOC 2007, 2012&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;각 detector들 별로 PASCAL VOC 2007, 2012 데이터로 학습했을 때의 성능 평가를 나타낸 표입니다. mAP는 정확성, FPS는 컴퓨팅 속도의 지표입니다. Fast YOLO가 155에 달하는 속도를 보여주며 매우 빠른 detector임을 자랑했습니다. YOLO의 경우 FPS 45를 기록하며 Real-Time detector 중에서도 매우 빠른 속도를 가짐에도 mAP에서 다른 detector들에 비해 그다지 떨어지지 않는 퍼포먼스를 보여줍니다. Detection의 정확도가 정말 높지 않더라도 매우 빠르게 이미지를 인식하자는 YOLO의 방향성에 맞는 결과인 것 같습니다.&lt;/p&gt;

&lt;h3 id=&quot;limitation&quot;&gt;Limitation&lt;/h3&gt;
&lt;p&gt;이런 YOLO에도 한계점이 존재합니다. 먼저 YOLO가 grid cell을 나누어 bounding box를 설정하기 때문에 공간적 constraint를 받을 수 밖에 없습니다.
또한 loss function이 bounding box에 크기에 상관없이 계산되기 때문에 큰 box에서의 작은 error는 무시될 확률이 높은 반면 작은 box에서의 작은 error는 critical하게 반영됩니다.
이러한 두 한계점으로 인해 새와 같은 작은 물체를 detection하는 데 어려움이 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;마무리&quot;&gt;마무리&lt;/h3&gt;
&lt;p&gt;저자들은 (YOLO의 이름에서도 알 수 있듯이) 인간이 살짝 물체를 보아도 그 것이 무엇인지 알아채는 것에 아이디어를 얻어 이 detector를 고안했다고 합니다. 점차 image detection의 활용 범위가 넓어지고 있어 이렇게 빠른 속도에 기반한 detector들이 더 쓰임새가 많아질 것 같습니다. YOLO가 version을 upgrade하면서 얼마나 발전할지 다음 논문에서 살펴봐야겠습니다!&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Paper: &lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf&quot;&gt;You Only Look Once: Unified, Real-Time Object Detection, Redmon, Joseph, et al. , Proceedings of the IEEE conference on computer vision and pattern recognition (2016)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://machinethink.net/blog/object-detection/&quot;&gt;https://machinethink.net/blog/object-detection&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Mon, 31 May 2021 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/papers/2021/05/31/Yolo_review.html</link>
        <guid isPermaLink="true">http://localhost:4000/papers/2021/05/31/Yolo_review.html</guid>
        
        <category>CV</category>
        
        <category>object-detection</category>
        
        <category>yolo</category>
        
        
        <category>Papers</category>
        
      </item>
    
      <item>
        <title>Coursera Computer Vision Course 과제 정리</title>
        <description>&lt;h1 id=&quot;coursera-computer-vision-course-과제-정리&quot;&gt;Coursera Computer Vision Course 과제 정리&lt;/h1&gt;

&lt;p&gt;장장 1월부터 4월까지 3개월을 질질 끌었던 (&lt;del&gt;심지어 다 듣지도 못함&lt;/del&gt;)&lt;/p&gt;

&lt;p&gt;Coursera의 &lt;strong&gt;“Deep Learning in Computer Vision”&lt;/strong&gt;(&lt;em&gt;HSE Univ.&lt;/em&gt;) 수업에서 나왔던 과제들을 간단하게 정리해보고자 합니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;week-1&quot;&gt;Week 1&lt;/h3&gt;

&lt;p&gt;1주차 과제는 주어진 gray scale 이미지의 &lt;strong&gt;윤곽선을 detection&lt;/strong&gt;하는 것입니다.
이 과정에서 &lt;em&gt;Canny Edge Detector&lt;/em&gt; 라는 것을 사용하게 되는데요! 작동 매커니즘을 간략히 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Noise Reduction&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;제일 먼저 이미지의 Noise를 제거해줍니다. Canny detector는 윤곽선을 잡아내기 위해 미분을 하게 되는데 이 때 Noise가 끼어있으면 임계점을 찾아내기가 힘듭니다. 이에 Gaussian kernel을 이용한 Blur처리를 통해 이미지를 흐릿하게 바꿔줍니다. (핵심 포인트만 남기는 거죠!)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Calculating gradient&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이렇게 smoothing된 이미지를 $I$라고 할 때, 이 이미지의 픽셀에서 그 값이 뚜렷하게 구분되는 지점이 어디인지(&lt;em&gt;Slope&lt;/em&gt;), 또 그 곳에서 이미지 값의 강도는 얼마인지(&lt;em&gt;Magnitude&lt;/em&gt;)를 찾아야 윤곽선을 찾을 수 있습니다.
이를 찾기 위해 &lt;em&gt;Sobel Kernel&lt;/em&gt; 을 활용합니다. 구하는 수식은 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;$$
\begin{gathered}
|G| = \sqrt{I_x^2+I_y^2}, &lt;br /&gt;
\theta(x,y) = arctan(\frac{I_y}{I_x}) &lt;br /&gt;
where\ G\ is\ Magnitude\ and\ \theta\ is\ the\ Slope
\end{gathered}
$$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Non-maximum suppression&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;위에서 찾아진 Magnitude를 가지고 진짜 Maxumum, 즉, 더욱 명확한 픽셀 경계선을 찾기 위해 Non-maximum value들을 지워줍니다. Neighbor에 있는 점들끼리 $G$를 비교해서 이루어지게 됩니다.
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. Double Threshold &amp;amp; Edge tracking&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이렇게 찾아진 윤곽선 후보들 중에서도 아직도 noise들이 섞여 있습니다. 이를 더 명확히 하기 위해서 &lt;em&gt;Low Threshold, High Threshold&lt;/em&gt;를 설정해서 이 범위 밖에 있는 값들은 이제 윤곽선으로 확정해줍니다.
Threshold들 사이에 있는 값들의 경우 Edge tracking을 통해 윤곽선으로 인정해줄 수 있는지를 최종 판단하는 절차를 거칩니다.
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이제 과제를 통해 Canny detector를 적용한 모습을 살펴보겠습니다. 과정은 복잡하지만 코드를 통해 간단하게 구현할 수 있습니다. 먼저 edge를 detect해야 하는 이미지의 모습입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/coursera_ComputerVision/canny1.PNG&quot; alt=&quot;canny1&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;이제 이 이미지에 Canny edge를 적용해보겠습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/coursera_ComputerVision/canny2.PNG&quot; alt=&quot;canny2&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;보시는 것처럼 성과 나무의 경계선 만이 뚜렷하게 남은 것을 확인할 수 있습니다. 성 이미지의 3가지 모습들에서 구름의 모양이 각기 다른데, 이 부분에 대한 윤곽선도 잡아내는 부분이 흥미롭네요!&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;week-2&quot;&gt;Week 2&lt;/h3&gt;

&lt;p&gt;2주차 과제는 &lt;strong&gt;Facial Point Detection&lt;/strong&gt;입니다. 말 그대로 얼굴의 다양한 포인트들을 잡아내주는 영역이죠!
다양한 사람 이미지들에서 정확히 눈썹, 눈, 코, 입 등을 잡아내주는 과제입니다.
일련의 코드 작업을 통해 Point를 찍어주면 다음과 같습니다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{.python}&quot;&gt;import matplotlib.pyplot as plt
from matplotlib.patches import Circle

def visualize_points(img, points):
    fig,ax = plt.subplots(1)
    ax.set_aspect('equal')

    # Show the image
    ax.imshow(img)

    for i in range(int(len(points)/2)):
        circ = Circle( ( (points[i*2] + 0.5) * 100, (points[(i*2 + 1)] + 0.5) * 100 ) ,1, color=&quot;red&quot;)

        ax.add_patch(circ)

        # Show the image
    plt.show()

visualize_points(imgs[1], points[1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/coursera_ComputerVision/facial1.png&quot; alt=&quot;facial1&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;이러한 이미지들은 아래와 같이 좌우 flip을 통해 모델에 넣을 때 Data augmentation 효과를 볼 수 있습니다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{.python}&quot;&gt;def flip_img(img, points):

    f_points = zeros(int(points.shape[0]))
    reverse_points = points[::-1]
    for i in range(int(points.shape[0]/2)):
        f_points[i*2] = - reverse_points[i*2 + 1] # 음수 부호: X축 기준으로 좌우 반전 해주어야하기 때문
        f_points[i*2 + 1] = reverse_points[i*2]

    return f_img, f_points

f_img, f_points = flip_img(imgs[1], points[1])
visualize_points(f_img, f_points)
&lt;/code&gt;&lt;/pre&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/coursera_ComputerVision/facial2.png&quot; alt=&quot;facial2&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;del&gt;물론 이런 이미지를 가지고 모델에 피팅해서 테스트 셋 이미지에서 facial point를 찾는 것이 과제의 완성이었지만.. 파일을 잃어버림…그래서 구질구질 코드까지 삽입;;&lt;/del&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;week-3&quot;&gt;Week 3&lt;/h3&gt;

&lt;p&gt;3주차 과제는 &lt;strong&gt;Face detection&lt;/strong&gt;입니다. 여러 종류의 사람들의 이미지(FDDB dataset)에서 얼굴만 정확하게 추출해내는 것이 과제의 목표입니다. 일종의 얼굴 object detection이라고도 할 수 있을 것 같습니다.&lt;/p&gt;

&lt;p&gt;먼저 이미지들과 Target을 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/coursera_ComputerVision/data&amp;amp;target.PNG&quot; alt=&quot;PNG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음과 같이 약 1,000여 장의 사람들의 이미지가 존재하고 각 이미지마다 얼굴을 특정하는 &lt;strong&gt;Bounding Box&lt;/strong&gt;가 있습니다. 과제의 목표는 Test set 이미지에서도 정확한 Bounding box를 그려줄 수 있는지 입니다.&lt;/p&gt;

&lt;p&gt;출제자의 의도는 &lt;strong&gt;Positive bounding box&lt;/strong&gt; (주어진 label, 실제 사람 얼굴)와 &lt;strong&gt;Negative bounding box&lt;/strong&gt; (사람이 얼굴이 아닌 다른 곳)를 활용하여 Binary classification으로 model이 얼굴을 찾아낼 수 있도록 하는 것입니다. 이를 위해서 이미지의 shape limit을 바탕으로 얼굴이 아닌 곳에 해당하는 negative bounding box를 임의로 생성하였습니다. 그 비율은 5:5 정도입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://decision-J.github.io/assets/coursera_ComputerVision/pos_neg.png&quot; alt=&quot;pos_neg&quot; /&gt;
  Top 2: Positive box, Bottom 2: Negative box
&lt;/p&gt;

&lt;p&gt;과제에서 사용한 모델은 &lt;strong&gt;Lenet&lt;/strong&gt; (&lt;em&gt;LeCun, Y., Bottou, L., Bengio, Y. and Haffner, P., 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), pp.2278-2324&lt;/em&gt;)입니다. Keras를 이용해서 간단하게 구현할 수 있으며 데이터의 명확성 때문인지 epoch을 많이 주지 않아도 training accuracy 높게 나타납니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/coursera_ComputerVision/performance.PNG&quot; alt=&quot;Model Performance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 fitting된 모델을 가지고 test set에서도 얼굴을 잘 잡아낼 수 있을 지 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/coursera_ComputerVision/pred.PNG&quot; alt=&quot;PNG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;파란색 box가 모델이 얼굴이라고 예측한 부분입니다. 언뜻 잘 맞추는 것도 같지만 자세히 살펴볼수록 부정확한 모습입니다. 특히, &lt;strong&gt;얼굴을 전부 포함하지 못하고 부분 부분만을 잡아내고 있습니다&lt;/strong&gt;. 아무래도 netgative box와 postive box를 동시에 넣고 fitting을 시키다 보니 발생하는 문제점인 것 같습니다. 또한 이미지의 size가 작고 그에 비해 box 사이즈는 크다보니 정확히 포착을 못하는 것 같습니다. 물론 더 하이엔드 모델을 쓴다거나 model을 더 정교하게 fitting하는 방법으로 performance는 개선될 여지가 매우 많아 보입니다. (&lt;del&gt;강의도 다 못들었는데 성능 개선까지 할리가 없습니다 ㅠㅠ&lt;/del&gt;)&lt;/p&gt;
</description>
        <pubDate>Wed, 28 Apr 2021 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/computer-vision/2021/04/28/Coursera-ComputerVision-course.html</link>
        <guid isPermaLink="true">http://localhost:4000/computer-vision/2021/04/28/Coursera-ComputerVision-course.html</guid>
        
        <category>CV</category>
        
        <category>canny</category>
        
        <category>edge</category>
        
        <category>facial-points</category>
        
        <category>face-detection</category>
        
        
        <category>Computer-vision</category>
        
      </item>
    
      <item>
        <title>[Dacon] 한국어 문서 추출요약 AI 경진대회 참여기</title>
        <description>&lt;h1 id=&quot;한국어-문서-추출요약-ai-경진대회&quot;&gt;한국어 문서 추출요약 AI 경진대회&lt;/h1&gt;

&lt;p&gt;2020년 겨울, Dacon에서 실시한 한국어 문서 추출요약 AI 대회에 참가했던 것을 기록해보고자 합니다.
(대회에 대한 자세한 사항은 &lt;a href=&quot;https://dacon.io/competitions/official/235671/overview/&quot;&gt;여기에!&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;이러한 대회에 항상 함께해주는 쇠똥구리 팀원들에게 먼저 감사 인사를 전합니다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;대회의 목적은 간단합니다! 다양한 한국어 기사 원문으로부터 적절한 &lt;strong&gt;추출 요약문&lt;/strong&gt;을 도출해내는 모델을 만들면 됩니다.
먼저 데이터의 모습과 함께 모델링의 목적을 파악해보겠습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;데이터--분석-목적&quot;&gt;데이터 &amp;amp; 분석 목적&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/BertSum_Competition/data.PNG&quot; alt=&quot;PNG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 데이터는 약 20~30여개의 문장으로 이루어진 기사들입니다. 각 기사에는 &lt;strong&gt;신문사, 기사원문, 요약문(label), 해당 요약문의 인덱스 값&lt;/strong&gt;이 포함되어 있습니다. train 데이터의 요약문의 경우 사람이 라벨링을 했다고 설명되어 있습니다.&lt;/p&gt;

&lt;p&gt;인덱스가 3개인 것에서 눈치채셨겠지만 기사 원문에서 &lt;u&gt;가장 중요한 3개&lt;/u&gt;의 문장을 추출하는 것이 대회의 목적입니다! 저희는 요약문을 추출하는 데 신문사 정보는 필요없다고 판단하여 원문과 요약문 데이터만을 가지고 모델링을 진행했습니다. 또한 학습 전에 특수 문자 제거, embedding 등 preprocessing 작업을 거쳤습니다. (embedding에는 뒤에서 언급할 koBert의 embedding 방법을 활용했습니다.)&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;사용-모델-kobert를-이용한-bertsum-model&quot;&gt;사용 모델: koBert를 이용한 BertSum model&lt;/h3&gt;
&lt;p&gt;먼저 문서의 요약문을 만드는 방식에는 크게 두 가지가 있는 것 같습니다. 전체 텍스트에 적절한 요약문을 생성하는 &lt;strong&gt;생성요약(Abstractive)&lt;/strong&gt;과 텍스트에 있는 문장 중 전체 내용을 대표한다고 생각되는 문장을 가져오는 &lt;strong&gt;추출요약(Extractive)&lt;/strong&gt;이 있습니다. 저희 팀은 대회의 목적에 따라 추출요약에 해당하는 모델을 사용했습니다.&lt;/p&gt;

&lt;p&gt;다양한 추출요약 모델 중에서 &lt;em&gt;Fine-tune BERT for Extractive Summarization(Yang Liu, 2019)&lt;/em&gt; 논문을 참고했습니다. (편의상 BertSum 모델이라고 부르겠습니다.)
BertSum은 쉽게 말해서 구글의 pre-trained Bert 모델을 활용하여 텍스트 데이터를 학습하고 Transformer encoding을 통해 대표성을 갖는 문장을 classification해주는 방법입니다. 모델에 대한 더욱 상세하고 자세한 사항은 갓누누의 블로그, &lt;a href=&quot;https://seonu-lim.github.io/nlp/BertSum/&quot;&gt;이 곳&lt;/a&gt;을 참고해주시면 도움이 되실 겁니다!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/BertSum_Competition/bertsum.png&quot; alt=&quot;갓누누가 만든 모델 설명도&quot; title=&quot;&amp;lt;u&amp;gt;갓누누&amp;lt;/u&amp;gt;가 만든 모델 설명도&quot; /&gt;&lt;/p&gt;

&lt;p&gt;앞 서 언급하였듯이 BertSum을 사용하려면 사전 학습된 Bert model이 필요한데요! 구글의 Bert는 영어를 기반으로 학습되어 있기 때문에 한국어를 기반한 모델이 필요했습니다. 다행히도 &lt;strong&gt;SKT Brain&lt;/strong&gt;에서 한국어를 기반한 &lt;strong&gt;koBert&lt;/strong&gt; 모형을 만들어주신 것을 알고 이 모형을 바탕으로 공모전을 진행했습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;모델링-특이사항&quot;&gt;모델링 특이사항&lt;/h3&gt;
&lt;p&gt;몇 개의 Baseline 모델을 만들어 본 후, 갓단단 팀원께서 한 가지 문제점을 발견했습니다. 몇몇 요약문들을 살펴보니 대표성이 없는 문장들이 추출된다는 것입니다. 예를 들어 다음과 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;단양 시청의 전경&lt;/li&gt;
  &lt;li&gt;삼성 라이온즈 사진 제공&lt;/li&gt;
  &lt;li&gt;신축 아파트 조감도&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이러한 문장들이 대표 요약문으로 추출되는 이유를 다음 두가지 정도로 생각해보았습니다. 먼저, 기사와 관련된 강력한 키워드가 포함되어 있기 때문입니다. 예를 들어, “삼성 라이온즈 사진 제공”이라는 문장의 경우 기사 전반이 삼성 라이온즈 야구단에 관한 내용이기 때문에 해당 키워드가 직접적으로 포함되어 요약문으로 추출되었다는 것입니다.&lt;/p&gt;

&lt;p&gt;두 번째로 데이터의 특성 때문입니다. 학습하고 있는 데이터가 기사이기 때문에 주로 두괄식으로 작성되어 있습니다. 따라서 1~3번째 문장에 핵심 요약문들이 포함된 경우가 많습니다. (위의 데이터 label index를 살펴보아도 0,1,2가 굉장히 많은 것을 알 수 있습니다.) 이 때, 몇몇 기사들은 맨 앞 문장에 기사에 포함된 사진에 관한 내용을 담는 경우가 있기 때문에 단순히 index에 따라서 저 문장들이 딸려 나온다는 것입니다.&lt;/p&gt;

&lt;p&gt;이에 따라 갓단단 팀원은 &lt;u&gt;문장의 음절이 10개 이하인 문장&lt;/u&gt;을 &lt;strong&gt;사전에 제거&lt;/strong&gt;하고 모델링을 진행해봤다고 합니다. 그 결과 저런 터무니없는 요약문이 줄어들고 score 측면에서도 상당히 개선된 결과를 확인할 수 있었습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;결과&quot;&gt;결과&lt;/h3&gt;
&lt;p&gt;수 많은 파라미터 튜닝 과정들을 거쳐 최적의 모형을 구축하였고, 최종적으로 428팀 중 20위라는 결과를 기록했습니다. 개인적으로는 첫 NLP대회에서 만족할만한 결과를 거둬 팀원들에게 고맙고 뿌듯한 마음입니다!&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;참고-사이트&quot;&gt;참고 사이트&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;갓누누 블로그: &lt;a href=&quot;https://seonu-lim.github.io/&quot;&gt;https://seonu-lim.github.io/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Fine-tune BERT for Extractive Summarization, Yang Liu, 2019: &lt;a href=&quot;https://arxiv.org/pdf/1903.10318.pdf&quot;&gt;https://arxiv.org/pdf/1903.10318.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;BertSum 저자 Github: &lt;a href=&quot;https://github.com/nlpyang/BertSum&quot;&gt;https://github.com/nlpyang/BertSum&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;SKT Brain koBert Github: &lt;a href=&quot;https://github.com/SKTBrain/KoBERT&quot;&gt;https://github.com/SKTBrain/KoBERT&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 30 Dec 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/competition/2020/12/30/Dacon-Korean-Summary-Extraction-Competition.html</link>
        <guid isPermaLink="true">http://localhost:4000/competition/2020/12/30/Dacon-Korean-Summary-Extraction-Competition.html</guid>
        
        <category>text-mining</category>
        
        
        <category>Competition</category>
        
      </item>
    
      <item>
        <title>Personality Detection from Text in Korean</title>
        <description>&lt;h1 id=&quot;personality-detection-from-text-in-korean&quot;&gt;Personality Detection from Text in Korean&lt;/h1&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;​	이번 포스팅은 이전 포스팅에서 다루었던 논문의 architecture를 실제로 적용해보는 프로젝트를 진행한 과정을 소개하겠습니다. 수업에서 간단한 활용 사례를 보이기 위해 진행한 예제로 모델의 성능 등은 우수하지 못함을 알려드립니다 (ㅠㅠ)
우선 &lt;a href=&quot;https://decision-j.github.io/text-mining/2020/10/03/DeepLearning-Based-Personality-detection-from-Text.html&quot;&gt;이전 포스팅&lt;/a&gt;에서 다룬 논문은 간략하게 설명하자면 주어진 Text에서 저자의 Personality를 detection하는 것입니다.&lt;/p&gt;

&lt;p&gt;참조 논문의 모델은 영어 text를 기반으로 작성되어 있지만 이번 예제에서는 더 직관적인 이해를 위해서 &lt;strong&gt;한국어 text&lt;/strong&gt;에서의 감성 분석을 진행해보고자 합니다. 한국어 text로 변경하면서 참조 논문과 약간 달라진 &lt;u&gt;수정 사항&lt;/u&gt;들을 정리해보았습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Text dataset이 essay에서 &lt;strong&gt;news article&lt;/strong&gt;로 변경&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;한국어 word2vec&lt;/strong&gt; embedding 사용&lt;/li&gt;
    &lt;li&gt;기존의 5-level classification에서 Positive personality만을 활용한 &lt;strong&gt;1-level classification&lt;/strong&gt;으로 변경&lt;/li&gt;
    &lt;li&gt;Document level Mairesse vector 미 삽입&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;한가지씩 살펴보겠습니다. 먼저 text dataset을 기존 논문의 essay에서 비교적 구하기 쉬운 news article로 바꿨습니다. 네이버 스포츠 뉴스에서 크롤링한 기사들인데요, 자세한 내용은 다음 챕터에서 설명하겠습니다. 두 번째로는 한국어 데이터 분석이니 word embedding을 한국어 word2vec으로 바꿨습니다. 세 번째로 참조 논문은 저자의 특성을 5가지로 구분하여 classification하고 있는데요! 레이블을 달기 어렵기도 하고 다른 특성들은 구분하기 어렵다고 판단하여 positive만을 활용한 간단한 분류 문제로 재 정의했습니다. positive 특성도 뉴스 특성에 따라 다르게 정의할텐데, 이 부분도 다음 파트에서 다루겠습니다. 마지막으로 Mairesse vector를 삭제했습니다. 본 vector는 document level의 essay특성을 더 잘 잡아주기 위한 조정 단계인데, dataset이 news로 바뀐만큼 불필요하다고 판단했습니다. (영어 text를 위한 조정이기도 하구요!)
그렇다면 데이터의 형태부터 소개하도록 하겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;데이터-소개&quot;&gt;데이터 소개&lt;/h3&gt;
&lt;p&gt;앞서 소개해드린대로 데이터는 News article입니다. 뉴스 기사를 선택한 이유는 네이버에서 비교적 쉽게 크롤링할 수 있기 때문입니다. 그런데 크롤링하고 보니 뉴스 기사의 저자가 인성 특성을 보인다는 것이 말이 안된다는 것을 깨달았습니다… 그래서 저자의 특성이 아닌 기사 자체의 특성으로 분류 문제를 재 정의했습니다! (본 논문에서 너무 바뀌는 것이 아닌가 하는..하하)&lt;/p&gt;

&lt;p&gt;기사의 긍정적인 특성을 빠르고 쉽게 캐치해야 되기 때문에 (제가 직접 label을 달아줄 것이기 때문입니다..) 이기고 지는 것이 명확한 스포츠 뉴스를 타겟으로 삼았습니다. 제가 관심을 많이 가지고 있는 야구에서, 제가 응원하는 팀인 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;기아 타이거즈&lt;/code&gt;를 키워드로 뉴스 기사들을 모았습니다. 2019 시즌의 기사들을 대상으로 하였고 대략 일주일에 100여개씩, 한달 동안 총 550여개의 기사들을 모을 수 있었습니다.&lt;/p&gt;

&lt;p&gt;레이블을 다는 방법은 간단합니다! 기사의 내용을 읽고 &lt;u&gt;기아 타이거즈가 승리&lt;/u&gt;하였거나 &lt;u&gt;기아 타이거즈 팀에 긍정적인&lt;/u&gt; 내용에 해당하면 1, 반대의 경우 0을 부여했습니다. (엄청난 팬심이 가미된 레이블링입니다 ㅎㅎ)&lt;/p&gt;

&lt;p&gt;완성된 데이터 셋의 모습은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/Personality_detection/dataset_korean.PNG&quot; alt=&quot;PNG&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;데이터-전처리&quot;&gt;데이터 전처리&lt;/h3&gt;

&lt;p&gt;완성된 dataset을 tokenize해줍니다! 또한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#, &amp;amp;, *&lt;/code&gt;과 같은 분석에 큰 도움을 주지 않는 (뉴스 기사에 특히 많은) 특수 문자들을 제거해줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/Personality_detection/dataset_token.PNG&quot; alt=&quot;PNG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 이 데이터를 한국어 버전 word2vec embedding matrix를 사용하여 vectorize해주겠습니다. 이제 이로써 모델에 들어갈 input dataset이 완성되었습니다!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/Personality_detection/dataset_embed.PNG&quot; alt=&quot;PNG&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;모델-결과&quot;&gt;모델 결과&lt;/h3&gt;

&lt;p&gt;모델의 architecture는 참조 논문의 형태와 동일합니다. 간단하게 리뷰하자면 다음과 같습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;No.&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Layer&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Level&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Input layer&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Word&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Convolution layer&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Word&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Max-pooling layer&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Word&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Concatenation layer&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Sentence&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1-max pooling layer&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Document&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Fully connected layer&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Document&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Output layer&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Document&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;간단하게 Accuracy와 Objective function(Negative log likelihood)의 Loss를 기준으로 CV과정을 살펴보겠습니다. Dataset이 비교적 작기 때문에 5:1의 비율로 train / valid set을 나누고 진행했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/Personality_detection/result1.PNG&quot; alt=&quot;PNG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫 50 epoch을 진행한 결과입니다. 처음부터 끝까지 overfitting의 향기가 진하게 납니다. 데이터가 비교적 간단하기 때문에 벌어지는 현상 같은데요. Layer의 수정이 조금은 필요하지 않나 생각했습니다. 이에 Dropout layer를 추가하고 epoch도 조금 줄여서 data에 customizing을 시켜주었습니다!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/Personality_detection/result2.PNG&quot; alt=&quot;PNG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전 결과보다는 비교적 괜찮아졌습니다! 대략 7~10 정도의 epoch에서 모델을 끊어주는 것이 좋아보입니다. Accuracy는 대략 &lt;strong&gt;0.73&lt;/strong&gt;정도의 퍼포먼스를 보여주는데요. 적용해보는 예제의 성격인만큼 모델 업그레이드는 이 정도만 하도록 하겠습니다! (후다닥)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;시사점&quot;&gt;시사점&lt;/h3&gt;

&lt;p&gt;아무래도 Accuracy가 높지 않다보니 개운하지 못한 느낌입니다 ㅎㅎ. Essay에서 News로 데이터의 성격이 바뀐 부분도 있고 (기사의 특성을 제가 잘 처리하지 못한 것 같습니다) Label을 제가 직접 달았다보니 모델이 학습하기 애매한 부분 (분석자의 직관이 반영된 부분)이 많이 포함되었기 때문이라고 보입니다.&lt;/p&gt;

&lt;p&gt;예제를 진행하면서 아무래도 2017년 논문이다보니 딥러닝 모델의 architecture가 조금 단순하다는 생각을 했습니다. 요즘 유행하는 발전된 모델들을 적용한다면 더 좋은 성능을 보여줄 수 있지 않을까 생각합니다.&lt;/p&gt;

&lt;p&gt;본 프로젝트는 논문 리뷰와 실전 적용을 해본 것에 만족하고 마무리하도록 하겠습니다~!&lt;/p&gt;
</description>
        <pubDate>Fri, 30 Oct 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/text-mining/2020/10/30/DeepLearning-Based-Personality-detection-from-Text-in-Korean.html</link>
        <guid isPermaLink="true">http://localhost:4000/text-mining/2020/10/30/DeepLearning-Based-Personality-detection-from-Text-in-Korean.html</guid>
        
        <category>text-mining</category>
        
        
        <category>Text-mining</category>
        
      </item>
    
      <item>
        <title>Deep Learning-Based Document Modeling for Personality Detection from Text</title>
        <description>&lt;h1 id=&quot;deep-learning-based-personality-detection-from-text&quot;&gt;Deep Learning-Based Personality Detection from Text&lt;/h1&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;​	이번 포스팅은 CNN을 활용하여 Text 저자의 Personality를 판별하는 감성 분석 논문에 대해 리뷰해보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Deep Learning-Based Document Modeling for Personality Detection from Text (N. Majumder, S. Poria and A. Gelbukh and E. Cambria)&lt;/em&gt; 의 논문을 기초로 리뷰하였으며 해당 논문은 2017년에 IEEE Computer Society에 발표된 논문입니다. 본 포스팅에서 논문에서 소개하는 프로세스와 CNN 아키텍처에 대해 살펴보고 다음 포스팅에서는 이 논문을 활용하여 실제 실습 프로젝트를 진행해보겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;objective&quot;&gt;Objective&lt;/h3&gt;
&lt;p&gt;본 논문의 목표는 주어진 Text에서 저자의 Personality를 detection하는 것입니다. 일종의 감성 분석이라고 볼 수 있는데요. 논문의 저자는 찾아내고자 하는 특성으로 5가지 Personality traits를 제안합니다. 따라서 &lt;strong&gt;5-level classification&lt;/strong&gt; 문제라고 정의할 수 있습니다.
​
&lt;img src=&quot;https://decision-J.github.io/assets/Personality_detection/traits.PNG&quot; alt=&quot;PNG&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;process&quot;&gt;Process&lt;/h3&gt;
&lt;p&gt;Classification을 진행하는 절차는 다음과 같습니다. 하나하나 차근차근 살펴보겠습니다.&lt;/p&gt;

&lt;h4 id=&quot;1-data-input&quot;&gt;1. Data Input&lt;/h4&gt;
&lt;p&gt;분석을 진행하기 위해 Text data를 확보합니다. 여기서 중요한 것은 각 Text data마다 5 class에 대한 &lt;strong&gt;label&lt;/strong&gt;이 붙어있어야 한다는 것입니다. 이는 뒤에서 소개할 모델링이 Supervised learning으로 구성되어 있기 때문인데, 본 논문의 아쉬운 점 중 하나라고 생각합니다. (새로운 text data를 구할 때마다 5가지 특성에 대한 class를 새로 달아주어야 하기 때문이죠.) 저자는 &lt;em&gt;James Pennebaker and Laura King’s stream-of-consciousness essay dataset&lt;/em&gt;을 사용하였고 2,467개의 essay data가 포함되어 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;2-preprocessing--filtering&quot;&gt;2. Preprocessing &amp;amp; Filtering&lt;/h4&gt;
&lt;p&gt;Data를 받고나면 전처리 과정을 거쳐서 model에 넣어주어야 합니다. 본 방법론은 단어 수준의 vector에서부터 패턴을 파악하여 전체 글의 특성을 파악하는 bottom-up 분석 방법이기 때문에 text data를 word level로 분해해주어야 합니다. 주로 ., ?, ! 등의 문장부호를 기준으로 하거나 띄어쓰기를 기준으로 분해해줍니다.&lt;/p&gt;

&lt;p&gt;이후에는 Filtering단계를 거칩니다. 저자는 생성된 단어 벡터들 중 emotion이 포함되지 않은 단어는 분석에서 제외합니다. Emotion이 포함된 단어 벡터를 저자는 “Emotionally Charged Vector”라고 합니다. 이러한 ECV를 찾기 위해서 &lt;em&gt;NRC Emotion Lexicon&lt;/em&gt;을 참고합니다. Lexicon에는 10개의 emotion으로 tagging된 6,468개의 단어가 포함되어 있습니다. 저자는 단어 벡터들 중 이 lexicon에 포함되지 않은 단어들은 분석 대상에서 제거합니다. 이러한 Filtering 작업은 모델의 성능을 높혀준다고 밝혀져 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;3-modeling&quot;&gt;3. Modeling&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/Personality_detection/architecture.PNG&quot; alt=&quot;PNG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 본격적으로 CNN Model의 Architecture에 대해 살펴보겠습니다.&lt;br /&gt;
Model에서 중요한 layer는 다음과 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Input layer&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Convolution layer&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Max-pooling layer&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Concatenation layer&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;1-max pooling layer&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fully connected layer&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Output layer&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 중에서 Input, Convolution, Max-pooling layer는 Word-level에서 분석이 진행되고 Concatenation layer는 Sentence-level에서 진행됩니다. 마지막 1-max pooling, Fully connected, Output layer는 Document-level에서 최종적으로 classification을 수행합니다. 이제 각각에 대해 알아보도록 하겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Input layer&lt;/strong&gt;
Input layer에 투입되는 text data는 4-dimensional array로 구성됩니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$
\begin{gathered}
\Re^{D\times S\times W\times E}&lt;br /&gt;
\textit{where}\ D = \textit{Number of documents}&lt;br /&gt;
S = \textit{Maximum number of sentences}&lt;br /&gt;
W = \textit{Maximum number of words}&lt;br /&gt;
E = \textit{Length of word embeddings}
\end{gathered}
$$&lt;/p&gt;

&lt;p&gt;여기서 S, W는 단어와 문장의 최대값으로 표현되는 데, 이보다 적은 단어 혹은 문장을 가진 벡터의 경우 padding을 통해 채워줍니다. 또한 단어들의 관계에 대한 word representation이 이루어져야 하므로 embedding space를 활용하게 되는 데 이 때의 dimension이 E로 표현됩니다. 본 논문에서는 구글의 word2vec을 사용하므로 E는 300이 되겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Convolution layer&lt;/strong&gt;
Convolution layer에서는 n-gram filter로 word vector의 feature map을 생성합니다. 본 논문에서는 uni, bi, trigram filter 3가지 종류를 사용하고, 각 filter마다 200개씩의 개수를 가지고 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$
\begin{gathered}
F_n^{conv} \in \Re^{200 \times n \times E} &lt;br /&gt;
FM_n \in \Re^{200 \times (W-n+1) \times 1} &lt;br /&gt;
\textit{where}\,\,\, n = 1,2,3
\end{gathered}
$$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Max pooling layer&lt;/strong&gt;
각 Feature map의 대표 특성만을 추출하면서 down-sizing 해주기 위해 max pooling layer를 거칩니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$
\begin{gathered}
DFM_n \in \Re^{200 \times 1 \times 1}
\end{gathered}
$$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Concatenation layer&lt;/strong&gt;
지금까지 word-level에서의 feature 추출 작업을 진행했습니다. 이를 결합해서 sentence-level에 해당하는 vector를 생성해야 합니다. Concatenation layer에서는 max pooling을 거친 feature vector를 flatten한 뒤 결합하여 sentence-level vector를 만듭니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$
\begin{gathered}
s_i \in \Re^{600} &lt;br /&gt;
\textit{where i is the number of sentences}
\end{gathered}
$$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;1-max pooling layer&lt;/strong&gt;
이제 각 sentence vector들의 최대 특성을 추출하여 document-level의 vector를 생성해줍니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$
\begin{gathered}
d^{network} = max(s_i, s_j) \in \Re^{600}
\end{gathered}
$$&lt;/p&gt;

&lt;p&gt;이 때 본 논문에서는 document자체의 특성에 더하여 $\textit{Mairesse}$ (2007)의 document feature를 추가해줍니다. 이 벡터는 document의 feature를 detection하는 데 도움을 주는 역할입니다. 총 84개의 feature를 포함하고 있습니다. 따라서 이를 모두 합쳐 classification에 넣을 vector를 생성합니다.&lt;/p&gt;

&lt;p&gt;$$
\begin{gathered}
d^{concat} = (d^{network}, d^{Mairesse}) \in \Re^{684}
\end{gathered}
$$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Fully connected &amp;amp; Output layer&lt;/strong&gt;
구해진 최종 document vector를 사용하여 Classification할 Personality에 해당하는지 아닌지 softmax function을 활용하여 확률을 예측합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$
\begin{gathered}
 p(i|\theta) = \frac{exp(x_i)}{exp(x_{yes})+exp(x_{no})}, \textit{for i} \in \text{[yes, no]}
\end{gathered}
$$&lt;/p&gt;

&lt;p&gt;여기까지 아키텍처를 구성하는 7가지의 layer들을 살펴보았습니다. 본 모델에 train이 필요한 부분은 총 3가지 layer입니다. Convolution layer에서의 word filter, Fully connectec layer에서의 weight와 bias, Softmax layer에서의 weight, bias이 이에 해당합니다. 저자는 이 파라미터들을 &lt;strong&gt;Negative log likelihood&lt;/strong&gt;를 loss function으로 하여 &lt;strong&gt;Ada delta method&lt;/strong&gt;로 학습시키고 있습니다.&lt;/p&gt;

&lt;p&gt;여기까지 논문의 리뷰를 마치고 다음 포스팅에서 본 모델을 활용한 기사 분류 프로젝트를 실습해보겠습니다!
​&lt;/p&gt;
</description>
        <pubDate>Sat, 03 Oct 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/text-mining/2020/10/03/DeepLearning-Based-Personality-detection-from-Text.html</link>
        <guid isPermaLink="true">http://localhost:4000/text-mining/2020/10/03/DeepLearning-Based-Personality-detection-from-Text.html</guid>
        
        <category>text-mining</category>
        
        
        <category>Text-mining</category>
        
      </item>
    
      <item>
        <title>[Dacon] COVID-19 Modeling</title>
        <description>&lt;h2 id=&quot;covid-19-modeling&quot;&gt;COVID-19 Modeling&lt;/h2&gt;

&lt;h3 id=&quot;spatio-variation-of-covid-19-spread-focusing-on-infection-hotspot&quot;&gt;Spatio variation of COVID-19 spread focusing on infection Hotspot&lt;/h3&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;이번 포스팅에서는 앞 서 EDA를 진행한 데이터들을 바탕으로 Modeling에 들어가보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;본 게시글은 연세대학교 응용통계학과 대학원 “시공간 자료분석 (박재우 교수님)” 수업의 파이널 프로젝트였음을 미리 밝힙니다.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;objective&quot;&gt;Objective&lt;/h3&gt;

&lt;p&gt;저는 EDA를 진행하던 중 확진자들의 감염 경로에서 다양한 집단 발병지에 주목했습니다. 20년 초 신천지 case가 그러했듯이, 하나의 집단 발병지역이 발생한 후에 그 주변 지역으로 확진자 전파의 효과가 크며 이를 예측한다면 COVID-19 대처에 유용할 것이라고 생각했습니다. 따라서 &lt;strong&gt;집단 발병지역을 기준&lt;/strong&gt;으로 COVID-19가 주변 지역으로 퍼져나가는 것을 예측하는 것이 본 분석의 목적입니다.&lt;/p&gt;

&lt;p&gt;우선은 집단 발병으로 인한 확진자 발생 현상이 뚜렷했던 대구/경북 지역을 중심으로 모델을 fitting한 뒤 전국으로 확대해보겠습니다. 아래 그림은 대구/경북 지역의 집단 발병 지역을 나타낸 plot입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/covid/Hotspots.png&quot; alt=&quot;PNG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 16개의 집단 발병 지역(ex. 신천지 교회, 청도 대남병원 등)을 기준으로 주변 지역의 전파를 예측해보겠습니다. 전파 추세를 살펴볼 때는 각 지역의 &lt;strong&gt;첫 확진자 발생 날짜&lt;/strong&gt;를 기준으로 살펴보고자 합니다. 최종적으로 다음의 plot을 예측하는 것이 목표입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/covid/FirstDate.png&quot; alt=&quot;PNG&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;methodology&quot;&gt;Methodology&lt;/h3&gt;

&lt;h4 id=&quot;predict-the-date&quot;&gt;Predict the date&lt;/h4&gt;

&lt;p&gt;저는 분석을 위해 Bayesian inference를 통한 Hierarchical SGLMM을 이용하고자 합니다. 일단 예측하고자 하는 첫 확진자 발생 날짜를 $Y$라고 한다면 우리의 데이터가 point-reference 데이터이므로 다음과 같이 표현이 가능합니다.&lt;/p&gt;

&lt;p&gt;$$
\begin{gathered}
\mathbf{Y(s)} = {Y(s_1), Y(s_2), … , Y(s_n)},  s_i \in \Re^2\\textit{where } {s_1, s_2, … , s_n}  \textit{are locations}
\end{gathered}
$$&lt;/p&gt;

&lt;p&gt;이러한 $\mathbf{Y(s)}$를 예측하기 위해 Gaussian process를 이용한 Kriging으로 일종의 선형식을 fitting합니다. 사용되는 항으로는 첫 째로 $\mathbf{Y(s)}$의 추세를 예측할 mean function, 둘 째로 spatial correlation effect를 반영해주는 term, 마지막으로 불규칙성을 반영할 nugget term이 존재합니다.&lt;/p&gt;

&lt;p&gt;여기서 mean function을 예측할 때 3가지 변수가 사용되며 각각 위도, 경도, 집단 발병지의 확진자 수가 사용됩니다. 집단 발병지의 확진자 수를 세부 변수로 넣는 이유는 집단 발병의 심각성에 따라 확진자 전파에 weight를 다르게 부여하고 싶었기 때문입니다.&lt;/p&gt;

&lt;p&gt;$$
\begin{gathered}
\mathbf{Y(s)} = \mathbf{\mu(s)} + \mathbf{w(s)}+\mathbf{\epsilon(s)}&lt;br /&gt;
 \textit{where } \mathbf{\mu(s)}=\mathbf{\beta_0}+\mathbf{\beta_1 X_1} +\mathbf{\beta_2 X_2}+\mathbf{\beta_3 X_3},&lt;br /&gt;
 \mathbf{w(s)} \sim GP(0, K(\cdot)) \textit{ with Matern covariance},&lt;br /&gt;
 \mathbf{\epsilon(s)} \sim N(0, \tau^2)
\end{gathered}
$$&lt;/p&gt;

&lt;p&gt;위 식에서는 총 4가지의 변수가 fitting되어야 합니다. 먼저 mean function에서 사용한 변수들에 대한 $\beta$, spatial correlation 반영을 위해 사용한 Matern covariance 내부의 변수 $\sigma^2$, $\phi$, 불규칙 nugget term의 변수 $\tau^2$가 바로 그 것입니다. 우리는 Hierarchical MCMC를 통해서 각 변수들을 estimate해보겠습니다. 먼저 MCMC를 위해 각 변수들에 대한 prior를 줍니다. 최대한 non-informative하면서 conjugate한 prior로 선정합니다.&lt;/p&gt;

&lt;p&gt;$$
\begin{gathered}
\mathbf{\theta} = (\beta_0, \beta_1, \beta_2, \beta_3, \sigma^2, \phi, \tau^2)&lt;br /&gt;
\textit{where priors are }&lt;br /&gt;
p(\mathbf{\beta}) \sim N(m_{\beta}, V_{\beta})&lt;br /&gt;
p(\sigma^2) \sim IG(a_{\sigma^2},b_{\sigma^2})&lt;br /&gt;
p(\phi) \sim U(a_{\phi},b_{\phi})&lt;br /&gt;
p(\tau^2) \sim IG(a_{\tau^2},b_{\tau^2})
\end{gathered}
$$&lt;/p&gt;

&lt;p&gt;이 후 20만번의 iteration을 통해서 변수들의 estimate을 구합니다.&lt;/p&gt;

&lt;p&gt;$$
\begin{gathered}
\mathbf{\hat{\theta}} = \dfrac{1}{n}\sum_{i=1}^{n}\mathbf{\theta_i} \textit{, where } n=200,000
\end{gathered}
$$&lt;/p&gt;

&lt;p&gt;(MCMC의 수렴 결과는 다음과 같습니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/covid/MCMC.PNG&quot; alt=&quot;PNG&quot; width=&quot;200&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;predict-the-direction&quot;&gt;Predict the direction&lt;/h4&gt;

&lt;p&gt;위의 과정으로 $\mathbf{Y(s)}$를 fitting한다면 일종의 estimate된 함수 공간을 갖게 됩니다. 이 때 집단 발병 point를 기준으로 공간의 gradient의 derivative $\nabla\mathbf{\hat{Y(s)}}$를 구할 수 있습니다. 이렇게 구해진 gradient derivative를 기준으로 COVID-19의 전파에 대한 dominant effect direction을 예상해볼 수 있으며 이의 length, $\mid\nabla\mathbf{\hat{Y}(s)}\mid$를 이용하여 전파의 범위까지 예측할 수 있습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt;

&lt;p&gt;최종적으로 예측된 결과를 plot으로 그려주면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/covid/result_arrow.png&quot; alt=&quot;PNG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;집단 발병 지역을 기준으로 전파 범위에 대한 예측이 화살표로 표시되어 있습니다. 이 예측이 실제로 정확한 지 글 초반에 target map에 중첩하여 표현해보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/covid/result_targetmap.png&quot; alt=&quot;PNG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;화살표의 방향과 색깔이 지역별 색깔과 비슷할수록 정확한 분석입니다. 대체로 첫 확진자 날짜와 비슷하게 예측하는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;마지막으로 예측 범위를 전국으로 확대해보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/covid/result_korea.png&quot; alt=&quot;PNG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;대구/경북 지역을 제외한 전국의 집단 발병지를 중심으로 전파 범위를 예측한 결과입니다.
범위 확장에 따라 본 분석의 두 가지 한계점이 드러납니다. 먼저 첫 번째로 분석 대상 자료가 point-reference 데이터이기 때문에 강원도와 같이 집단 발병 point가 없다면 예측이 어렵습니다. 또한 부산 지역의 집단 발병지에서 남해 지역으로 화살표가 뻗어나가는 것을 볼 수 있는데 이는 육지에 대한 정보가 없기 때문입니다. 따라서 model fitting 단계에서 사람들이 거주하는 지역에 대한 boundary를 넣어주어야 보다 정확한 분석이 가능할 것입니다.&lt;/p&gt;
</description>
        <pubDate>Fri, 19 Jun 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/competition/2020/06/19/COVID-19-Modeling.html</link>
        <guid isPermaLink="true">http://localhost:4000/competition/2020/06/19/COVID-19-Modeling.html</guid>
        
        <category>competition</category>
        
        
        <category>Competition</category>
        
      </item>
    
      <item>
        <title>[Dacon] COVID-19 EDA</title>
        <description>&lt;h2 id=&quot;covid-19-visualization-competition-by-dacon&quot;&gt;COVID-19 Visualization Competition by DACON&lt;/h2&gt;

&lt;p&gt;이번 포스팅에서는 3월 29일부터 5월 10일까지 진행된 DACON 주관 COVID-19 바이러스의 시각화 공모전의 내용을 담아보겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;data-merge&quot;&gt;Data Merge?&lt;/h3&gt;
&lt;p&gt;Dacon에서 주어진 데이터 셋은 총 10개입니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;n&quot;&gt;files&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dataset/*.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
 &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;files&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;nb&quot;&gt;globals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

 &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; ['Case','PatientInfo','PatientRoute','Region','SearchTrend',
  'Time','TimeAge','TimeGender','TimeProvince','Weather']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;크게 &lt;strong&gt;환자와 관련된 자료&lt;/strong&gt;, &lt;strong&gt;시계열 자료&lt;/strong&gt;, &lt;strong&gt;지역 연관 자료&lt;/strong&gt;, &lt;strong&gt;검색어 트렌드&lt;/strong&gt; 로 데이터를 구분지을 수 있을 것 같습니다. (Key 값 기준)
최대한 많은 정보를 한 셋에 묶어서 시각화할 수 있도록 데이터들을 Merge하고 분석을 진행할 예정입니다.
(하지만 먼저 개별 데이터셋을 살펴보고 Merge를 생각해보겠습니다.)&lt;/p&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;eda&quot;&gt;EDA&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/covid/time1.jpg&quot; alt=&quot;jpg&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 plot은 날짜가 지남에 따라 확진자와 사망자 수의 증가 추이를 보여주는 시계열 플랏입니다. 확진자 수가 늘어나는 추세와 사망자 수가 늘어나는 추세가 비슷한 모양을 보입니다. 특히 확진자의 증가 폭이 2월 말에서 3월 초 폭발적으로 늘어난 점을 알 수 있습니다&lt;/p&gt;

&lt;p&gt;이제 &lt;strong&gt;TimeProvince&lt;/strong&gt; 데이터를 통해 지역별 확진자 수를 살펴봅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/covid/time2.jpg&quot; alt=&quot;jpg&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Plotly express는 다이나믹 시각화를 제공하지만 markdown에 넣는 방법을 아직 구글링중입니다 ㅠㅠ)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;본 공모전은 3월 20일까지의 데이터가 주어져 있습니다. 역시 대구, 경북 지역의 확진자가 가장 많이 보여짐을 알 수 있습니다. 이 두 지역을 제외하고 다른 곳의 확진자 수를 좀 더 자세히 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/covid/time3.jpg&quot; alt=&quot;jpg&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/covid/time8.PNG&quot; alt=&quot;PNG&quot; width=&quot;600&quot; height=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;250명 전까지의 확진자 수를 보여주는 플랏입니다. 서울, 경기 지역이 압도적인 2위 그룹임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 데이터를 날짜별 시계열 데이터로 나타내어 지역별 증가 추이를 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/covid/time5.jpg&quot; alt=&quot;jpg&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;2월 25일을 기점으로 폭발적으로 대구 지역 확진자가 증가하는 것을 알 수 있습니다. 신천지 예배 영향인 것으로 생각됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/covid/time4.jpg&quot; alt=&quot;jpg&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음은 안타까운 지역별 사망자 플랏입니다. 확진자 플랏과 비슷하게 대구, 경북 지역에 집중되어 있습니다. 앞으로의 시각화를 통해 사망자 수와 확진자 수가 강력한 양의 선형 관계를 보이는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;날짜별 지역 데이터 다음으로 &lt;strong&gt;연령대&lt;/strong&gt;와 &lt;strong&gt;성별&lt;/strong&gt; 데이터를 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/covid/time6.PNG&quot; alt=&quot;PNG&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/covid/time7.PNG&quot; alt=&quot;PNG&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;잘 알려져있는 대로 다른 나이대에 비하여 20대 확진자 수가 가장 많습니다. 또한 남성에 비해 여성이 더 코로나 바이러스 감염에 취약한 것으로 보입니다.&lt;/p&gt;

&lt;p&gt;마지막으로 날짜별 &lt;strong&gt;감염 경로&lt;/strong&gt; 증가 추세를 알아보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/covid/time9.PNG&quot; alt=&quot;PNG&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Contact with patient, 즉, 감염자와의 접촉이 가장 큰 감염 경로인 것을 확인할 수 있습니다. 전염성이 높은 COVID-19의 위험성을 잘 보여줍니다.&lt;/p&gt;

&lt;p&gt;다음 포스팅에서는 위 데이터들을 활용하여 Hierarchical SGLMM을 통한 지역별 첫 확진자 발생 추이 예측 프로젝트에 대해 다루겠습니다.&lt;/p&gt;

</description>
        <pubDate>Tue, 24 Mar 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/competition/2020/03/24/COVID-19-EDA.html</link>
        <guid isPermaLink="true">http://localhost:4000/competition/2020/03/24/COVID-19-EDA.html</guid>
        
        <category>competition</category>
        
        
        <category>Competition</category>
        
      </item>
    
      <item>
        <title>[Kaggle] Walmart SaleType Classification</title>
        <description>&lt;h2 id=&quot;kaggle-study-1---walmart-saletype-classification&quot;&gt;Kaggle Study #1. - Walmart SaleType Classification&lt;/h2&gt;

&lt;p&gt;​	이번 포스팅에서 다룰 Kaggle competition은 Walmart SaleType Classification입니다. Walmart에서 제공되는 고객들의 구매이력 data를 바탕으로 해당 고객의 구매타입을 예측해보는 competition입니다. 우선 dataset의 기본적인 형태부터 살펴보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;./train.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;TripType&lt;/th&gt;
      &lt;th&gt;VisitNumber&lt;/th&gt;
      &lt;th&gt;Weekday&lt;/th&gt;
      &lt;th&gt;Upc&lt;/th&gt;
      &lt;th&gt;ScanCount&lt;/th&gt;
      &lt;th&gt;DepartmentDescription&lt;/th&gt;
      &lt;th&gt;FinelineNumber&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;999&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;6.811315e+10&lt;/td&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;FINANCIAL SERVICES&lt;/td&gt;
      &lt;td&gt;1000.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;6.053882e+10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SHOES&lt;/td&gt;
      &lt;td&gt;8931.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;7.410811e+09&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;PERSONAL CARE&lt;/td&gt;
      &lt;td&gt;4504.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;2.238404e+09&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;PAINT AND ACCESSORIES&lt;/td&gt;
      &lt;td&gt;3565.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;2.006614e+09&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;PAINT AND ACCESSORIES&lt;/td&gt;
      &lt;td&gt;1017.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;train set의 shape은&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/ test set의 shape은&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;train set의 shape은 (647054, 7) / test set의 shape은 (653646, 6)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;​	Dataset에는 총 7개의 변수가 있으며, 이 중 TripType이 우리가 맞추어야할 구매 타입 변수입니다. Train과 Test set에 약 65만 건의 구매 정보가 포함되어 있습니다. 이제 이 데이터의 특징에 대해 살펴보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;data-eda&quot;&gt;Data EDA&lt;/h2&gt;

&lt;h3 id=&quot;triptype&quot;&gt;TripType&lt;/h3&gt;

&lt;p&gt;​	Competition description을 살펴보면 TripType변수의 999 값은 “기타” 항목입니다. plot의 가독성을 위해 이 값을 -1로 변경한 뒤 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/walmart/output_3_0.jpg&quot; alt=&quot;png&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​	Type값이 0~44까지 존재하고 있음을 알 수 있습니다. 상당히 많은 multi-label classification 문제 입니다. 또 한가지 주목할만한 특징은 많은 타입들 중 39와 40번 타입이 눈에 띄게 많다는 점입니다. 특히 40번 type은 굉장히 많은 빈도를 보여줍니다. 따라서 이 두 가지 type에 대한 특징을 잘 잡아내는 것이 중요할 것으로 판단됩니다.&lt;/p&gt;

&lt;h3 id=&quot;weekday&quot;&gt;Weekday&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/walmart/output_3.jpg&quot; alt=&quot;png2&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;
​
​	Weekday변수는 물건을 구매한 요일에 대한 정보입니다. 아무래도 휴일인 금, 토, 일에 대한 정보가 많은 것이 확인됩니다. 평일과 주말로 구분하여 분석하는 것도 좋은 방법일 것 같습니다.&lt;/p&gt;

&lt;h3 id=&quot;scancount&quot;&gt;ScanCount&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://decision-J.github.io/assets/walmart/output_3.1.jpg&quot; alt=&quot;png3&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;
​&lt;/p&gt;

&lt;p&gt;​	ScanCount 변수는 고객이 구입한 물건의 개수에 관한 정보입니다. 주로 1개의 물건을 구입하는 고객이 많은 것을 확인할 수 있습니다. 최대 많은 물품을 구입한 개수는 5개입니다. -1값은 고객이 물건을 “반품” 처리 하는 경우입니다. 그 수가 많지는 않지만 특수한 Type을 구분하는 데 도움이 될 수 있을 것 같습니다. 이에 어떤 품목이 가장 많이 반품되는지 알아봤습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ScanCount'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;DepartmentDescription&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FINANCIAL SERVICES    1138
LADIESWEAR             786
PRODUCE                687
PERSONAL CARE          624
MENS WEAR              618
DSD GROCERY            599
GROCERY DRY GOODS      524
PHARMACY OTC           439
ELECTRONICS            436
BEAUTY                 426
Name: DepartmentDescription, dtype: int64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;​	반품이 가장 많은 상위 10개 품목을 뽑았습니다. 금융 서비스가 가장 취소가 많은 것으로 나타납니다. 또한 의류,  약품 같은 생필품의 반품이 가장 많은 것으로 보입니다.&lt;/p&gt;

&lt;h3 id=&quot;upc-departmentdescription-finelinenumber&quot;&gt;UPC, DepartmentDescription, FinelineNumber&lt;/h3&gt;

&lt;p&gt;​	UPC와 DepartmentDescription, FinelineNumber 이 3가지 변수는 한꺼번에 살펴보겠습니다. 그 이유는 첫 째,  전체 data set에서 결측값이 존재하는 변수들이기 때문입니다. 둘 째, UPC와 FinelineNumber변수는 유사도가 높습니다. 먼저 이들의 결측값에 대해 살펴보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isnull&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;VisitNumber&lt;/span&gt;                 &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Weekday&lt;/span&gt;                     &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Upc&lt;/span&gt;                      &lt;span class=&quot;mi&quot;&gt;3986&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ScanCount&lt;/span&gt;                   &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;DepartmentDescription&lt;/span&gt;    &lt;span class=&quot;mi&quot;&gt;1328&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;FinelineNumber&lt;/span&gt;           &lt;span class=&quot;mi&quot;&gt;3986&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;int64&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;​	UPC와 FinelineNumber는 정확히 동일한 결측값의 수를 가집니다. 두 변수는 모두 Walmart에서 고유하게 부여한 상품에 대한 정보인데, 결측값의 추이로 보아 동일한 정보를 담고 있는 것으로 보여집니다. 따라서 추후 분석에서는 두 변수를 모두 사용하지 않고 FinelineNumber만 이용하겠습니다. (FinelineNumber를 선택한 이유는 UPC에 비해 자료의 값이 더 직관적으로 표현되어 있기 때문입니다.) 또한 DepartmentDescription 1,328개 변수도 FinelineNumber Missing case에 포함됩니다.&lt;/p&gt;

&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;TripType&lt;/th&gt;
      &lt;th&gt;VisitNumber&lt;/th&gt;
      &lt;th&gt;Weekday&lt;/th&gt;
      &lt;th&gt;Upc&lt;/th&gt;
      &lt;th&gt;ScanCount&lt;/th&gt;
      &lt;th&gt;DepartmentDescription&lt;/th&gt;
      &lt;th&gt;FinelineNumber&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;25&lt;/th&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;548&lt;/th&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;259&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;549&lt;/th&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;259&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;959&lt;/th&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;409&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1116&lt;/th&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;479&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;643137&lt;/th&gt;
      &lt;td&gt;41&lt;/td&gt;
      &lt;td&gt;190408&lt;/td&gt;
      &lt;td&gt;Sunday&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;643991&lt;/th&gt;
      &lt;td&gt;44&lt;/td&gt;
      &lt;td&gt;190651&lt;/td&gt;
      &lt;td&gt;Sunday&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;645990&lt;/th&gt;
      &lt;td&gt;44&lt;/td&gt;
      &lt;td&gt;191080&lt;/td&gt;
      &lt;td&gt;Sunday&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;645991&lt;/th&gt;
      &lt;td&gt;44&lt;/td&gt;
      &lt;td&gt;191080&lt;/td&gt;
      &lt;td&gt;Sunday&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;645992&lt;/th&gt;
      &lt;td&gt;44&lt;/td&gt;
      &lt;td&gt;191080&lt;/td&gt;
      &lt;td&gt;Sunday&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;3986 rows × 7 columns&lt;/p&gt;

&lt;p&gt;​	그렇다면 FinelineNumber의 값이 Missing이지만 DepartmentDescription의 값이 Not Missing인 case는 어떤 것일까요?&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'FinelineNumber'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isnull&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'DepartmentDescription'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;notnull&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'DepartmentDescription'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PHARMACY RX    2768
Name: DepartmentDescription, dtype: int64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;​	&lt;em&gt;PHARMACY RX&lt;/em&gt; 라는 품목이었습니다. 단일 품목만이 조회되는 것으로 보아 해당 품목에 대한 FinelineNumber 변수 입력 처리에 어떤 문제가 있었던 것으로 보여집니다.&lt;/p&gt;

&lt;h2 id=&quot;base-line-model&quot;&gt;Base-line Model&lt;/h2&gt;

&lt;p&gt;​	우선 가장 간단하게 base-line model을 만들어서 성능을 체크해보겠습니다. Base-line model이니 만큼 가장 간단하게 building하여 성능의 바로미터로 삼겠습니다.&lt;/p&gt;

&lt;p&gt;​	먼저 EDA를 통해 얻은 정보를 바탕으로 데이터 전처리 과정을 거쳐봅니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;UPC 변수 제거&lt;/li&gt;
  &lt;li&gt;FinelineNumber 변수 결측값 제거&lt;/li&gt;
  &lt;li&gt;Weekday변수: 금토일 = 1, 평일 = 0 으로 범주화&lt;/li&gt;
  &lt;li&gt;문자형 변수 DepartmentDescription 제거&lt;/li&gt;
  &lt;li&gt;VisitNumber 기준 중복 행 제거&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;​	최대한 간단하게 model을 construct하는 것이 목적이므로 정보의 손실을 감수하고 과감히 변수들을 제거하였습니다. Building한 model의 spec을 살펴보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;build_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],)))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;38&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'softmax'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rmsprop'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'categorical_crossentropy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;​	Keras를 사용하여 Deep Learning 모델을 구축하였으며 3개의 층을 쌓았습니다. rsmprop optimizer를 사용하고 categorical_crossentropy를 loss function으로 사용하였습니다. Metric은 Customize를 통해 build-up할 수 있으나 간단하게 accuracy를 사용하였습니다.&lt;/p&gt;

&lt;p&gt;​	3-fold로 모델을 학습한 결과 Kaggle leader보드 기준 &lt;strong&gt;2.36&lt;/strong&gt; 의 score를 기록했습니다.&lt;/p&gt;

&lt;h2 id=&quot;advanced-model-with-feature-engineering&quot;&gt;Advanced Model with Feature Engineering&lt;/h2&gt;

&lt;p&gt;​	다음으로 Feature engineering을 거친 보다 발전된 model을 만들어보도록 하겠습니다. Department Description변수와 FinelineNumber 변수를 기준으로 파생변수를 생성하는 작업을 진행해 보겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;departmentdescription-파생변수&quot;&gt;DepartmentDescription 파생변수&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;해당 작업은 Coursera: Learning from the Top Kagglers 강의를 참고하였음을 밝힙니다.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;​	앞서 base-line model에서는 DepartmentDescription 변수를 제거하고 분석을 진행했습니다. 그 이유는 Visit Number당 중복 정보가 존재하기 때문이었습니다. 이 것이 어떤 점이 문제인지 살펴보겠습니다. 먼저 주어진 data는 구매이력 데이터입니다. 즉, VisitNumber는 고객의 ID이며 VisitNumber 하나당 구매 물품 정보들이 중복되어 들어 있습니다. 예를 들어, 다음과 같은 사례를 살펴보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;VisitNumber&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;TripType&lt;/th&gt;
      &lt;th&gt;VisitNumber&lt;/th&gt;
      &lt;th&gt;Weekday&lt;/th&gt;
      &lt;th&gt;Upc&lt;/th&gt;
      &lt;th&gt;ScanCount&lt;/th&gt;
      &lt;th&gt;DepartmentDescription&lt;/th&gt;
      &lt;th&gt;FinelineNumber&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;55&lt;/th&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;7.675336e+09&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;IMPULSE MERCHANDISE&lt;/td&gt;
      &lt;td&gt;8904.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;56&lt;/th&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;6.115665e+10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;JEWELRY AND SUNGLASSES&lt;/td&gt;
      &lt;td&gt;556.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;57&lt;/th&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;8.874396e+10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;MENS WEAR&lt;/td&gt;
      &lt;td&gt;144.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;58&lt;/th&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;6.926568e+11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;FABRICS AND CRAFTS&lt;/td&gt;
      &lt;td&gt;397.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;59&lt;/th&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;7.675336e+09&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;IMPULSE MERCHANDISE&lt;/td&gt;
      &lt;td&gt;8904.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;60&lt;/th&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;6.953344e+11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;ACCESSORIES&lt;/td&gt;
      &lt;td&gt;122.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;61&lt;/th&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;8.853064e+10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;MENS WEAR&lt;/td&gt;
      &lt;td&gt;5201.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;62&lt;/th&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;8.830961e+10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;MENS WEAR&lt;/td&gt;
      &lt;td&gt;5661.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;63&lt;/th&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;3.181070e+09&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;HOME MANAGEMENT&lt;/td&gt;
      &lt;td&gt;8124.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;​	위 자료는 VisitNumber가 19인 고객의 정보만을 따로 추려낸 것입니다. 자료를 살펴보면 19번 고객이 같은 날에 총 9개의 품목을 구매하였음을 확인할 수 있습니다. DepartmentDescription 변수를 보면 다양한 품목들을 구매했습니다. 하지만 이 모든 행의 TripType이 동일하게 42로 규정되어 있습니다. 즉, Walmart 측에서는 이 9개 품목의 정보를 전부 반영하여 (동일한 날의 구매 정보를 모두 반영하여) 고객의 구매 Type을 정하고 있음을 알 수 있습니다. 따라서 최종 모델에 VisitNumber를 기준으로 중복제거를 할 시 나머지 품목들의 정보가 모두 사라지게 되어 정확한 분석이 불가능해지게 됩니다.&lt;/p&gt;

&lt;p&gt;​	따라서 모든 품목에 대한 정보를 반영하는 작업을 거칠 것입니다. 그 방법은 VisitNumber를 기준으로 DepartmentDescription 정보를 횡으로 늘어 놓는 것입니다. 다음의 예시를 보면 이해가 쉽습니다.&lt;/p&gt;

&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;VisitNumber&lt;/th&gt;
      &lt;th&gt;1-HR PHOTO&lt;/th&gt;
      &lt;th&gt;ACCESSORIES&lt;/th&gt;
      &lt;th&gt;AUTOMOTIVE&lt;/th&gt;
      &lt;th&gt;BAKERY&lt;/th&gt;
      &lt;th&gt;BATH AND SHOWER&lt;/th&gt;
      &lt;th&gt;BEAUTY&lt;/th&gt;
      &lt;th&gt;BEDDING&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;94242&lt;/th&gt;
      &lt;td&gt;191343&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;94243&lt;/th&gt;
      &lt;td&gt;191344&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;94244&lt;/th&gt;
      &lt;td&gt;191345&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;94245&lt;/th&gt;
      &lt;td&gt;191346&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;94246&lt;/th&gt;
      &lt;td&gt;191347&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;​	DepartmentDescription 변수에 해당하는 모든 값들을 새로운 변수로 추가합니다. 이 후 해당 품목이 있으면 1, 없으면 0을 반환하는 one-hot encoding을 통해 변수 정보를 추가해주면 됩니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;이 방법은 수 많은 0,1 변수들을 생성하게 되어, Sparse data가 갖는 문제를 야기할 가능성이 높습니다. 일단 변수를 생성하고 분석을 진행해본 뒤 추가 논의 해보겠습니다.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;finelinenumber-파생변수&quot;&gt;FinelineNumber 파생변수&lt;/h3&gt;

&lt;p&gt;​	FinelineNumber 또한 DepartmentDescription과 동일한 문제를 갖고 있습니다. VisitNumber별로 정보가 중복된다는 것인데요. 이를 해결하기 위해 두 가지 파생 변수를 만들어보고자 합니다. 첫 째, &lt;strong&gt;FinelineNumber의 Count를 활용&lt;/strong&gt;하는 변수, 둘 째, &lt;strong&gt;FinelineNumber의 CountSum&lt;/strong&gt;을 활용하는 방법입니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;FinelineNumber의 Count값을 활용&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;​	FinelineNumber는 0~2,000까지 다양한 값을 갖습니다. 하지만 품목 별로 고객들이 자주 구매하는 품목, 자주 구매하지 않는 품목으로 나뉩니다. 따라서 각 FinelineNumber들을 Count값으로 변환한 뒤, VisitNumber를 기준으로 그 값을 평균내어 변수로 활용해보겠습니다. 이 변수를 통해 고객이 보편적인 성향을 갖는 타입인지, 마이너한 품목을 즐겨 찾는 고객인지를 살펴볼 수 있을 겁니다.&lt;/p&gt;

    &lt;p&gt;​	먼저 FinlineNumber 값의 Count 값을 구해봅니다.&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;FineLineCount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'VisitNumber'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'FinelineNumber'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'FinelineNumber'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;VisitNumber&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;FinelineNumber&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0.0&lt;/th&gt;
      &lt;td&gt;3837&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1.0&lt;/th&gt;
      &lt;td&gt;461&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2.0&lt;/th&gt;
      &lt;td&gt;224&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3.0&lt;/th&gt;
      &lt;td&gt;94&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4.0&lt;/th&gt;
      &lt;td&gt;187&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9974.0&lt;/th&gt;
      &lt;td&gt;54&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9975.0&lt;/th&gt;
      &lt;td&gt;28&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9991.0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9997.0&lt;/th&gt;
      &lt;td&gt;50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9998.0&lt;/th&gt;
      &lt;td&gt;411&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
    &lt;p&gt;5195 rows × 1 columns&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;​			이제 이 값을 dataset에 반영하여 VisitNumber를 기준으로 평균낸 변수를 구해보겠습니다.&lt;/p&gt;

&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;TripType&lt;/th&gt;
      &lt;th&gt;VisitNumber&lt;/th&gt;
      &lt;th&gt;Weekday&lt;/th&gt;
      &lt;th&gt;Upc&lt;/th&gt;
      &lt;th&gt;ScanCount&lt;/th&gt;
      &lt;th&gt;DepartmentDescription&lt;/th&gt;
      &lt;th&gt;FineLineCount&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;999&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;6.811315e+10&lt;/td&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;FINANCIAL SERVICES&lt;/td&gt;
      &lt;td&gt;836.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;6.053882e+10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SHOES&lt;/td&gt;
      &lt;td&gt;98.500000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;7.410811e+09&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;PERSONAL CARE&lt;/td&gt;
      &lt;td&gt;98.500000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;2.238404e+09&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;PAINT AND ACCESSORIES&lt;/td&gt;
      &lt;td&gt;420.045455&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;Friday&lt;/td&gt;
      &lt;td&gt;2.006614e+09&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;PAINT AND ACCESSORIES&lt;/td&gt;
      &lt;td&gt;420.045455&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;647049&lt;/th&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;191346&lt;/td&gt;
      &lt;td&gt;Sunday&lt;/td&gt;
      &lt;td&gt;3.239000e+10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;PHARMACY OTC&lt;/td&gt;
      &lt;td&gt;645.823529&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;647050&lt;/th&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;191346&lt;/td&gt;
      &lt;td&gt;Sunday&lt;/td&gt;
      &lt;td&gt;7.874205e+09&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;FROZEN FOODS&lt;/td&gt;
      &lt;td&gt;645.823529&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;647051&lt;/th&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;191346&lt;/td&gt;
      &lt;td&gt;Sunday&lt;/td&gt;
      &lt;td&gt;4.072000e+03&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;PRODUCE&lt;/td&gt;
      &lt;td&gt;645.823529&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;647052&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;191347&lt;/td&gt;
      &lt;td&gt;Sunday&lt;/td&gt;
      &lt;td&gt;4.190008e+09&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;DAIRY&lt;/td&gt;
      &lt;td&gt;1277.500000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;647053&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;191347&lt;/td&gt;
      &lt;td&gt;Sunday&lt;/td&gt;
      &lt;td&gt;3.800060e+09&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;GROCERY DRY GOODS&lt;/td&gt;
      &lt;td&gt;1277.500000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;647054 rows × 7 columns&lt;/p&gt;

&lt;p&gt;​			위 data set의 맨 오른쪽에 새로운 파생변수가 생성되었습니다. 7번 VisitNumber를 가진 고객은 98.5의 값	을 갖는 것으로 보아 다른 고객들보다 마이너한 물건 품목들을 구입했을 가능성이 높습니다. (실제로 Shoes나 	personal care와 같은 품목은 구매 빈도수가 떨어지는 제품입니다.) 반대로 191347번 고객은 1277.5를 갖습니	다. 다른 고객들이 많이 구입하는 보편적인 품목을 구매했음을 알 수 있습니다. (GROCERY DRY GOODS와 같은 	품목이 그렇습니다.)&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;FinelineNumber의 Count Sum값을 활용&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;​	이 변수는 위 변수 보다 간단합니다! 한 고객이 몇개의 FinelineNumber를 가지고 있는지 반영해주는 변수입니다. 즉, 몇개의 물품을 구매했는지에 대한 정보가 되겠습니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;FineLineSum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'VisitNumber'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'FinelineNumber'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'VisitNumber'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;FinelineNumber&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;VisitNumber&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;22&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;191343&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;191344&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;191345&lt;/th&gt;
      &lt;td&gt;13&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;191346&lt;/th&gt;
      &lt;td&gt;17&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;191347&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;95674 rows × 1 columns&lt;/p&gt;

&lt;p&gt;​		위 테이블을 data set에 붙이기만 하면 됩니다. 5번 고객은 1개의 물품밖에 구매하지 않았습니다. 이에 비해 8	번 고객은 22개의 물품이나 구매했네요! 이러한 정보들이 고객의 Type을 맞추는 데 도움을 줄 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;modeling&quot;&gt;Modeling&lt;/h3&gt;

&lt;p&gt;​	새로 생성한 파생변수들만을 가지고 Modeling을 진행해보겠습니다. Model의 스펙은 앞서 base-line model과 동일합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;build_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],)))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;38&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'softmax'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rmsprop'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'categorical_crossentropy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;​	Kaggle leaderboard기준 &lt;strong&gt;7.80&lt;/strong&gt; 의 Score를 기록했습니다. 앞서 파생변수를 생성하기 전에 비해 상당히 성능이 나빠진 모습을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;​	이는 앞서 잠시 언급했었던 data의 sparsity때문입니다. DepartmentDescription 변수를 기준으로 만든 파생변수들의 수가 많은 데다 많은 행이 0의 값을 가지고 있기 때문에 model의 성능이 나빠질 수 밖에 없습니다. 이를 해결하기 위해서는 다양한 방법론들이 존재합니다. 본 분석에서는 Deep Learning model의 layer의 Dense층을 더 늘리는 방법을 통해 이 방법을 해결해보고자 합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;build_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],)))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;38&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'softmax'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rmsprop'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'categorical_crossentropy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;​	앞 선 model에 비하여 각 층의 Dense를 128로 확장시켰습니다. 이는 변수가 늘어남에 따라 Deep Learning model이 학습할 parameter space를 확장시켜주기 위함입니다. 새로 학습한 model의 score는 &lt;strong&gt;2.27&lt;/strong&gt; 입니다. Base-line model에 대비하여 향상된 성능을 보여줍니다.&lt;/p&gt;

&lt;h2 id=&quot;cf-random-forest-simulation&quot;&gt;cf) Random Forest Simulation&lt;/h2&gt;

&lt;p&gt;​	단순한 호기심으로 Random Forest에 자료를 학습시켜 보았습니다. 각 feature들은 bayesian optimization을 통해 tuning 해주었습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bayes_opt&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BayesianOptimization&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;n_folds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;random_seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# BayesianOptimization
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_depth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_samples_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_samples_leaf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;n_estimators&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'max_depth'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_depth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'min_samples_split'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_samples_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'min_samples_leaf'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_samples_leaf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rfc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cv_result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_val_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rfc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'max_depth'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'min_samples_leaf'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'min_samples_split'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'n_estimators'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;rfc_optimization&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BayesianOptimization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;init_round&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;opt_round&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rfc_optimization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_points&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opt_round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rfc_optimization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rfc_optimization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;params&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;op_clf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;n_estimators&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;max_depth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;max_depth&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;min_samples_split&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;min_samples_split&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;min_samples_leaf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;min_samples_leaf&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;op_clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;### Get prediction
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op_clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;​	아무래도 딥러닝이 RF에 비해 추후에 나온 학습법이기 때문에 앞선 모델에 비해 성능이 안좋을 것이라고 생각했던 제 예측은 완벽히 오산이었습니다. Kaggle 기준 &lt;strong&gt;2.09&lt;/strong&gt; 로 현재까지의 model들 중 가장 뛰어난 성능을 보여줬습니다.&lt;/p&gt;

&lt;h2 id=&quot;시사점&quot;&gt;시사점&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;단순한 딥러닝 모델은 정교한 머신러닝 모델보다 못하다?&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;앞서 살펴본 것처럼 딥러닝 모델에 비해 parameter tuning을 거친 Random Forest model이 더 좋은 성능을 보여줬습니다. 이에 대해 크게 두 가지 이유를 예상해보았습니다.&lt;/p&gt;

    &lt;p&gt;먼저, 우리가 가진 딥러닝 모델이 너무 단순합니다. 은닉 layer가 2개뿐이고 Dropout과 같은 over-fitting 방지 기법들도 적용해주지 않았기 때문에 정교하게 tuning된 RF에 비해 성능이 떨어지는 것이라고 예측해볼 수 있습니다.&lt;/p&gt;

    &lt;p&gt;다음으로는 RF의 Robustness때문입니다. Tree 기반인 RF는 다른 모델들에 비해 안정적이고 Robust한 모델로 알려져 있습니다. DepartmentDesciption 파생변수로 인해 data의 sparsity가 높아진 지금, 이러한 RF의 특징이 빛을 발한 것으로 생각됩니다. 따라서 PCA등과 같이 Spasity를 잡아주는 방법을 적용하여 딥러닝 모델에 적용한다면 더 좋은 성능을 얻을 수 있을 것 같습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;결측값 imputation&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;위 분석에서 결측값에 대한 처리는 따로 해주지 않았습니다. 그 이유는 결측값이 3,000여 개로 전체 dataset크기 대비 작았기 때문인데요. 그래도 이 부분에 대한 imputation이 이루어졌다면 더 좋은 분석이 되었을 것 같습니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 20 Feb 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/competition/2020/02/20/Walmart-Saletype-Classification.md.html</link>
        <guid isPermaLink="true">http://localhost:4000/competition/2020/02/20/Walmart-Saletype-Classification.md.html</guid>
        
        <category>kaggle</category>
        
        
        <category>Competition</category>
        
      </item>
    
      <item>
        <title>Gloval Vectors for Word Representation, GloVe</title>
        <description>&lt;h1 id=&quot;glove-이해하기&quot;&gt;GloVe 이해하기!&lt;/h1&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;​	이번 포스팅은 Word representation의 한 방법론인 GloVe에 대해서 알아보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;GloVe는 Word embedding 방법 중 Distributed representation에 해당하는 방법론으로, 기존의 Word2Vec이나 LSA와 같은 알고리즘이 가지고 있던 한계점들을 보완하여 개발한 알고리즘입니다.&lt;/p&gt;

&lt;p&gt;본 글에서는 GloVe가 기존 알고리즘에 비해 어떤 이 점을 갖는지, 어떤 프로세스를 거쳐 텍스트의 Distributed form을 찾게 되는 지 등에 대해 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;distributed-representation&quot;&gt;Distributed Representation&lt;/h3&gt;

&lt;p&gt;​	&lt;strong&gt;Word representation&lt;/strong&gt; 방법의 초기 모델은 &lt;strong&gt;WordNet&lt;/strong&gt;이나 &lt;strong&gt;One-hot Vector&lt;/strong&gt; 방법이 있습니다. 이 방법론들은 분석자가 미리 단어의 사전적 뜻을 정해두어 표현하거나, 단순 1,0 변환으로 표현하는 것입니다. 이는 비교적 직관적인 이해를 하기에 용이하나 단어의 분포적 특성을 파악하지 못하므로 단어들의 관계나 동명이의어 등을 쉽게 파악해내지 못합니다. 이에 보다 발전적인 방법론이 대두된 것이 &lt;strong&gt;Distributed Representation&lt;/strong&gt; 방법입니다. Distributed Representation은 말뭉치(Corpus)를 기반으로 단어의 출현 빈도를 Word vector로 변환하여 학습하는 방법입니다. 이 전 방법론에 비해 단어의 관계를 표현하는것이 가능하며, 사람의 주관적 개입도 최소화할 수 있다는 장점을 가지고 있습니다. Distributed Representation의 대표적인 알고리즘으로는 &lt;strong&gt;Word2Vec&lt;/strong&gt;과 &lt;strong&gt;LSA&lt;/strong&gt;를 들 수 있습니다.(본 포스팅에서 다루는 GloVe도 포함됩니다.)&lt;/p&gt;

&lt;p&gt;​	하지만 이러한 Word2Vec과 LSA에도 단점이 존재합니다. 먼저 Word2Vec의 경우 중심단어로부터 사용자가 지정한 window내에서만 관계를 파악하여 분석하기 때문에 Corpus 전체의 통계정보, 즉, &lt;u&gt;Co-occurrence 정보를 반영하지 못하는 단점&lt;/u&gt;이 있습니다. 더하여 한 번에 하나의 계산만 수행하기 때문에 업데이트에 어려움이 있습니다. 다음으로 LSA의 경우 Corpus 전체의 Co-occurrence 정보는 모두 반영되는 대신, &lt;u&gt;단어 간 유사도나 관계에 대해서는 파악하기 어려운 단점&lt;/u&gt;을 가집니다.&lt;/p&gt;

&lt;p&gt;​	&lt;strong&gt;GloVe&lt;/strong&gt;는 이러한 단점들을 보완하여 발전시킨 방법론이라 할 수 있습니다. 즉, &lt;strong&gt;Corpus 전체의 Co-occurrecnce는 모두 반영하면서 단어간 유사도도 측정할 수 있는 방법론&lt;/strong&gt;이라 할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;how-to-do-that--probability-of-co-occurrence&quot;&gt;How to do that?  “Probability of Co-Occurrence”&lt;/h3&gt;

&lt;p&gt;​	위의 단점들을 해결하는 방법으로 GloVe 연구팀이 제시한 것이 바로 &lt;strong&gt;“동시등장확률(Probability of Co-occurrence)”&lt;/strong&gt;입니다. 먼저 논문 속 예시를 보며 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://hyj0103.github.io/assets/Glove.jpg&quot; alt=&quot;PR&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​	위의 표 중 첫 번째와 두 번째 row는 각각 ice과 steam이라는 단어가 주어졌을 때, k 단어가 나타날 확률을 나타냅니다. 예시에서는 k로 solid, gas, water, fashion이 주어져 있습니다. 마지막 세 번째 row에는 1, 2 row의 비로 표현됩니다. 각각의 값을 살펴보면 solid는 ice가 조건부 확률로 주어졌을 경우가 steam의 경우보다 더 등장확률이 높은 것을 알 수 있습니다. gas의 경우 solid와 반대로 나타나게 됩니다. 또한 ice, steam모두 관련이 있는 water / 모두 관련이 없는 fashion의 등장확률은 1, 2 행의 값이 비슷하게 나타나게 됩니다.&lt;/p&gt;

&lt;p&gt;​	따라서 우리가 주목해야할 지점은 3번째 행입니다. 분자의 확률이 분모보다 더 클 경우, 즉 본 예시에서는 k 단어가 steam보다 ice에 더 연관이 많을 경우 해당 값은 1보다 큰 값으로 도출됩니다. (분모, 분자 차이가 클수록 값이 더 커져갑니다.) 반대의 경우에는 1보다 작은 값을 가지게 됩니다. 또한 3, 4번째 열과 같이 조건부 단어에 둘다 관련이 없거나 둘다 관련이 있는 경우(water, fashion) 1과 비슷한 값을 가지게 됩니다. 이 것이 바로 &lt;strong&gt;동시등장확률&lt;/strong&gt;입니다.&lt;/p&gt;

&lt;p&gt;​	GloVe 방법론은 Corpus전체에서 위와 같은 단어 간의 동시등장확률을 계산하여 단어들을 표현해내게 됩니다. 이에 따라 앞서 언급하였 듯, Corpus 전체의 Co-occurrence를 반영하면서도 단어 간의 관계를 표현해낼 수 있게 되는 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;process-of-glove&quot;&gt;Process of GloVe&lt;/h3&gt;

&lt;p&gt;​	그렇다면 동시등장확률을 활용하여 텍스트를 학습하는 &lt;strong&gt;목적함수(Objective function)&lt;/strong&gt;를 알아보겠습니다. 동시등장확률을 이용한 목적함수를 수식으로 표현해보면 아래와 같습니다. (기본적인 notation은 위의 예시에서와 동일하게 사용)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://hyj0103.github.io/assets/glove2.jpg&quot; alt=&quot;ojf1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​	위 수식을 살펴보면, 어떠한 단어 벡터 K가 주어졌을 때 i, j 와의 관계에 대한 비를 구하는 함수 F를 의미합니다. 여기서 함수 F가 바로 우리가 구하고자 하는 목적함수가 될 것입니다. 위 수식을 차근차근 재표현해보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://hyj0103.github.io/assets/glove3.jpg&quot; alt=&quot;ojf2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​	첫 번째 수식부터 살펴보도록 하겠습니다. 먼저 우리가 관심있는 i와 j 단어 벡터의 관계를 linear space에서 가장 쉽게 표현할 수 있는 방법인 차를 이용하여 함수를 구성합니다. 그 다음으로 scalar인 확률 값을 계산해주기 위하여 두 벡터를 내적한 값으로 표현하였습니다. 이 때, 두 번째 수식에서 우변에 나타난 확률 값의 비를 우리가 구하고자 하는 목적함수로 표현해주면 보다 손쉽게 단어 간의 관계를 파악할 수 있겠다는 아이디어로 i, j와 k를 내적한 값을 목적함 수에 넣어주는 형태로 표현을 하게 됩니다.&lt;/p&gt;

&lt;p&gt;​	최종 수식을 만족하는 목적함수를 구하려면, 다음의 조건을 만족해야 합니다. 먼저 첫 째로, corpus상에서 i 단어 벡터는 언제든지 k 단어 벡터로 변환될 수 있어야 합니다. 또한 co-occurrence matrix가 symmetric하다는 것을 반영할 수 있어야 하며,  마지막 수식에서 나타난 것과 같이 함수 내 값의 차가 함수의 비로 표현되는 homomorphism 조건을 만족해야 합니다. 우리는 이 조건을 모두 만족하는 함수가 &lt;strong&gt;지수함수&lt;/strong&gt;임을 알고 있습니다. 따라서 지수함수를 이용하여 위 식을 표현해보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://hyj0103.github.io/assets/glove4.jpg&quot; alt=&quot;obj3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;​	이 때, 두 번째 수식의 우변의 경우 위에서 언급한 것처럼 언제든 k와 i가 바뀔 수 있기 때문에 k와 i가 교체되더라도 바뀌지 않도록 수식을 로그가 아닌 &lt;strong&gt;상수 b&lt;/strong&gt;로 재 표현해줍니다.&lt;/p&gt;

   																	 &lt;img src=&quot;https://hyj0103.github.io/assets/glove5.jpg&quot; alt=&quot;ojf4&quot; /&gt;

&lt;p&gt;​	따라서 최종 수식에서 우변을 좌변으로 이항시키면 우리는 원하던 목적함수를 얻을 수 있습니다. 여기에 GloVe 연구팀은 조건식을 하나 더 추가해줍니다. 바로 대량으로 등장하는 단어들에 cap을 걸어주기 위함인데요. 만약 주어진 corpus 내에  굉장히 잦은 빈도로 출현하는 단어의 경우 우리의 목적 함수에서 noise가 될 수 있습니다. (제곱 term으로 빈도수가 높다면 목적함수의 값이 커질 것이므로) 따라서 단어의 빈도수가 아무리 많더라도 1에 cap을 걸어줌으로써 해당 단어에 대한 Overfitting을 방지하는 역할을 하는 &lt;strong&gt;f(X) term&lt;/strong&gt;을 추가해서 곱해주게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://hyj0103.github.io/assets/glove6.jpg&quot; alt=&quot;final&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​		&lt;em&gt;※ f(X) term의 알파의 경우 hyper parameter입니다. 논문에 따르면 simulation 결과, 알파가 3/4 일 때 가장 최상의 결과를 낸다고 주어져 있습니다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;### GloVe 실습&lt;/p&gt;

&lt;p&gt;​&lt;/p&gt;
</description>
        <pubDate>Thu, 16 May 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/text-mining/2019/05/16/Gloval-Vectors-for-Word-Representation,-GloVe.md.html</link>
        <guid isPermaLink="true">http://localhost:4000/text-mining/2019/05/16/Gloval-Vectors-for-Word-Representation,-GloVe.md.html</guid>
        
        <category>text-mining</category>
        
        
        <category>Text-mining</category>
        
      </item>
    
      <item>
        <title>NLP, Text mining start!</title>
        <description>&lt;h1 id=&quot;nlp--text-mining-study&quot;&gt;NLP &amp;amp; Text Mining Study&lt;/h1&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;​	19년도 1학기 Data Science Lab 스터디 주제는 &lt;strong&gt;Text Mining&lt;/strong&gt;입니다.&lt;/p&gt;

&lt;p&gt;매주 1회 NLP(Nature Language Processing) 기법 중 Text Mining에 활용할 수 있는 알고리즘들을&lt;/p&gt;

&lt;p&gt;공부하여 포스팅할 계획입니다.&lt;/p&gt;

&lt;p&gt;이번 포스트에서는 본격적인 알고리즘 소개에 앞서 NLP가 어떤 프로세스를 가지고 있는 지, 어떤 방법&lt;/p&gt;

&lt;p&gt;으로 Language data를 처리하는 지 먼저 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(본문에 앞서 본 포스트는 &lt;a href=&quot;https://ratsgo.github.io/natural%20language%20processing/2017/03/22/lexicon/&quot;&gt;ratsgo님의 블로그&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=OQQ-W_63UgQ&quot;&gt;Stanford University School of Engineering&lt;/a&gt;의 NLP강의를 참고하여 작성하였음을 미리 밝힙니다.)&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;why-is-nlp-hard&quot;&gt;Why is NLP hard?&lt;/h2&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;​	기본적으로 인간의 언어를 데이터로 표현하고 분석하는 일은 결코 쉬운 작업이 아닙니다.&lt;/p&gt;

&lt;p&gt;인간의 언어는 동음이의어와 같이 문맥 상 해석이 필요한 경우가 잦고,  동일한 의미 내에서도&lt;/p&gt;

&lt;p&gt;단어들의 뉘앙스와 같은 모호한 차이점이 발생합니다. 때로는 신조어나 문법에 맞지 않는 구어적&lt;/p&gt;

&lt;p&gt;표현으로 분석을 어렵게 하는 경우도 많습니다.&lt;/p&gt;

&lt;p&gt;​	이렇게 어려운 인간의 말을 잘 분석해내기 위해 자연어 처리에는 다양한 분야의 지식들이 서로&lt;/p&gt;

&lt;p&gt;연관되어 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://hyj0103.github.io/assets/textmining_field.jpg&quot; alt=&quot;field&quot; width=&quot;500&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​	먼저 NLP는 본질적으로 언어학에 큰 뿌리를 두고 있습니다. 언어학에서 말을 구분하는&lt;/p&gt;

&lt;p&gt;음운론(Pnomology), 형태론(Morphology), 통사론(Syntax), 의미론(Senmantics) 등의 아이디어를 기반&lt;/p&gt;

&lt;p&gt;으로 문장을 구분하고 단어를 파악합니다.&lt;/p&gt;

&lt;p&gt;​	또한 문장들을 계산할 수 있는 벡터로 representation하고 이를 분석하는 과정에서 다양한 수학적,&lt;/p&gt;

&lt;p&gt;통계적 지식이 활용됩니다.&lt;/p&gt;

&lt;h2 id=&quot;basic-nlp-process&quot;&gt;Basic NLP Process&lt;/h2&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;​	다음 그림은 &lt;strong&gt;NLP 처리 과정&lt;/strong&gt;을 한 눈에 보기 쉽게 정리한 것인데 이를 이용하여 기본적인&lt;/p&gt;

&lt;p&gt;NLP 프로세스를 따라가보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://hyj0103.github.io/assets/NLPprocess.jpg&quot; alt=&quot;process&quot; width=&quot;500&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;1-sentence-splitting&quot;&gt;1. Sentence Splitting&lt;/h4&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;​	일반적으로 우리가 분석하고자 하는 Text Data를 말뭉치, &lt;strong&gt;Corpus&lt;/strong&gt;라고 지칭합니다.&lt;/p&gt;

&lt;p&gt;컴퓨터에게 이러한 Corpus는 분석할 수 없는 언어들의 나열일 것이므로 우리는 이를 적절히&lt;/p&gt;

&lt;p&gt;representation 해주는 것이 필요합니다. 이 과정의 첫 번째 단계가 &lt;strong&gt;Sentence Splitting&lt;/strong&gt;입니다.&lt;/p&gt;

&lt;p&gt;단어의 의미 그대로 Corpus를 문장별로 끊어준다는 의미입니다. 보통 마침표, 느낌표, 물음표 등으로&lt;/p&gt;

&lt;p&gt;구분합니다.&lt;/p&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;h4 id=&quot;2-tokenize&quot;&gt;2. Tokenize&lt;/h4&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;​	&lt;strong&gt;토큰(Token)&lt;/strong&gt;은 의미를 가지는 문자열을 지칭합니다. 작게는 형태소에서 크게는 단어에 이르기까지&lt;/p&gt;

&lt;p&gt;독립적인 의미를 갖는 문자열을 의미하는 것입니다. &lt;strong&gt;Tokenize&lt;/strong&gt;단계에서는 앞 서 끊어준 문장들을 다시&lt;/p&gt;

&lt;p&gt;의미를 가지는 토큰들로 나누어 주는 것을 의미합니다.&lt;/p&gt;

&lt;p&gt;​	독립적인 의미를 갖는 문자열을 찾는 방법은 언어별, 쓰임새별, 문법별로 다양한 차이가 나타날 수&lt;/p&gt;

&lt;p&gt;있고 때때로 Sentence Splitting 기법보다 더 난해한 작업이 되기도 합니다.&lt;/p&gt;

&lt;h4 id=&quot;3-pos-tagging-part-of-speech-tagging&quot;&gt;3. POS Tagging (Part-Of-Speech Tagging)&lt;/h4&gt;

&lt;p&gt;​	&lt;strong&gt;포스태깅&lt;/strong&gt;은 앞서 나뉘어진 토큰들에 품사를 지정해주는 작업(Tagging)을 말합니다.&lt;/p&gt;

&lt;p&gt;토큰에 품사를 정해주기 위해서는 단어간의 관계나 문장 내 쓰임새에 대한 분석 능력이 필요할텐데요.&lt;/p&gt;

&lt;p&gt;다양한 머신러닝 계열 기법들이 활용되어  포스태깅 작업을 진행하고 있습니다.&lt;/p&gt;

&lt;p&gt;​	더하여 &lt;strong&gt;Named entity recognition&lt;/strong&gt;은 포스태깅 작업이 확장된 개념으로 각각의 태그별로 사람의&lt;/p&gt;

&lt;p&gt;이름이나, 지역 이름, 회사 이름과 같은 고유 명사를 분류해주는 작업을 말합니다.&lt;/p&gt;

&lt;p&gt;​	여기까지의 Sentence Splitting과 Tokenize, 포스태깅을 묶어 자연어 데이터의&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;전처리 작업(Preprocessing)&lt;/strong&gt;이라고 볼 수 있겠습니다.&lt;/p&gt;

&lt;p&gt;​	실제 Corpus예시를 가지고 전처리 작업 실습을 진행해보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;파이썬의 &lt;strong&gt;KoLNPy 패키지&lt;/strong&gt;를 활용하면 전처리 일련의 작업들을 손 쉽게 처리할 수 있습니다.&lt;/p&gt;

&lt;p&gt;다음은 KIA타이거즈 양현종 선수 관련 기사를 발췌하여 실습을 진행한 결과입니다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{python}&quot;&gt;from konlpy.tag import Twitter
Twitter.pos('돌을 던질 수 없다. KIA타이거즈 에이스 양현종이 개막 이후 부진에 빠졌다. 3경기에 등판해 14이닝을 던져 14자책점을 기록했다. 왜 그럴까?')

 ('돌', 'Noun'),
 ('을', 'Josa'),
 ('던질', 'Verb'),
 ('수', 'Noun'),
 ('없다', 'Adjective'),
 ('.', 'Punctuation'),
 ('KIA', 'Alpha'),
 ('타이거즈', 'Noun'),
 ('에이스', 'Noun'),
 ('양현종', 'Noun'),
 ('이', 'Josa'),
 ('개막', 'Noun'),
 ('이후', 'Noun'),
 ('부진', 'Noun'),
 ('에', 'Josa'),
 ('빠졌다', 'Verb'),
 ('.', 'Punctuation'),
 ('3', 'Number'),
 ('경기', 'Noun'),
 ('에', 'Josa'),
 ('등', 'Noun'),
 ('판해', 'Verb'),
 ('14', 'Number'),
 ('이닝', 'Noun'),
 ('을', 'Josa'),
 ('던져', 'Verb'),
 ('14', 'Number'),
 ('자책점', 'Noun'),
 ('을', 'Josa'),
 ('기록', 'Noun'),
 ('했다', 'Verb'),
 ('.', 'Punctuation'),
 ('왜', 'Noun'),
 ('그럴까', 'Adjective'),
 ('?', 'Punctuation')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;​	결과를 살펴보면 먼저 마침표, 물음표 등을 기준으로 문장이 나뉘어진 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;또한 독립적인 의미를 갖는 형태소별로 각 문장이 잘 나뉘는 것을 볼 수 있으며, 영어나 특수문자도&lt;/p&gt;

&lt;p&gt;잘 구별됩니다. 마지막으로 각 단어들에 품사들이 잘 주어져 있어 포스태깅 작업도 이상없이&lt;/p&gt;

&lt;p&gt;진행되었음을 알 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;4-syntactic-semantic-analysis&quot;&gt;4. Syntactic, Semantic Analysis&lt;/h4&gt;

&lt;p&gt;​	일반적인 데이터 분석 과정에서 데이터 전처리가 끝나면 알고리즘을 적용하듯이, 자연어 처리에서도&lt;/p&gt;

&lt;p&gt;데이터 전처리 후 Corpus에서 여러 의미를 도출하기 위해 알고리즘을 적용합니다.&lt;/p&gt;

&lt;p&gt;Corpus 내의 문장, 단어의 관계나 의미를 파악하는 데 있어 크게 두 가지 관점으로 나누어 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;​	먼저, &lt;strong&gt;통사론적(Syntactic)&lt;/strong&gt; 분석 방법이 있습니다. 통사론적 분석은 문장이 가지는 문법 구조,&lt;/p&gt;

&lt;p&gt;특정 단어가 주위 단어들과 어떤 구조로 얽혀있는 지에 대해 분석하는 방법입니다. 자연어 데이터를&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parsing한다&lt;/strong&gt;는 개념이 바로 통사론적 분석 방법에 속합니다. 주로 앞서 진행된 전처리 과정에서&lt;/p&gt;

&lt;p&gt;나타나는 토큰들과 태깅들을 바탕으로 핵심 단어를 선택해서 주변 단어들과의 관계를 보는 방식으로&lt;/p&gt;

&lt;p&gt;진행됩니다. 올바른 전처리 과정, Parsing 방법을 통해 컴퓨터는 주어진 Corpus의 구조를 더욱 명확하게&lt;/p&gt;

&lt;p&gt;파악할 수 있게 됩니다.&lt;/p&gt;

&lt;p&gt;​	두 번째는 &lt;strong&gt;의미론적(Semantic)&lt;/strong&gt; 분석 방법입니다. 앞 서 통사론적 분석 방법이 문법이나 품사와&lt;/p&gt;

&lt;p&gt;같은 문장의 전반적인 구조에 대해서 살펴보았다면, 의미론적 분석 방법은 특정 단어, 구, 문장 등이&lt;/p&gt;

&lt;p&gt;가지는 &lt;strong&gt;의미&lt;/strong&gt;에 방점을 두고 분석하는 방법입니다.  CNN, RNN과 같은 Deep Learning 기법을 통해&lt;/p&gt;

&lt;p&gt;단어의 의미를 학습하게 되며, 이를 기반으로 더 큰 단위인 구, 문장 등으로 확장해가며 Corpus 내의&lt;/p&gt;

&lt;p&gt;Semantic 정보들을 분석하게 됩니다.&lt;/p&gt;

&lt;p&gt;​	Syntactic과 Semantic 모두 Text의 관계를 파악하는 데 중요한 개념입니다. 두 분석을 위한 알고리즘&lt;/p&gt;

&lt;p&gt;들은 다양하게 준비되어 있습니다. Corpus의 특징, 분석의 목적 등에 따라 적절한 알고리즘을 사용하여&lt;/p&gt;

&lt;p&gt;Corpus를 적절하게 분석하는 것이 Text Mining의 핵심이라고 할 수 있습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;​	지금까지 &lt;strong&gt;NLP 분석 과정&lt;/strong&gt;에 대해 간략하게 살펴보았습니다. 다음 포스트 부터는 본격적으로&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Text Mining 알고리즘&lt;/strong&gt;들에 대해 하나씩 공부해 보겠습니다.&lt;/p&gt;

&lt;p&gt;​&lt;/p&gt;
</description>
        <pubDate>Fri, 05 Apr 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/text-mining/2019/04/05/NLP,-TextMining-Start!.md.html</link>
        <guid isPermaLink="true">http://localhost:4000/text-mining/2019/04/05/NLP,-TextMining-Start!.md.html</guid>
        
        <category>text-mining</category>
        
        
        <category>Text-mining</category>
        
      </item>
    
  </channel>
</rss>
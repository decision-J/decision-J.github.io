I"s<h1 id="paper-review-anomaly-transformer">[Paper review] Anomaly Transformer</h1>

<p>시계열 데이터의 이상치 탐색 방법론으로서 Transformer 매커니즘을 활용한 <strong>Anomaly Transformer</strong>에 대해 리뷰해보겠습니다. ICLR 2022 의 spotlight 논문이라고 하네요!</p>

<p><em>Xu, Jiehui, et al. “Anomaly transformer: Time series anomaly detection with association discrepancy.” arXiv preprint arXiv:2110.02642 (2021)</em></p>

<p>(본 리뷰의 모든 수식과 그림은 원 논문을 참고했습니다.)</p>

<hr />

<h3 id="idea">Idea</h3>
<p><strong>Anomaly Transformer</strong>는 이름에서 알 수 있듯이 <em>Transformer</em>를 활용하여 시계열 데이터에서의 <em>Anomaly detection</em>을 잘 해보자는 목적을 가지고 있습니다. 이상점을 탐색하는 방법으로는 reconstruction based 방법을 사용하고 있습니다. Unsupervised learning으로 주어진 train 시계열 데이터의 패턴을 통해 정상 시계열 패턴을 잘 재구축 하도록 모형을 학습시킵니다. 추후에 test set이 들어왔을 때, 모형이 재구축(reconstruction)한 시계열 패턴과의 비교를 통해 그 차이가 큰 (anomaly score가 큰) 지점을 이상점으로 탐색하는 방법론입니다. 이 때, 정상 시계열 패턴을 잘 학습하도록 해주기 위해 Transformer를 활용합니다. Self-attention을 이상점 탐지에 특화되게 바꾼 Anomaly-attention을 제안하였습니다. 그럼 Anomaly Transformer에 대해 본격적으로 알아보도록 하겠습니다.</p>

<h3 id="anomaly-transformer">Anomaly Transformer</h3>
<p>Anomaly Transformer의 architecture는 다음과 같습니다.</p>

<p align="center">
  <img src="https://decision-J.github.io/assets/Anomaly_Detection/Anomaly_Transformer/Architecture.png" alt="Anomaly Transformer architecture" />
</p>

<p>Input embedding $X_{0}$가 들어오면 크게 <strong>Anomaly Attenion</strong> layer와 Feed Forward layer를 L번 반복 수행하며 input time series의 pattern을 학습하게 됩니다. 이후 이 학습된 정보를 가지고 가장 보편적인 pattern의 reconstruction data를 산출하는데요! 이 구조에서 가장 주목해야 하고, Anomaly transformer가 높은 성능을 가지는 이유는 단연 Anomaly attention일 것입니다.</p>

<h3 id="anomaly-attention">Anomaly Attention</h3>
<p>Anomaly attention은 기본적인 attention을 가지고 저자들이 time series anomaly detection에 맞게 매커니즘을 조금 수정한 형태입니다. 가장 큰 부분은 attention 내부를 <em>Prior-Association</em>과 <em>Series-Association</em>의 두 가지로 나누었다는 것입니다. 
먼저 prior association은 이름에서 알 수 있듯이, 모형에서 사용하는 Gaussian kernel의 시그마를 학습합니다. Gaussian Kernel은 attention의 Query, Key를 학습할 때 영향을 줍니다. 커널을 통해 다양한 패턴의 시계열 자료에 적용할 수 있다고 저자는 설명하고 있습니다. Series asssociation은 Query와 Key를 학습하는 부분입니다. 저자들은 이 두 association을 통해서(정확히는 prior association) 시계열 자료를 point-wise가 아닌, temporal dependency를 학습할 수 있다고 설명합니다. Gaussian Kernel의 $\sigma$를 통해서 인접한 time point에 더 큰 가중치를 줄 수 있다는 것이지요.</p>

<h3 id="association-discrepancy">Association Discrepancy</h3>
<p>Association Discrepancy는 앞서 살펴본 prior association과 series association간의 차이를 계산하는 것입니다. 차이를 계산하는 방법은 KL divergence를 활용하였으며, KL divergence의 assymetric한 점을 보완하기 위해 순서를 바꿔가며 계산한 평균을 사용하였습니다. Association Discrepancy는 Loss function의 한 term으로 포함되며 가중치를 업데이트 하는데 사용하기도 하고 추후 anomaly score를 계산하는 데에도 사용됩니다.</p>

<p>$$ AssDis(P,S; X) = \frac{1}{L} \sum^L_{l=1}(KL(P^l_i||S^l_i) + KL(S^l_i||P^l_i)) $$
$$ where\,\, i=1,…,N $$</p>

<h3 id="mini-max-association-learning">Mini Max Association Learning</h3>
<p>저자들은 Attention 내부에 prior &amp; series association 두 개의 term이 있는 만큼 이를 활용하여 Loss function을 병렬적으로 구성하는 mini max learning strategy를 제안합니다. 우선 Anomaly transformer에서 사용하는 Loss function은 다음과 같습니다.</p>

<table>
  <tbody>
    <tr>
      <td>$$ L_{total}(\hat{X}, P, S, \lambda; X) = (|</td>
      <td>X-\hat{X}|</td>
      <td>)^2_F - \lambda \cdot |</td>
      <td>AssDis(P, S; X)|</td>
      <td>$$</td>
    </tr>
  </tbody>
</table>

<p>수식을 보면 $\lambda$ term에 의해 Association discrepancy가 조절됨을 알 수 있습니다. $\lambda$가 0보다 크면 모형은 association discrepancy를 더 크게 만드는 방향으로 학습이 진행되며, 0보다 작으면 반대로 작게 만드는 방향으로 학습이 진행됩니다. 이 개념을 활용한 것이 바로 <strong>Mini max Strategy</strong> 입니다.</p>

<p>Mini max는 말 그대로 <strong>Minimize phase, Maxmize phase</strong> 두 부분으로 나누어 Loss function을 다르게 적용하는 것입니다.</p>

<p>$$ Minimize: L_{Total}(\hat{X}, P, S_{detach}, -\lambda; X) $$
$$ Maximize: L_{Total}(\hat{X}, P_{detach}, S, \lambda; X) $$</p>

<p>수식에서 알 수 있듯이, Minimize phase때는 Prior association을 학습하고, Maximize phase 때는 Series association을 학습합니다. 이 때, $\lambda$의 부호에 따라 학습의 방향이 달라지는 것입니다. 논문의 그림을 보면 좀 더 이해하기 쉽습니다.</p>

<p align="center">
  <img src="https://decision-J.github.io/assets/Post Images/Anomaly Transformer.png" />
</p>

<h3 id="association-based-anomaly-criterion">Association-based Anomaly Criterion</h3>

<h3 id="experiments">Experiments</h3>

<hr />
<h3 id="reference">Reference</h3>

<ol>
  <li>Paper: <a href="https://arxiv.org/pdf/2110.02642.pdf">Anomaly Transformer</a></li>
  <li>Youtube: <a href="https://www.youtube.com/watch?v=C3dphckvyn0&amp;ab_channel=%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90%EC%82%B0%EC%97%85%EA%B2%BD%EC%98%81%EA%B3%B5%ED%95%99%EB%B6%80DSBA%EC%97%B0%EA%B5%AC%EC%8B%A4">고려대학교 산업경영공학과 DSBA연구실 세미나</a></li>
  <li>Github: <a href="https://github.com/thuml/Anomaly-Transformer">Code (Pytorch)</a></li>
</ol>

:ET